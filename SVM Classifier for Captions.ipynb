{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Classifier for smaller dataset\n",
    "\n",
    "http://www.developintelligence.com/blog/2017/06/practical-neural-networks-keras-classifying-yelp-reviews/\n",
    "\n",
    "\n",
    "Conversely, for smaller datasets, the SVM is much better than the neural network.\n",
    "The SVM, on the other hand, will perform well even for much smaller datasets.\n",
    "\n",
    "Now, instead of converting each word to a single number and learning an Embedding layer, we use a term-frequency inverse document frequency (TF-IDF) vectorisation process. Using this vectorisation scheme, we ignore the order of the words completely, and represent each review as a large sparse matrix, with each cell representing a specific word, and how often it appears in that review. We normalize the counts by the total number of times the word appears in all of the reviews, so rare words are given a higher importance than common ones (though we ignore all words that aren’t seen in at least three different reviews.\n",
    "\n",
    "    Line six sets up the vectorizer. We set ngram_range to (1,2) which means we’ll consider all words on their own but also look at all pairs of words. This is useful because we don’t have a concept of word order anymore, so looking at pairs of words as single tokens allows the classifier to learn that word pairs such as “not good” are usually negative, even though “good” is positive. We also set min_df to 3, which means that we’ll ignore words that aren’t seen at least three times (in three different reviews)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "from keras.layers import Embedding, LSTM, Dense, Conv1D, MaxPooling1D, Dropout, Activation\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "%matplotlib inline \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes:  3\n",
      "Counter({0: 420, 1: 317, -1: 163})\n",
      "Vectors shape: (900, 21279)\n",
      "Cross-Validation scores:  [0.52747253 0.3956044  0.6043956  0.41111111 0.56666667 0.41111111\n",
      " 0.53333333 0.52808989 0.59550562 0.56179775]\n",
      "Cross-validation results: 0.51% (+/- 0.07%)\n"
     ]
    }
   ],
   "source": [
    "captions_root = \"dataset/metadata\"\n",
    "captions_root_path = pathlib.Path(captions_root)\n",
    "human_output_caption_to_label_csv_path = captions_root_path/'humanCaptionWithLabeldf.csv'\n",
    "dataset_path = human_output_caption_to_label_csv_path\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def get_words_withoutstopwords(words):\n",
    "    words_without_stopwords = []\n",
    "    for word in words:\n",
    "        if word not in stop_words:\n",
    "            words_without_stopwords.append(word)\n",
    "    return words_without_stopwords\n",
    "\n",
    "def change_label_str_to_int(labelStr,isBinaryClassification):\n",
    "    \"\"\"this is only for binary classification, negative sentiment has value 0, positive sentiment has value 1\"\"\"\n",
    "    if labelStr == \"negative\":\n",
    "        if isBinaryClassification:\n",
    "            return 0\n",
    "        else:\n",
    "            return -1\n",
    "    elif  labelStr == \"neutral\":\n",
    "        if isBinaryClassification:\n",
    "            raise \"Neutral Should not exist\"\n",
    "        else:\n",
    "            return 0\n",
    "    elif  labelStr == \"positive\":\n",
    "        return 1\n",
    "    else:\n",
    "        raise labelStr+ \" Should not exist\"\n",
    "      \n",
    "        \n",
    "def get_df(isBinaryClassification):\n",
    "    df = pd.read_csv(dataset_path, header=0, sep=\"|\")\n",
    "    if isBinaryClassification:\n",
    "        # Doing only binary classification\n",
    "        df = df[(df.label == \"negative\") | (df.label == \"positive\")]\n",
    "        df = df.reset_index()\n",
    "    df[\"label\"] = df[\"label\"].apply(lambda x: change_label_str_to_int(x, isBinaryClassification))\n",
    "    df[\"caption\"] = df[\"caption\"].apply(lambda x: \" \".join(get_words_withoutstopwords(x.lower().split())))\n",
    "    return df    \n",
    "        \n",
    "\n",
    "isBinaryClassification = False    \n",
    "    \n",
    "df = get_df(isBinaryClassification)\n",
    "print(\"Classes: \", df.label.nunique())\n",
    "balanced_texts = []\n",
    "balanced_labels = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    label = int(row['label'])\n",
    "    caption = row['caption']\n",
    "    balanced_texts.append(caption)\n",
    "    balanced_labels.append(label)\n",
    "\n",
    "        \n",
    "print(Counter(balanced_labels))\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,4), min_df=0)\n",
    "classifier = LinearSVC()\n",
    "Xs = vectorizer.fit_transform(balanced_texts)\n",
    "print(\"Vectors shape:\", Xs.shape)\n",
    "\n",
    "kFoldSplits = 10\n",
    "\n",
    "scores = cross_val_score(classifier, Xs, balanced_labels, cv=kFoldSplits, n_jobs=-1)\n",
    "\n",
    "print(\"Cross-Validation scores: \",scores)\n",
    "print(\"Cross-validation results: %.2f%% (+/- %.2f%%)\" % (np.mean(scores), np.std(scores)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Neural Network based on Tf-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/elkhand/anaconda3/envs/cs231n/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py:497: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NHWC is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`NHWC` for data_format is deprecated, use `NWC` instead\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 40, 32)            320000    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 40, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 38, 32)            3104      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 38, 32)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 19, 32)            0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 19, 32)            0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 32)                8320      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 331,523\n",
      "Trainable params: 331,523\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "##############################################################\n",
    "# New Neural Network with Embedding layer\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import tensorflow as tf\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "num_words = 10000\n",
    "maxlen = 40\n",
    "\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "def generate_model_name(filename, best_acc_val):\n",
    "    timestamp = str(time.time()).split(\".\")[0]\n",
    "    best_acc_val = round(best_acc_val,4)\n",
    "    filename += \"-\" + str(best_acc_val) + \"-\" + timestamp\n",
    "    return filename\n",
    "\n",
    "def plot_model_accuracy(history):\n",
    "    \"\"\"plot acc and loss for train and val\"\"\"\n",
    "    filename = \"custom-hec\" \n",
    "    filename = generate_model_name(filename + \"-acc\", max(history.history['val_acc']))\n",
    "    fig = plt.figure()\n",
    "    print(history.history.keys())\n",
    "    print(\"best_val_acc\", max(history.history['val_acc']))\n",
    "    print(\"best_train_acc\", max(history.history['acc']))\n",
    "    print(\"lowest_val_loss\", min(history.history['val_loss']))\n",
    "    print(\"lowest_train_loss\", min(history.history['loss']))\n",
    "    #  \"Accuracy\"\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.show()\n",
    "    fig.savefig(\"custom-model/\" + filename + \".png\") \n",
    "    \n",
    "    # \"Loss\"\n",
    "    fig = plt.figure()\n",
    "    filename = \"hec\" \n",
    "    filename = generate_model_name(filename + \"-loss\", min(history.history['val_loss']))\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.show()\n",
    "    fig.savefig(\"custom-model/\" + filename + \".png\") \n",
    "\n",
    "tokenizer = Tokenizer(num_words=num_words)\n",
    "tokenizer.fit_on_texts(balanced_texts)\n",
    "sequences = tokenizer.texts_to_sequences(balanced_texts)\n",
    "data = pad_sequences(sequences, maxlen=maxlen)\n",
    "\n",
    "#This might take a while to run. Here, we use the most common 20000 words instead of 5. \n",
    "#The only other difference is that we pass maxlen=300 when we pad the sequences. \n",
    "#This means that as well as padding the very short texts with zeros,\n",
    "#we’ll also truncate the very long ones. All of our texts will then be represented by 300 numbers.\n",
    "\n",
    "\n",
    "filepath=\"custom-model/customNN.weights.best.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=0, save_best_only=True, mode='max')\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=25)\n",
    "callbacks_list = [ checkpoint, early_stopping] # reduce_lr,\n",
    "\n",
    "#In line two, we add an Embedding layer. This layer lets the network expand each token to a larger vector, \n",
    "#allowing the network to represent words in a meaningful way. We pass 20000 as the first argument, \n",
    "#which is the size of our vocabulary (remember, we told the tokenizer to only use the 20 000 most common words \n",
    "#earlier), and 128 as the second, which means that each token can be expanded to a vector of size 128. \n",
    "#We give it an input_length of 300, which is the length of each of our sequences.\n",
    "\n",
    "\n",
    "def load_dataset_StratifiedKFold(dfKFold, max_seq_len):\n",
    "    sentences = []\n",
    "    label_index = []\n",
    "    num_of_classes = 0\n",
    "    class_to_index = {}\n",
    "    index_to_class = {}\n",
    "    for index, row in dfKFold.iterrows():\n",
    "        caption = row['caption']\n",
    "        label = row['label']\n",
    "        sentence = caption\n",
    "        words = sentence.split(\" \")\n",
    "        words = get_non_stop_words(words)\n",
    "        sentence_embedding = get_sequence_embedding(words, max_seq_len)\n",
    "        sentences.append(sentence_embedding)\n",
    "        if label in class_to_index:\n",
    "            label_index.append(class_to_index[label])\n",
    "        else:\n",
    "            num_of_classes += 1\n",
    "            class_to_index[label] = num_of_classes - 1\n",
    "            index_to_class[num_of_classes - 1] = label\n",
    "            label_index.append(class_to_index[label])\n",
    "    X_train = np.array(sentences)\n",
    "    y_train = np.array(label_index)\n",
    "    return (X_train, y_train, num_of_classes)\n",
    "\n",
    "\n",
    "\n",
    "def get_LSTM_model(dropout, recurrent_dropout, isBinaryClassification):\n",
    "    # Cross-validation results: 0.84% (+/- 0.06%)\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(num_words, 32, input_length=maxlen))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(LSTM(32, dropout=dropout, recurrent_dropout=recurrent_dropout))\n",
    "    model.add(Dropout(dropout))\n",
    "    if isBinaryClassification:\n",
    "        activation = 'sigmoid'\n",
    "        loss='binary_crossentropy'\n",
    "        model.add(Dense(1, activation=activation))\n",
    "    else:\n",
    "        activation = 'softmax'\n",
    "        #loss='categorical_crossentropy'\n",
    "        loss = 'sparse_categorical_crossentropy'\n",
    "        num_classes = 3\n",
    "        model.add(Dense(num_classes, activation=activation))\n",
    "        #model.add(Activation(\"softmax\"))\n",
    "\n",
    "    model.compile(loss=loss, optimizer='adam', metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_CNN_model(dropout, recurrent_dropout, isBinaryClassification):\n",
    "    # Cross-validation results: 0.80% (+/- 0.03%), with dropout = 0.2\n",
    "    # Cross-validation results: 0.83% (+/- 0.05%)\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(num_words, 32, input_length=maxlen))# 128\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Conv1D(32, 3, activation='relu'))# 64, 5\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(MaxPooling1D(pool_size=2)) #4\n",
    "    model.add(Dropout(dropout)) # 0.2\n",
    "    model.add(LSTM(32, dropout=dropout, recurrent_dropout=recurrent_dropout)) # 128\n",
    "    model.add(Dropout(dropout))\n",
    "    \n",
    "    if isBinaryClassification:\n",
    "        activation = 'sigmoid'\n",
    "        loss='binary_crossentropy'\n",
    "        model.add(Dense(1, activation=activation))\n",
    "    else:\n",
    "        activation = 'softmax'\n",
    "        #loss='categorical_crossentropy'\n",
    "        loss = 'sparse_categorical_crossentropy'\n",
    "        num_classes = 3\n",
    "        model.add(Dense(num_classes, activation=activation))\n",
    "        #model.add(Activation(\"softmax\"))\n",
    "    \n",
    "    model.compile(loss=loss, optimizer='adam', metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_label_count(labels):\n",
    "    labelToCount = {}\n",
    "    for label in labels:\n",
    "        if label not in labelToCount:\n",
    "            labelToCount[label] = 1\n",
    "        else:\n",
    "            labelToCount[label] += 1\n",
    "    return labelToCount\n",
    "\n",
    "\n",
    "\n",
    "labelencoder_y_1 = LabelEncoder()\n",
    "\n",
    "# Instantiate the cross validator\n",
    "kfold_splits = 5\n",
    "skf = StratifiedKFold(n_splits=kfold_splits, shuffle=True)\n",
    "printCnt = 5\n",
    "cvscores = []\n",
    "isBinaryClassification = False\n",
    "balanced_labels = np.array(balanced_labels)\n",
    "# Loop through the indices the split() method returns\n",
    "for index, (train_indices, val_indices) in enumerate(skf.split(data, balanced_labels)):\n",
    "    X_train, X_val = data[train_indices], data[val_indices]\n",
    "    y_train, y_val = balanced_labels[train_indices], balanced_labels[val_indices]\n",
    "    \n",
    "    \n",
    "    #print(\"y_val\",y_val)\n",
    "    y_train = labelencoder_y_1.fit_transform(y_train)\n",
    "    y_val = labelencoder_y_1.fit_transform(y_val)\n",
    "    \n",
    "    #print(\"y_val\",y_val)\n",
    "    \n",
    "    #y_train = to_categorical(y_train)\n",
    "    #y_val = to_categorical(y_val)\n",
    "    \n",
    "    \n",
    "    dropout = 0.65\n",
    "    recurrent_dropout = 0.65\n",
    "    model = get_CNN_model(dropout, recurrent_dropout, isBinaryClassification)\n",
    "    #model = get_LSTM_model(dropout, recurrent_dropout, isBinaryClassification)\n",
    "    #history = model.fit(data, np.array(balanced_labels), validation_split=0.2, epochs=100, callbacks=callbacks_list)\n",
    "    history = model.fit(x=X_train,\n",
    "                          y=y_train, \n",
    "                          batch_size=16,# 64 seems fine, 32 is better \n",
    "                          epochs=100, \n",
    "                          verbose=0, \n",
    "                          validation_data = (X_val, y_val),\n",
    "                          shuffle=True,\n",
    "                          callbacks=callbacks_list) \n",
    "\n",
    "\n",
    "    val_acc_list = history.history['val_acc']\n",
    "    best_val_acc =  max(val_acc_list)\n",
    "    cvscores.append(best_val_acc)\n",
    "    print(\"best_val_acc: \", best_val_acc)\n",
    "    plot_model_accuracy(history)\n",
    "    \n",
    "print(\"Cross-validation results: %.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
