{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Captions+Twitter Keras GRU \n",
    "\n",
    "## New Train on Twitter dataset, validate on OASIS images caption dataset\n",
    "\n",
    "1,578,628 Tweets with positive/negative sentiments - labelled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os, sys, io,re, string, pathlib, random\n",
    "from collections import Counter, OrderedDict\n",
    "\n",
    "import hecutils.data_utils as dt\n",
    "import hecutils.scoring_utils as sc\n",
    "import hecutils.plotting_utils as pt\n",
    "import hecutils.caption_utils as caput\n",
    "import hecutils.image_utils as imut\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.callbacks import Callback, EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "from keras.layers import Dense, Conv1D, MaxPooling1D, GRU, Bidirectional, LSTM\n",
    "from keras.layers.core import Dropout, Flatten, Masking, ActivityRegularization\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from keras.utils import plot_model\n",
    "from keras import regularizers\n",
    "from keras.preprocessing import text\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "from sklearn.utils import shuffle, class_weight\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from gensim.models import word2vec\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "#--\n",
    "%matplotlib inline\n",
    "import os, sys\n",
    "import re\n",
    "import string\n",
    "import pathlib\n",
    "import random\n",
    "from collections import Counter, OrderedDict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import spacy\n",
    "from tqdm import tqdm, tqdm_notebook, tnrange\n",
    "#tqdm.pandas(desc='Progress')\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity='all'\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "num_cores = 8\n",
    "GPU=True\n",
    "CPU = not GPU\n",
    "\n",
    "if GPU:\n",
    "    num_GPU = 1\n",
    "    num_CPU = 8\n",
    "if CPU:\n",
    "    num_CPU = 8\n",
    "    num_GPU = 0\n",
    "\n",
    "config = tf.ConfigProto(intra_op_parallelism_threads=num_cores,\\\n",
    "        inter_op_parallelism_threads=num_cores, allow_soft_placement=True,\\\n",
    "        device_count = {'CPU' : num_CPU, 'GPU' : num_GPU})\n",
    "session = tf.Session(config=config)\n",
    "K.set_session(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "\n",
    "oasis_csv_path = \"dataset/metadata/OASIS.csv\"\n",
    "captions_root = \"/home/elkhand/git-repos/human-emotions-classifier/dataset/metadata\"\n",
    "captions_root_path = pathlib.Path(captions_root)\n",
    "human_output_caption_to_label_csv_path = captions_root_path/'humanCaptionWithLabeldf.csv'\n",
    "human_caption_csv_path = captions_root_path/'captions.csv'\n",
    "#fasttext_embedding_path = 'embedding/wiki-news-300d-1M.vec'\n",
    "fasttext_embedding_path = '/home/elkhand/datasets/glove-vectors/glove.twitter.27B.200d.txt'\n",
    "model_results_root_dir = \"model/\"\n",
    "inputDataset_csv_path = captions_root_path/\"inputDataset.csv\"\n",
    "testDataset_csv_path = captions_root_path/\"testDataset.csv\"\n",
    "\n",
    "neutralLow = 3.0 \n",
    "neutralHigh = 5.0\n",
    "\n",
    "auto_output_caption_to_label_csv_path = captions_root_path/'autoCaptionWithLabeldf.csv'\n",
    "auto_caption_csv_path = captions_root_path/'auto_generated_captions.csv'\n",
    "\n",
    "\n",
    "dataset_path = human_output_caption_to_label_csv_path\n",
    "# dataset_path = auto_output_caption_to_label_csv_path\n",
    "\n",
    "kfold_splits = 10 # 7 # 5 # 10 # 7 \n",
    "test_size = 0.1\n",
    "\n",
    "embedding_dimension = 200 # 300\n",
    "hidden_layer_dim = 32\n",
    "batch_size = 16 # 64\n",
    "nb_epochs = 100\n",
    "dropout = 0.3\n",
    "recurrent_dropout=  0.6\n",
    "patience = 10\n",
    "verbose = 0\n",
    "\n",
    "useF1Score = False # True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df.head()\n",
      "   imageId                                            caption  label\n",
      "0      I1           two acorns lying ground next oak leaves.      0\n",
      "1      I2  three acorns flat surface. one acorn missing c...      0\n",
      "2      I3  five acorns arranged circle black background. ...      0\n",
      "3      I4  two cocktails brown wooden table. cocktail fro...      0\n",
      "4      I5                three shelves full bottles whiskey.      0\n",
      "inputDataset.head()\n",
      "    index                                            caption  label\n",
      "0    723                    two sea lions face open mouths.      0\n",
      "1    751  empty lighted ski run twilight spectators watc...      0\n",
      "2    309                        pile dog feces lying grass.     -1\n",
      "3    104  group young students classroom knees hands fol...      0\n",
      "4    446         close-up computer keyboard. layout german.      0\n",
      "testDataset.head()\n",
      "    index                                            caption  label\n",
      "0    340  flooded river flowing bridge carrying vehicula...     -1\n",
      "1    108  three young boys older boy blue white canoe. t...      1\n",
      "2     91  three plastic bottles water blue caps laid fla...      0\n",
      "3    719  group students, sitting front blackboard asian...      0\n",
      "4     88  man sitting chair head tilted back eyes open g...      0\n",
      "\n",
      "\n",
      "Label distribution in inputDataset label\n",
      "-1    147\n",
      " 0    378\n",
      " 1    285\n",
      "Name: label, dtype: int64\n",
      "Label distribution in testDataset label\n",
      "-1    16\n",
      " 0    42\n",
      " 1    32\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "dt.create_caption_to_label(oasis_csv_path,human_caption_csv_path, human_output_caption_to_label_csv_path,neutralLow, neutralHigh)\n",
    "dt.create_caption_to_label(oasis_csv_path,auto_caption_csv_path, auto_output_caption_to_label_csv_path,neutralLow, neutralHigh)\n",
    "\n",
    "\n",
    "df = pd.read_csv(dataset_path, header=0, sep=\"|\")\n",
    "df[\"caption\"] = df[\"caption\"].apply(lambda x: \" \".join(caput.get_words_withoutstopwords(x.lower().split())))\n",
    "df[\"label\"] = df[\"label\"].apply(lambda x: caput.change_label_str_to_int(x))\n",
    "\n",
    "\n",
    "\n",
    "input_x, test_x, input_y,  test_y = train_test_split(df[\"caption\"],\n",
    "                                                     df[\"label\"],\n",
    "                                                     test_size=test_size,\n",
    "                                                     random_state=seed,\n",
    "                                                     stratify=df[\"label\"])\n",
    "\n",
    "inputDataset = pd.concat([input_x, input_y], axis=1)\n",
    "testDataset = pd.concat([test_x, test_y], axis=1)\n",
    "\n",
    "inputDataset = inputDataset.dropna()\n",
    "testDataset = testDataset.dropna()\n",
    "inputDataset = inputDataset.reset_index()\n",
    "testDataset = testDataset.reset_index()\n",
    "\n",
    "\n",
    "inputDataset.to_csv(inputDataset_csv_path, index=False, sep=\"|\")\n",
    "testDataset.to_csv(testDataset_csv_path, index=False, sep=\"|\")\n",
    "\n",
    "print(\"df.head()\\n\", df.head())\n",
    "print(\"inputDataset.head()\\n\", inputDataset.head())\n",
    "print(\"testDataset.head()\\n\", testDataset.head())\n",
    "print(\"\\n\")\n",
    "print(\"Label distribution in inputDataset\", inputDataset.groupby('label').label.count())\n",
    "print(\"Label distribution in testDataset\", testDataset.groupby('label').label.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding size : 1193514\n",
      "embedding dimension : (200,)\n"
     ]
    }
   ],
   "source": [
    "def load_embedding(path):\n",
    "    word2vec = {}\n",
    "    with io.open(path, encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            entries = line.rstrip().split(\" \")\n",
    "            word, entries = entries[0], entries[1:]\n",
    "            word2vec[word] = np.array(entries).astype(np.float) # Convert String type to float\n",
    "    print('embedding size : %d' % len(word2vec))\n",
    "    print('embedding dimension : %s' % (word2vec['apple'].shape,))\n",
    "    return word2vec\n",
    "    \n",
    "wordToVec = {}\n",
    "wordToVec = load_embedding(fasttext_embedding_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 8836: expected 4 fields, saw 5\\n'\n",
      "b'Skipping line 535882: expected 4 fields, saw 7\\n'\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en',disable=['parser', 'tagger', 'ner'])\n",
    "def tokenizer(s): return [w.text.lower() for w in nlp(tweet_clean(s))]\n",
    "\n",
    "def tweet_clean(text):\n",
    "    text = re.sub(r'[^A-Za-z0-9]+', ' ', text) # remove non alphanumeric character\n",
    "    text = re.sub(r'https?:/\\/\\S+', ' ', text) # remove links\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "twitter_dataset_path = \"/home/elkhand/datasets/twitter-sentiment/Sentiment_Analysis_Dataset.csv\" \n",
    "cleaned_twitter_dataset_path = \"/home/elkhand/datasets/twitter-sentiment/Cleaned_Sentiment_Analysis_Dataset.csv\" \n",
    "df = pd.read_csv(twitter_dataset_path, error_bad_lines=False)\n",
    "df = df[['SentimentText','Sentiment']]\n",
    "df[\"SentimentText\"] = df[\"SentimentText\"].apply(lambda x: tokenizer(x))\n",
    "df[\"SentimentText\"] = df[\"SentimentText\"].apply(lambda x: \" \".join(x))\n",
    "df[\"SentimentText\"] = df[\"SentimentText\"].apply(lambda x: \" \".join(get_words_withoutstopwords(x.lower().split())))\n",
    "df.columns = ['caption','label']\n",
    "df = df.dropna()\n",
    "df.to_csv(cleaned_twitter_dataset_path, index=False)\n",
    "print(\"df.shape\", df.shape)\n",
    "print(\"df.head()\", df.head())\n",
    "\n",
    "fig = plt.figure(figsize=(8,5))\n",
    "ax = sns.barplot(x=df.label.unique(),y=df.label.value_counts());\n",
    "ax.set(xlabel='Labels');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train on Twitter dataset, validate on Image captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_twitter_dataset_path = \"/home/elkhand/datasets/twitter-sentiment/Cleaned_Sentiment_Analysis_Dataset.csv\"\n",
    "\n",
    "\n",
    "def build_model(max_seq_len, num_of_classes, embedding_dim, hidden_layer_dim, dropout, recurrent_dropout, final_activation): \n",
    "    # Cross-validation results: 0.61% (+/- 0.13%)\n",
    "    model = Sequential()\n",
    "    model.add(Masking(mask_value=0., input_shape=(None, embedding_dim)))\n",
    "    #, dropout=dropout, recurrent_dropout=recurrent_dropout\n",
    "    model.add(GRU(max_seq_len,return_sequences=True, dropout=dropout, recurrent_dropout=recurrent_dropout))\n",
    "    #model.add(GRU(max_seq_len // 2,return_sequences=True, dropout=dropout, recurrent_dropout=recurrent_dropout))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(GRU(max_seq_len, dropout=dropout, recurrent_dropout=recurrent_dropout)) \n",
    "#     model.add(Dropout(dropout))\n",
    "    \n",
    "    \n",
    "    model.add(Dense(num_of_classes, activation=final_activation))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', \n",
    "                      metrics=['accuracy', sc.f1, sc.recall, sc.precision])#\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "\n",
    "def convert_twitter_labels(x):\n",
    "    \"\"\"twitter has 0 for negative, 1 for positive\"\"\"\n",
    "    if x == 0:\n",
    "        return -1\n",
    "    elif x == 1 :\n",
    "        return 1\n",
    "    else:\n",
    "        raise \"Unexpectedvalue\" + x\n",
    "\n",
    "def twitter_train_StratifiedKFold(config):\n",
    "    \"\"\"StratifiedKFold cross-validation to solve class imbalance issue\"\"\"\n",
    "    \n",
    "    dfTwitter = pd.read_csv(cleaned_twitter_dataset_path, header=0)\n",
    "    print(dfTwitter.head())\n",
    "    dfTwitter.columns = ['caption', 'label']\n",
    "    dfTwitter = dfTwitter.dropna()\n",
    "    dfTwitterPos = dfTwitter[dfTwitter.label == 1]\n",
    "    dfTwitterPos = dfTwitterPos[:5000]\n",
    "    dfTwitterNeg = dfTwitter[dfTwitter.label == 0]\n",
    "    dfTwitterNeg = dfTwitterNeg[:5000]\n",
    "    dfTwitter =  pd.concat([dfTwitterNeg, dfTwitterPos]) \n",
    "\n",
    "    print(\"dfTwitter.count()\",dfTwitter.count())\n",
    "    print(\"dfTwitter Label distribution: \",dfTwitter.groupby(\"label\").label.count())\n",
    "    dfTwitter[\"label\"] = dfTwitter[\"label\"].apply(lambda x: convert_twitter_labels(x))\n",
    "    print(\"dfTwitter Label distribution: \",dfTwitter.groupby(\"label\").label.count())\n",
    "    \n",
    "    dfCaption = pd.read_csv(dataset_path, header=0, sep=\"|\")   \n",
    "\n",
    "    print(\"Label distribution: \",dfCaption.groupby('label').label.count())\n",
    "    max_seq_len_captions = int(dfCaption['caption'].map(lambda x: caput.get_non_stop_word_count(x.split())).max())\n",
    "    print(\"max_seq_len_captions\", max_seq_len_captions)\n",
    "    \n",
    "    max_seq_len_tweets = int(dfTwitter['caption'].map(lambda x: len(x.split())).max())\n",
    "    print(\"max_seq_len_tweets\", max_seq_len_tweets)\n",
    "    \n",
    "    max_seq_len = max(max_seq_len_captions,max_seq_len_tweets)\n",
    "    print(\"max_seq_len\", max_seq_len)\n",
    "    \n",
    "    \n",
    "    \n",
    "    X = dfCaption['caption']\n",
    "    y = dfCaption['label']\n",
    "\n",
    "    final_activation ='softmax'\n",
    "    print(\"final_activation\",final_activation)\n",
    "    \n",
    "    \n",
    "    # Instantiate the cross validator\n",
    "    skf = StratifiedKFold(n_splits=kfold_splits, shuffle=True)\n",
    "    printCnt = 5\n",
    "    cv_accuracies = []\n",
    "    cv_f1s = []\n",
    "    \n",
    "    # Loop through the indices the split() method returns\n",
    "    for index, (train_indices, val_indices) in enumerate(skf.split(X, y)):\n",
    "        \n",
    "        X_train_caption, X_val_caption = X[train_indices], X[val_indices]\n",
    "        y_train_caption, y_val_caption = y[train_indices], y[val_indices]\n",
    "        \n",
    "        dfCaptionTrain = pd.concat([X_train_caption, y_train_caption], axis=1) \n",
    "        dfCaptionTrain[\"label\"] = dfCaptionTrain[\"label\"].apply(lambda x: caput.change_label_str_to_int(x))\n",
    "        dfTweetCaptionTrain = pd.concat([dfTwitter, dfCaptionTrain])               \n",
    "        \n",
    "        #X_train, y_train_index, num_of_classes = caput.load_dataset_StratifiedKFold(dfTweetCaptionTrain, max_seq_len)\n",
    "        \n",
    "        class_to_index = {}\n",
    "        index_to_class = {}\n",
    "        X_train, y_train_index, num_of_classes, class_to_index, index_to_class = \\\n",
    "            caput.load_dataset_StratifiedKFold(\n",
    "                            dfTweetCaptionTrain,\n",
    "                            wordToVec, \n",
    "                            max_seq_len, \n",
    "                            class_to_index, \n",
    "                            index_to_class,\n",
    "                            config)\n",
    "        \n",
    "        y_train = caput.convert_index_to_one_hot(y_train_index, num_of_classes)\n",
    "        print(\"Train label distribution: \",dfTweetCaptionTrain.groupby('label').label.count())\n",
    "        print(\"num_of_classes\",num_of_classes)\n",
    "        \n",
    "        y_ints = [y.argmax() for y in y_train]\n",
    "        class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(y_ints),\n",
    "                                                 y_ints)\n",
    "        print(\"class_weights\", class_weights)\n",
    "        print(\"class_to_index\", class_to_index)\n",
    "        \n",
    "        \n",
    "        dfCaptionVal = pd.concat([X_val_caption, y_val_caption], axis=1) \n",
    "        dfCaptionVal[\"label\"] = dfCaptionVal[\"label\"].apply(lambda x: caput.change_label_str_to_int(x))\n",
    "        #X_val, y_val_index, _ = load_dataset_StratifiedKFold(dfCaptionVal,max_seq_len)\n",
    "        X_val, y_val_index, _, _, _ = caput.load_dataset_StratifiedKFold(\n",
    "                            dfCaptionVal,\n",
    "                            wordToVec,\n",
    "                            max_seq_len, \n",
    "                            class_to_index, \n",
    "                            index_to_class,\n",
    "                            config)\n",
    "        \n",
    "        \n",
    "        \n",
    "        y_val = caput.convert_index_to_one_hot(y_val_index, num_of_classes) \n",
    "        print(\"Val label distribution: \",dfCaptionVal.groupby('label').label.count())       \n",
    "\n",
    "        model = build_model(max_seq_len,\n",
    "                                num_of_classes, \n",
    "                                embedding_dim=config['embedding_dimension'], \n",
    "                                hidden_layer_dim=config['hidden_layer_dim'], \n",
    "                                dropout=config['dropout'], \n",
    "                                recurrent_dropout=config['recurrent_dropout'],# 0.3\n",
    "                                final_activation=final_activation                                \n",
    "                                )\n",
    "        plot_model(model, to_file= 'model/model.png', show_shapes=True, show_layer_names=True)#\n",
    "\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', \n",
    "                                  factor=0.5, \n",
    "                                  patience=2, \n",
    "                                  min_lr=1e-7,\n",
    "                                  cooldown=1,\n",
    "                                  verbose=1)\n",
    "\n",
    "        history = {}\n",
    "        filename = \"\"\n",
    "        # checkpoint\n",
    "        filepath=\"model/weights.best.h5\"\n",
    "        checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=0, save_best_only=True, mode='max')\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=5)    \n",
    "        callbacks_list = [ checkpoint, early_stopping, reduce_lr]  #\n",
    "        history = model.fit(x=X_train,\n",
    "                          y=y_train, \n",
    "                          batch_size=config['batch_size'],# 64 seems fine, 32 is better \n",
    "                          epochs=config['nb_epochs'], \n",
    "                          verbose=config['verbose'], \n",
    "                          validation_data = (X_val, y_val),\n",
    "                          shuffle=True,\n",
    "                          callbacks=callbacks_list,\n",
    "                          class_weight = class_weights)        \n",
    "        \n",
    "        val_acc_list = history.history['val_acc']\n",
    "        val_f1_list = history.history['val_f1']\n",
    "        best_val_acc =  max(val_acc_list)\n",
    "        best_f1 =  max(val_f1_list)\n",
    "        print(\"best_val_acc: \", best_val_acc)\n",
    "        print(\"best_f1: \", best_f1)\n",
    "        filename = \"hec\" \n",
    "        filename = \"model/\" + generate_model_name(filename, best_val_acc) + \".h5\"\n",
    "        os.rename(filepath, filename)\n",
    "        cv_accuracies.append(best_val_acc)\n",
    "        cv_f1s.append(best_f1)\n",
    "        plot_model_accuracy(history, \"model/\")\n",
    "        end = time.time()\n",
    "        print(\"Time passed for training\", (end-start)/60)\n",
    "    \n",
    "    print(\"=========================================\")\n",
    "    print(\"Cross-validation val accuracy results: \" , cv_accuracies)\n",
    "    print(\"Cross-validation val accuracy results: %.2f%% (+/- %.2f%%)\" % (np.mean(cv_accuracies), np.std(cv_accuracies)))\n",
    "    \n",
    "    print(\"\\n\",\"Cross-validation val f1 results: \" , cv_f1s)\n",
    "    print(\"Cross-validation val f1 results: %.2f%% (+/- %.2f%%)\" % (np.mean(cv_f1s), np.std(cv_f1s)))\n",
    "    \n",
    "    \n",
    "    return history\n",
    "\n",
    "\n",
    "\n",
    "def get_config():\n",
    "    conf = {}\n",
    "    kfold_splits = 7\n",
    "    embedding_dimension = 200\n",
    "    conf[\"kfold_splits\"] = kfold_splits\n",
    "    conf[\"batch_size\"] = batch_size\n",
    "    conf['embedding_dimension'] = embedding_dimension\n",
    "    conf['recurrent_dropout'] = recurrent_dropout\n",
    "    conf['dropout'] = dropout\n",
    "    conf[\"nb_epochs\"] = nb_epochs\n",
    "    conf['useF1Score'] = useF1Score\n",
    "    conf['verbose'] = 1\n",
    "    conf['useF1Score'] = useF1Score\n",
    "    conf['hidden_layer_dim'] = hidden_layer_dim\n",
    "    return conf   \n",
    "\n",
    "dataset_path = human_output_caption_to_label_csv_path\n",
    "# dataset_path = auto_output_caption_to_label_csv_path\n",
    "\n",
    "best_model = None\n",
    "def main():\n",
    "    global wordToVec, best_model\n",
    "    config = get_config()\n",
    "    print(\"config:\\n\", config)\n",
    "    if wordToVec is None:\n",
    "        wordToVec = load_embedding(fasttext_embedding_path)\n",
    "    print(\"# words: \", len(wordToVec.keys()),\" word vector dimension\", len(wordToVec[list(wordToVec.keys())[1]]))\n",
    "    best_model = history = twitter_train_StratifiedKFold(config) \n",
    "    # Evaluate Test data set\n",
    "    #evalaute_on_test_data(best_model, testDataset, inputDataset, wordToVec, config)\n",
    "    pass\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # execute only if run as a script\n",
    "    main()\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
