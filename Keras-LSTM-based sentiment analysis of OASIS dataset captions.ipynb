{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras based GRU Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset\n",
    "\n",
    "Reformat dataset to be int format\n",
    "```\n",
    "caption | valence_class\n",
    "caption | {negative,neutral,positive}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os, sys\n",
    "import re\n",
    "import string\n",
    "import pathlib\n",
    "import random\n",
    "from collections import Counter, OrderedDict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import hecutils.data_utils as dt\n",
    "import hecutils.scoring_utils as sc\n",
    "import hecutils.plotting_utils as pt\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "\n",
    "captions_root = \"/home/elkhand/git-repos/human-emotions-classifier/dataset/metadata\"\n",
    "oasis_csv_path = \"dataset/metadata/OASIS.csv\"\n",
    "captions_root_path = pathlib.Path(captions_root)\n",
    "\n",
    "human_output_caption_to_label_csv_path = captions_root_path/'humanCaptionWithLabeldf.csv'\n",
    "human_caption_csv_path = captions_root_path/'captions.csv'\n",
    "neutralLow = 3.0 \n",
    "neutralHigh = 5.0\n",
    "dt.create_caption_to_label(oasis_csv_path,human_caption_csv_path, human_output_caption_to_label_csv_path,neutralLow, neutralHigh)\n",
    "\n",
    "auto_output_caption_to_label_csv_path = captions_root_path/'autoCaptionWithLabeldf.csv'\n",
    "auto_caption_csv_path = captions_root_path/'auto_generated_captions.csv'\n",
    "neutralLow = 3.0 \n",
    "neutralHigh = 5.0\n",
    "dt.create_caption_to_label(oasis_csv_path,auto_caption_csv_path, auto_output_caption_to_label_csv_path,neutralLow, neutralHigh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Keras to use Tensorflow GPU in the backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elkhand/anaconda3/envs/cs231n/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "\n",
    "num_cores = 8\n",
    "GPU=True\n",
    "CPU = not GPU\n",
    "\n",
    "if GPU:\n",
    "    num_GPU = 2\n",
    "    num_CPU = 8\n",
    "if CPU:\n",
    "    num_CPU = 1\n",
    "    num_GPU = 0\n",
    "\n",
    "config = tf.ConfigProto(intra_op_parallelism_threads=num_cores,\\\n",
    "        inter_op_parallelism_threads=num_cores, allow_soft_placement=True,\\\n",
    "        device_count = {'CPU' : num_CPU, 'GPU' : num_GPU})\n",
    "session = tf.Session(config=config)\n",
    "K.set_session(session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Fasttext Embeddings\n",
    "\n",
    "You can download fasttext word vectors from here:\n",
    "\n",
    "https://fasttext.cc/docs/en/english-vectors.html    \n",
    "https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki-news-300d-1M.vec.zip\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding size : 999995\n",
      "embedding dimension : (300,)\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import MaxPooling1D\n",
    "from keras.layers import GRU, Bidirectional, LSTM\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers.core import Masking\n",
    "from keras.layers.core import  ActivityRegularization\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from keras.utils import plot_model\n",
    "from keras.utils import multi_gpu_model\n",
    "from keras import regularizers\n",
    "from keras.regularizers import L1L2\n",
    "import keras\n",
    "from sklearn.model_selection import KFold\n",
    "import os\n",
    "import io\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from gensim.models import word2vec\n",
    "import pandas as pd\n",
    "# from fastText import load_model\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from keras.preprocessing import text\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "def load_embedding(path):\n",
    "    word2vec = {}\n",
    "    with io.open(path, encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            entries = line.rstrip().split(\" \")\n",
    "            word, entries = entries[0], entries[1:]\n",
    "            word2vec[word] = np.array(entries).astype(np.float) # Convert String type to float\n",
    "    print('embedding size : %d' % len(word2vec))\n",
    "    print('embedding dimension : %s' % (word2vec['apple'].shape,))\n",
    "    return word2vec\n",
    "    \n",
    "wordToVec = {}\n",
    "\n",
    "\n",
    "fasttext_embedding_path = 'embedding/wiki-news-300d-1M.vec'\n",
    "wordToVec = load_embedding(fasttext_embedding_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Epoch 300/300\n",
    "663/663 [==============================] - 2s 3ms/step - loss: 0.1011 - acc: 0.9789 - val_loss: 1.9239 - val_acc: 0.6426\n",
    "\n",
    "best_val_acc:  0.7148936180358237\n",
    "filename adidas-0.7149-1527322080\n",
    "Total time passed for training 9.198103360335033\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from keras.layers.core import  ActivityRegularization\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import MaxPooling1D\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers import GRU, Bidirectional, LSTM\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def plot_model_accuracy(history):\n",
    "    \"\"\"plot acc and loss for train and val\"\"\"\n",
    "    filename = \"hec\" \n",
    "    filename = generate_model_name(filename + \"-acc\", max(history.history['val_acc']))\n",
    "    fig = plt.figure()\n",
    "    print(history.history.keys())\n",
    "    print(\"best_val_acc\", max(history.history['val_acc']))\n",
    "    print(\"best_train_acc\", max(history.history['acc']))\n",
    "    print(\"lowest_val_loss\", min(history.history['val_loss']))\n",
    "    print(\"lowest_train_loss\", min(history.history['loss']))\n",
    "    #  \"Accuracy\"\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.show()\n",
    "    fig.savefig(\"model/\" + filename + \".png\") \n",
    "    \n",
    "    # \"Loss\"\n",
    "    fig = plt.figure()\n",
    "    filename = \"hec\" \n",
    "    filename = generate_model_name(filename + \"-loss\", min(history.history['val_loss']))\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.show()\n",
    "    fig.savefig(\"model/\" + filename + \".png\") \n",
    "\n",
    "def build_model(max_seq_len, num_of_classes, embedding_dim, hidden_layer_dim, dropout, recurrent_dropout, final_activation): \n",
    "    shouldUnroll = False\n",
    "    model = Sequential()\n",
    "    model.add(Masking(mask_value=0., input_shape=(None, embedding_dim)))\n",
    "    print(\"max_seq_len\", max_seq_len)\n",
    "#     model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "#     model.add(MaxPooling1D(pool_size=2))\n",
    "#     model.add(Flatten())\n",
    "#     model.add(Dense(250, activation='relu'))\n",
    "    \n",
    "    \n",
    "    # ,dropout=dropout, recurrent_dropout=recurrent_dropout\n",
    "    #, unroll=shouldUnroll\n",
    "#     model.add(Bidirectional(GRU(max_seq_len, return_sequences=True, recurrent_dropout=recurrent_dropout), merge_mode='concat'))\n",
    "#     model.add(Bidirectional(GRU(max_seq_len // 2, return_sequences=True, recurrent_dropout=recurrent_dropout), merge_mode='concat'))\n",
    "#     model.add(Bidirectional(GRU(max_seq_len // 4, dropout=dropout, recurrent_dropout=recurrent_dropout), merge_mode='concat'))    \n",
    "    \n",
    "    model.add(Bidirectional(GRU(max_seq_len,activation='relu', return_sequences=True, dropout=dropout, recurrent_dropout=recurrent_dropout)))\n",
    "    model.add(Bidirectional(GRU(max_seq_len,activation='relu', return_sequences=True, dropout=dropout, recurrent_dropout=recurrent_dropout), merge_mode='mul'))\n",
    "    model.add(Bidirectional(GRU(max_seq_len,activation='relu', dropout=dropout, recurrent_dropout=recurrent_dropout))) \n",
    "    \n",
    "    # Adding new dense layers\n",
    "    #, kernel_regularizer=regularizers.l2(0.001)\n",
    "#     model.add(Dense(hidden_layer_dim * 8,  activation='relu'))\n",
    "#     model.add(Dropout(dropout))\n",
    "#     model.add(Dense(hidden_layer_dim * 4,  activation='relu'))\n",
    "#     model.add(Dropout(dropout))\n",
    "#     model.add(Dense(hidden_layer_dim * 2,  activation='relu'))\n",
    "#     model.add(Dropout(dropout))\n",
    "    #model.add(ActivityRegularization(l1=0.01, l2=0.001))\n",
    "\n",
    "\n",
    "    model.add(Dense(num_of_classes, activation=final_activation))\n",
    "    nadamOptim = keras.optimizers.Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_word_embedding(word):\n",
    "    word = word.lower()\n",
    "    if word in wordToVec:\n",
    "        return wordToVec[word]\n",
    "    else:\n",
    "        return np.zeros(embedding_dimension,)\n",
    "    \n",
    "def get_sequence_embedding(words, max_seq_len):\n",
    "    if len(words) <= max_seq_len:\n",
    "        # Add padding\n",
    "        x_seq = np.array([get_word_embedding(word) for word in words])\n",
    "        x_seq = np.lib.pad(x_seq, ((0,max_seq_len-x_seq.shape[0]),(0,0)), 'constant')\n",
    "    else:\n",
    "        x_seq = []\n",
    "        for i in range(max_seq_len):\n",
    "            x_seq.append(get_word_embedding(words[i]))\n",
    "        x_seq = np.array(x_seq)\n",
    "    return x_seq\n",
    "        \n",
    "def convert_index_to_one_hot(y_train_index, num_of_classes):\n",
    "    y_train = np.zeros((y_train_index.shape[0],num_of_classes))\n",
    "    y_train[range(y_train_index.shape[0]),y_train_index] = 1\n",
    "    return y_train\n",
    "\n",
    "\n",
    "def generate_model_name(filename, best_acc_val):\n",
    "    timestamp = str(time.time()).split(\".\")[0]\n",
    "    best_acc_val = round(best_acc_val,4)\n",
    "    filename += \"-\" + str(best_acc_val) + \"-\" + timestamp\n",
    "    return filename\n",
    "\n",
    "def get_words_withoutstopwords(words):\n",
    "    \n",
    "    words_without_stopwords = []\n",
    "    for word in words:\n",
    "        if word not in stop_words:\n",
    "            words_without_stopwords.append(word)\n",
    "    return words_without_stopwords\n",
    "\n",
    "def load_dataset_StratifiedKFold(dfKFold, max_seq_len):\n",
    "    sentences = []\n",
    "    label_index = []\n",
    "    num_of_classes = 0\n",
    "    class_to_index = {}\n",
    "    index_to_class = {}\n",
    "    for index, row in dfKFold.iterrows():\n",
    "        caption = row['caption']\n",
    "        label = row['label']\n",
    "        sentence = caption\n",
    "        words = sentence.split(\" \")\n",
    "        words = get_non_stop_words(words)\n",
    "        sentence_embedding = get_sequence_embedding(words, max_seq_len)\n",
    "        sentences.append(sentence_embedding)\n",
    "        if label in class_to_index:\n",
    "            label_index.append(class_to_index[label])\n",
    "        else:\n",
    "            num_of_classes += 1\n",
    "            class_to_index[label] = num_of_classes - 1\n",
    "            index_to_class[num_of_classes - 1] = label\n",
    "            label_index.append(class_to_index[label])\n",
    "    X_train = np.array(sentences)\n",
    "    y_train = np.array(label_index)\n",
    "    return (X_train, y_train, num_of_classes)\n",
    "\n",
    "\n",
    "def get_non_stop_word_count(words):\n",
    "    count = 0\n",
    "    for word in words:\n",
    "        if word not in stop_words:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "\n",
    "def get_non_stop_words(words):\n",
    "    non_stop_words = []\n",
    "    for word in words:\n",
    "        if word not in stop_words:\n",
    "            non_stop_words.append(word)\n",
    "    return non_stop_words\n",
    "\n",
    "def train_StratifiedKFold():\n",
    "    \"\"\"StratifiedKFold cross-validation to solve class imbalance issue\"\"\"\n",
    "    df = pd.read_csv(dataset_path, sep=\"|\")\n",
    "    X = df['caption']\n",
    "    y = df['label']\n",
    "    print(\"Label distribution: \",df.groupby('label').label.count())\n",
    "    max_seq_len = int(df['caption'].map(lambda x: get_non_stop_word_count(x.split())).max())\n",
    "    print(\"max_seq_len\", max_seq_len)\n",
    "    # Instantiate the cross validator\n",
    "    skf = StratifiedKFold(n_splits=kfold_splits, shuffle=True)\n",
    "    printCnt = 5\n",
    "    cvscores = []\n",
    "    # Loop through the indices the split() method returns\n",
    "    for index, (train_indices, val_indices) in enumerate(skf.split(X, y)):\n",
    "        start = time.time()\n",
    "        print(\"\\nTRAIN size:\", len(train_indices), \"\\t VAL size:\", len(val_indices))\n",
    "        print(\"TRAIN[:5]:\", train_indices[:printCnt], \"\\t VAL[:5]:\", val_indices[:printCnt])\n",
    "        print(\"TRAIN[-5:]:\", train_indices[-printCnt:], \"\\t VAL[-5:]:\", val_indices[-printCnt:])\n",
    "        X_train, X_val = X[train_indices], X[val_indices]\n",
    "        y_train_index, y_val_index = y[train_indices], y[val_indices]\n",
    "        dfTrain = pd.concat([X_train, y_train_index], axis=1)\n",
    "        dfTrain.columns = ['caption', 'label']\n",
    "        X_train, y_train_index, num_of_classes = load_dataset_StratifiedKFold(dfTrain, max_seq_len)\n",
    "        y_train = convert_index_to_one_hot(y_train_index, num_of_classes) \n",
    "        print(X_train[0])\n",
    "        print(y_train[0])\n",
    "        print(\"Train label distribution: \",dfTrain.groupby('label').label.count())\n",
    "        dfVal= pd.concat([X_val, y_val_index], axis=1)\n",
    "        dfVal.columns = ['caption', 'label']\n",
    "        \n",
    "        X_val, y_val_index, _ = load_dataset_StratifiedKFold(dfVal,max_seq_len)\n",
    "        y_val = convert_index_to_one_hot(y_val_index, num_of_classes) \n",
    "        print(\"Val label distribution: \",dfVal.groupby('label').label.count())\n",
    "        \n",
    "        model = build_model(max_seq_len,\n",
    "                            num_of_classes, \n",
    "                            embedding_dim=300, \n",
    "                            hidden_layer_dim=64, \n",
    "                            dropout=0.3, \n",
    "                            recurrent_dropout=0.5,# 0.3\n",
    "                            final_activation='softmax'\n",
    "                            )\n",
    "        plot_model(model, to_file= 'model/model.png', show_shapes=True, show_layer_names=True)#\n",
    "       \n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', \n",
    "                                  factor=0.2, \n",
    "                                  patience=5, \n",
    "                                  min_lr=0.00001)\n",
    "    \n",
    "        history = {}\n",
    "        filename = \"\"\n",
    "        # checkpoint\n",
    "        filepath=\"model/weights.best.h5\"\n",
    "        checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=0, save_best_only=True, mode='max')\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=25)\n",
    "        callbacks_list = [ checkpoint, early_stopping] # reduce_lr,\n",
    "        history = model.fit(x=X_train,\n",
    "                      y=y_train, \n",
    "                      batch_size=16,# 64 seems fine, 32 is better \n",
    "                      epochs=100, \n",
    "                      verbose=1, \n",
    "                      validation_data = (X_val, y_val),\n",
    "                      #shuffle=True,\n",
    "                      callbacks=callbacks_list)        \n",
    "        val_acc_list = history.history['val_acc']\n",
    "        best_val_acc =  max(val_acc_list)\n",
    "        filename = \"hec\" \n",
    "        filename = \"model/\" + generate_model_name(filename, best_val_acc) + \".h5\"\n",
    "        os.rename(filepath, filename)\n",
    "        print(\"Metric names: \", model.metrics_names)\n",
    "        cvscores.append(best_val_acc)\n",
    "\n",
    "        print(\"best_val_acc: \", best_val_acc)\n",
    "        print(\"filename\",filename)\n",
    "        #plot_model(model, to_file= filename +'.png', show_shapes=True, show_layer_names=True)#\n",
    "        plot_model_accuracy(history)\n",
    "        end = time.time()\n",
    "        print(\"Time passed for training\", (end-start)/60)\n",
    "    print(\"Cross-validation results: %.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))\n",
    "    return history\n",
    "\n",
    "\n",
    "dataset_path = human_output_caption_to_label_csv_path\n",
    "kfold_splits = 10\n",
    "# dataset_path = auto_output_caption_to_label_csv_path\n",
    "\n",
    "\n",
    "embedding_dimension = 300\n",
    "class_to_index = {}\n",
    "index_to_class = {}\n",
    "#max_seq_len = 40\n",
    "start = time.time()\n",
    "history = train_StratifiedKFold() \n",
    "#dataset_path = caption_to_label_dataset_path + \"/\" +'human-train.csv'\n",
    "#val_dataset_path = caption_to_label_dataset_path + \"/\" +'human-val.csv'\n",
    "#history = train()\n",
    "end = time.time()\n",
    "print(\"Total time passed for training\", (end-start)/60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Classifier for smaller dataset\n",
    "\n",
    "http://www.developintelligence.com/blog/2017/06/practical-neural-networks-keras-classifying-yelp-reviews/\n",
    "\n",
    "\n",
    "Conversely, for smaller datasets, the SVM is much better than the neural network.\n",
    "The SVM, on the other hand, will perform well even for much smaller datasets.\n",
    "\n",
    "Now, instead of converting each word to a single number and learning an Embedding layer, we use a term-frequency inverse document frequency (TF-IDF) vectorisation process. Using this vectorisation scheme, we ignore the order of the words completely, and represent each review as a large sparse matrix, with each cell representing a specific word, and how often it appears in that review. We normalize the counts by the total number of times the word appears in all of the reviews, so rare words are given a higher importance than common ones (though we ignore all words that aren’t seen in at least three different reviews.\n",
    "\n",
    "    Line six sets up the vectorizer. We set ngram_range to (1,2) which means we’ll consider all words on their own but also look at all pairs of words. This is useful because we don’t have a concept of word order anymore, so looking at pairs of words as single tokens allows the classifier to learn that word pairs such as “not good” are usually negative, even though “good” is positive. We also set min_df to 3, which means that we’ll ignore words that aren’t seen at least three times (in three different reviews)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 420, 1: 317, -1: 163})\n",
      "0:00:00.051442\n",
      "(900, 21279)\n",
      "0:00:01.878837\n",
      "[0.4640884  0.51933702 0.47777778 0.51955307 0.58659218]\n",
      "0.5134696887078134\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You are passing a target array of shape (900, 1) while using as loss `categorical_crossentropy`. `categorical_crossentropy` expects targets to be binary matrices (1s and 0s) of shape (samples, classes). If your targets are integer classes, you can convert them to the expected format via:\n```\nfrom keras.utils import to_categorical\ny_binary = to_categorical(y_int)\n```\n\nAlternatively, you can use the loss function `sparse_categorical_crossentropy` instead, which does expect integer targets.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-072351ca4105>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;31m# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbalanced_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0mplot_model_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cs231n/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    961\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 963\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/cs231n/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1628\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1629\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1630\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1631\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1632\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cs231n/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m   1491\u001b[0m         _check_loss_and_target_compatibility(y,\n\u001b[1;32m   1492\u001b[0m                                              \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_loss_fns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1493\u001b[0;31m                                              self._feed_output_shapes)\n\u001b[0m\u001b[1;32m   1494\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1495\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cs231n/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_check_loss_and_target_compatibility\u001b[0;34m(targets, loss_fns, output_shapes)\u001b[0m\n\u001b[1;32m    254\u001b[0m                 raise ValueError(\n\u001b[1;32m    255\u001b[0m                     \u001b[0;34m'You are passing a target array of shape '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m                     \u001b[0;34m' while using as loss `categorical_crossentropy`. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m                     \u001b[0;34m'`categorical_crossentropy` expects '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m                     \u001b[0;34m'targets to be binary matrices (1s and 0s) '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: You are passing a target array of shape (900, 1) while using as loss `categorical_crossentropy`. `categorical_crossentropy` expects targets to be binary matrices (1s and 0s) of shape (samples, classes). If your targets are integer classes, you can convert them to the expected format via:\n```\nfrom keras.utils import to_categorical\ny_binary = to_categorical(y_int)\n```\n\nAlternatively, you can use the loss function `sparse_categorical_crossentropy` instead, which does expect integer targets."
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "\n",
    "import json\n",
    "\n",
    "from keras.layers import Embedding, LSTM, Dense, Conv1D, MaxPooling1D, Dropout, Activation\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "dataset_path = human_output_caption_to_label_csv_path\n",
    "\n",
    "\n",
    "def change_label_str_to_int(labelStr):\n",
    "    if labelStr == \"negative\":\n",
    "        return -1\n",
    "    elif  labelStr == \"neutral\":\n",
    "        return 0\n",
    "    elif  labelStr == \"positive\":\n",
    "        return 1\n",
    "\n",
    "df = pd.read_csv(dataset_path, sep=\"|\")\n",
    "df[\"label\"] = df[\"label\"].apply(lambda x: change_label_str_to_int(x))\n",
    "#get_words_withoutstopwords\n",
    "df[\"caption\"] = df[\"caption\"].apply(lambda x: \" \".join(get_words_withoutstopwords(x.lower().split())))\n",
    "\n",
    "balanced_texts = []\n",
    "balanced_labels = []\n",
    "limit = 100000  # Change this to grow/shrink the dataset\n",
    "# -1 label is at index 0, 0 is at index 1, 1 is at ndex 2.\n",
    "neg_pos_counts = [0, 0, 0]\n",
    "for index, row in df.iterrows():\n",
    "    label = int(row['label'])\n",
    "    caption = row['caption']\n",
    "    if neg_pos_counts[label + 1] < limit:\n",
    "        balanced_texts.append(caption)\n",
    "        balanced_labels.append(label)\n",
    "        neg_pos_counts[label + 1] += 1\n",
    "        \n",
    "print(Counter(balanced_labels))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "t1 = datetime.now()\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,4), min_df=0)\n",
    "classifier = LinearSVC()\n",
    "Xs = vectorizer.fit_transform(balanced_texts)\n",
    "\n",
    "print(datetime.now() - t1)\n",
    "print(Xs.shape)\n",
    "\n",
    "score = cross_val_score(classifier, Xs, balanced_labels, cv=5, n_jobs=-1)\n",
    "\n",
    "print(datetime.now() - t1)\n",
    "print(score)\n",
    "print(sum(score) / len(score))\n",
    "\n",
    "\n",
    "##############################################################\n",
    "# New Neural Network with Embedding layer\n",
    "\n",
    "tokenizer = Tokenizer(num_words=20000)\n",
    "tokenizer.fit_on_texts(balanced_texts)\n",
    "sequences = tokenizer.texts_to_sequences(balanced_texts)\n",
    "data = pad_sequences(sequences, maxlen=300)\n",
    "\n",
    "#This might take a while to run. Here, we use the most common 20000 words instead of 5. \n",
    "#The only other difference is that we pass maxlen=300 when we pad the sequences. \n",
    "#This means that as well as padding the very short texts with zeros,\n",
    "#we’ll also truncate the very long ones. All of our texts will then be represented by 300 numbers.\n",
    "\n",
    "\n",
    "filepath=\"model/weights.best.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=0, save_best_only=True, mode='max')\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=25)\n",
    "callbacks_list = [ checkpoint, early_stopping] # reduce_lr,\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(20000, 128, input_length=300))\n",
    "# model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(20000, 128, input_length=300))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "#sparse_categorical_crossentropy\n",
    "#categorical_crossentropy\n",
    "\n",
    "# model.add(Bidirectional(GRU(max_seq_len,activation='relu', return_sequences=True, dropout=dropout, recurrent_dropout=recurrent_dropout)))\n",
    "# model.add(Bidirectional(GRU(max_seq_len,activation='relu', return_sequences=True, dropout=dropout, recurrent_dropout=recurrent_dropout), merge_mode='mul'))\n",
    "# model.add(Bidirectional(GRU(max_seq_len,activation='relu', dropout=dropout, recurrent_dropout=recurrent_dropout))) \n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "#In line two, we add an Embedding layer. This layer lets the network expand each token to a larger vector, \n",
    "#allowing the network to represent words in a meaningful way. We pass 20000 as the first argument, \n",
    "#which is the size of our vocabulary (remember, we told the tokenizer to only use the 20 000 most common words \n",
    "#earlier), and 128 as the second, which means that each token can be expanded to a vector of size 128. \n",
    "#We give it an input_length of 300, which is the length of each of our sequences.\n",
    "\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(20000, 128, input_length=300))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Conv1D(64, 5, activation='relu'))\n",
    "# model.add(MaxPooling1D(pool_size=4))\n",
    "# model.add(LSTM(128))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(data, np.array(balanced_labels), validation_split=0.2, epochs=100, callbacks=callbacks_list)\n",
    "plot_model_accuracy(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-fold cross validation\n",
    "\n",
    "As this vanilla k-fold does not take into account the class imbalance, is not used any more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_dataset(file_path):\n",
    "#     df = pd.read_csv(file_path, sep=\"|\")\n",
    "#     sentences = []\n",
    "#     label_index = []\n",
    "#     num_of_classes = 0\n",
    "#     for i in range(len(df)):\n",
    "#         sentence = df.iloc[:,0][i]  # first column of data frame\n",
    "#         words = sentence.split(\" \")\n",
    "#         sentence_embedding = get_sequence_embedding(words, max_seq_len)\n",
    "#         sentences.append(sentence_embedding)\n",
    "#         label = df.iloc[:,1][i]  # second column of data frame\n",
    "#         if label in class_to_index:\n",
    "#             label_index.append(class_to_index[label])\n",
    "#         else:\n",
    "#             num_of_classes += 1\n",
    "#             class_to_index[label] = num_of_classes - 1\n",
    "#             index_to_class[num_of_classes - 1] = label\n",
    "#             label_index.append(class_to_index[label])\n",
    "#     X_train = np.array(sentences)\n",
    "#     y_train = np.array(label_index)\n",
    "#     return (X_train, y_train, num_of_classes)\n",
    "\n",
    "\n",
    "# def train():\n",
    "#     X_train, y_train_index, num_of_classes = load_dataset(dataset_path)\n",
    "#     y_train = convert_index_to_one_hot(y_train_index, num_of_classes) \n",
    "    \n",
    "#     print('X_train shape : %s' % (X_train.shape,))\n",
    "#     print('y_train shape : %s' % (y_train.shape,))\n",
    "#     print('number of classes : %d' % num_of_classes)\n",
    "#     print(X_train[0])\n",
    "#     print(y_train[0])\n",
    "#     print(index_to_class)\n",
    "\n",
    "#     X_val, y_val_index, _ = load_dataset(val_dataset_path)\n",
    "#     y_val = convert_index_to_one_hot(y_val_index, num_of_classes) \n",
    "    \n",
    "#     model = _build_model(num_of_classes, \n",
    "#                          embedding_dim=300, \n",
    "#                          hidden_layer_dim=64, \n",
    "# #                          dropout=0.65, \n",
    "# #                          recurrent_dropout=0.65,\n",
    "#                          dropout=0.7, \n",
    "#                          recurrent_dropout=0.5,\n",
    "#                          final_activation='softmax')\n",
    "    \n",
    "#     reduce_lr = ReduceLROnPlateau(monitor='val_loss', \n",
    "#                                   factor=0.03, \n",
    "#                                   patience=15, \n",
    "#                                   min_lr=0.00001, verbose=1)\n",
    "    \n",
    "#     history = {}\n",
    "#     filename = \"\"\n",
    "#     # checkpoint\n",
    "#     filepath=\"model/weights.best.h5\"\n",
    "#     checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "#     early_stopping = EarlyStopping(monitor='val_loss', patience=20)\n",
    "#     callbacks_list = [ checkpoint, early_stopping] #reduce_lr, \n",
    "#     with tf.Session() as sess:\n",
    "#         history = model.fit(x=X_train,\n",
    "#                   y=y_train, \n",
    "#                   batch_size=32, \n",
    "#                   epochs=300, \n",
    "#                   verbose=1, \n",
    "#                   validation_data = (X_val, y_val),\n",
    "#                   shuffle=True\n",
    "#                   ,callbacks=callbacks_list)        \n",
    "#         val_acc_list = history.history['val_acc']\n",
    "#         best_val_acc =  max(val_acc_list)\n",
    "#         filename = \"hec\" \n",
    "#         filename = \"model/\" + generate_model_name(filename, best_val_acc) + \".h5\"\n",
    "#         os.rename(filepath, filename)\n",
    "    \n",
    "#     best_val_acc = -1\n",
    "#     val_acc_list = history.history['val_acc']\n",
    "#     best_val_acc =  max(val_acc_list)\n",
    "#     print(\"best_val_acc: \", best_val_acc)\n",
    "#     #filename = generate_model_name(filename, best_val_acc)\n",
    "#     print(\"filename\",filename)\n",
    "#     plot_model(model, to_file= filename +'.png', show_shapes=True, show_layer_names=True)#\n",
    "#     return history\n",
    "\n",
    "\n",
    "\n",
    "# # Not sued, as this cross validation does not take into account the class inbalance\n",
    "# ## Create k-fold cross validation\n",
    "# from sklearn.model_selection import KFold\n",
    "# dataset_path = human_output_caption_to_label_csv_path\n",
    "# class_to_index = {}\n",
    "# index_to_class = {}\n",
    "# kfold_splits = 5\n",
    "# # dataset_path = auto_output_caption_to_label_csv_path\n",
    "# def train_k_fold():\n",
    "#     X, y_index, num_of_classes = load_dataset(dataset_path)\n",
    "#     print('X shape : %s' % (X.shape,))\n",
    "#     print('y_index shape : %s' % (y_index.shape,))\n",
    "#     print('number of classes : %d' % num_of_classes)\n",
    "\n",
    "#     y = convert_index_to_one_hot(y_index, num_of_classes) \n",
    "#     print('y shape : %s' % (y.shape,))\n",
    "#     print(X[0])\n",
    "#     print(y[0])\n",
    "#     print(index_to_class)\n",
    "    \n",
    "#     kf = KFold(n_splits=kfold_splits)\n",
    "#     kf.get_n_splits(X)\n",
    "#     print(kf)  \n",
    "#     cvscores = []\n",
    "#     printCnt = 5\n",
    "#     for train_index, val_index in kf.split(X):\n",
    "#         print(\"TRAIN:\", len(train_index), \"VAL:\", len(val_index))\n",
    "#         print(\"TRAIN:\", train_index[:printCnt], \"VAL:\", val_index[:printCnt])\n",
    "#         print(\"TRAIN:\", train_index[-printCnt:], \"VAL:\", val_index[-printCnt:])\n",
    "#         X_train, X_val = X[train_index], X[val_index]\n",
    "#         y_train, y_val = y[train_index], y[val_index]\n",
    "#         model = _build_model(num_of_classes, \n",
    "#                              embedding_dim=300, \n",
    "#                              hidden_layer_dim=64, \n",
    "#                              dropout=0.5, \n",
    "#                              recurrent_dropout=0.5,\n",
    "#                              final_activation='softmax')\n",
    "\n",
    "#         reduce_lr = ReduceLROnPlateau(monitor='val_loss', \n",
    "#                                       factor=0.03, \n",
    "#                                       patience=15, \n",
    "#                                       min_lr=0.00001, verbose=1)\n",
    "\n",
    "#         history = {}\n",
    "#         filename = \"\"\n",
    "#         # checkpoint\n",
    "#         filepath=\"model/weights.best.h5\"\n",
    "#         #checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "#         #early_stopping = EarlyStopping(monitor='val_loss', patience=20)\n",
    "#         #callbacks_list = [ checkpoint, early_stopping] #reduce_lr, \n",
    "#         history = model.fit(x=X_train,\n",
    "#                           y=y_train, \n",
    "#                           batch_size=32, \n",
    "#                           epochs=100, \n",
    "#                           verbose=1\n",
    "#                           )# ,callbacks=callbacks_list        \n",
    "\n",
    "#         # evaluate the model\n",
    "#         scores = model.evaluate(X[val_index], y[val_index], verbose=0)\n",
    "#         print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "#         cvscores.append(scores[1] * 100)\n",
    "#     print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))\n",
    "#     return cvscores\n",
    "# #         val_acc_list = history.history['val_acc']\n",
    "# #         best_val_acc =  max(val_acc_list)\n",
    "# #         filename = \"hec\" \n",
    "# #         filename = \"model/\" + generate_model_name(filename, best_val_acc) + \".h5\"\n",
    "# #         os.rename(filepath, filename)\n",
    "\n",
    "# #         best_val_acc = -1\n",
    "# #         val_acc_list = history.history['val_acc']\n",
    "# #         best_val_acc =  max(val_acc_list)\n",
    "# #         print(\"best_val_acc: \", best_val_acc)\n",
    "# #         #filename = generate_model_name(filename, best_val_acc)\n",
    "# #         print(\"filename\",filename)\n",
    "# #         plot_model(model, to_file= filename +'.png', show_shapes=True, show_layer_names=True)#\n",
    "# #         return history\n",
    "\n",
    "# cvscores = train_k_fold()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Train and Val set\n",
    "\n",
    "Not used, as using Stratified k-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from shutil import rmtree\n",
    "\n",
    "# trainValSplitRatio = 0.75\n",
    "\n",
    "# def delete_file(filePath):\n",
    "#     if os.path.isfile(filePath):\n",
    "#         os.remove(filePath)\n",
    "\n",
    "# def create_train_dev_set(isRunningForHumanCaption, dataset_path, delimeter=\"|\"):\n",
    "#     if isRunningForHumanCaption:\n",
    "#         caption_csv_path = human_output_caption_to_label_csv_path\n",
    "#         prefix = \"human\"\n",
    "#     else:\n",
    "#         caption_csv_path = auto_output_caption_to_label_csv_path\n",
    "#         prefix = \"auto\"\n",
    "    \n",
    "    \n",
    "#     dataset_groups=[\"train\", \"val\"]\n",
    "#     if not os.path.isdir(dataset_path):\n",
    "#         os.makedirs(dataset_path)\n",
    "\n",
    "#     captionToLabelDf = dt.read_caption_to_label_csv_into_dataframe(caption_csv_path)\n",
    "#     captionToLabelDf.shape\n",
    "#     captionToLabelDf.head()\n",
    "\n",
    "#     random.seed(1)\n",
    "#     trainCount = 0\n",
    "#     valCount = 0\n",
    "#     for index, row in captionToLabelDf.iterrows():\n",
    "#         caption = row['caption']\n",
    "#         label = row['label']\n",
    "#         prob = random.uniform(0, 1)\n",
    "#         if prob > trainValSplitRatio:\n",
    "#             dGroup = \"val\"\n",
    "#             valCount += 1\n",
    "#         else:\n",
    "#             dGroup = \"train\"\n",
    "#             trainCount += 1\n",
    "#         filename = prefix + \"-\" + dGroup + \".csv\"\n",
    "#         filePath = dataset_path + filename\n",
    "#         if index == 0:\n",
    "#             trainFile = prefix + \"-\" + \"train\" + \".csv\"\n",
    "#             delete_file(dataset_path + trainFile)\n",
    "#             valFile = prefix + \"-\" + \"val\" + \".csv\"\n",
    "#             delete_file(dataset_path + valFile)\n",
    "            \n",
    "#         with open(filePath,  'a', encoding=\"utf8\") as f:\n",
    "#             f.write(caption + delimeter + label + \"\\n\")\n",
    "#     print(\"Train count: \", trainCount)\n",
    "#     print(\"Val count: \", valCount)\n",
    "\n",
    "    \n",
    "# caption_to_label_dataset_path = \"dataset/caption-lstm/\"\n",
    "\n",
    "# # Create Train and Val set for human generated captions\n",
    "# isRunningForHumanCaption = True\n",
    "# create_train_dev_set(isRunningForHumanCaption, caption_to_label_dataset_path)\n",
    "\n",
    "# # Create Train and Val set for auto generated captions\n",
    "# isRunningForHumanCaption = False\n",
    "# create_train_dev_set(isRunningForHumanCaption, caption_to_label_dataset_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
