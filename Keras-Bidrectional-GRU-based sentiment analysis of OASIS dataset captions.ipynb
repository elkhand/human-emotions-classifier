{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras based GRU Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset\n",
    "\n",
    "Reformat dataset to be int format\n",
    "```\n",
    "caption | valence_class\n",
    "caption | {negative,neutral,positive}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os, sys, io,re, string, pathlib, random\n",
    "from collections import Counter, OrderedDict\n",
    "\n",
    "import hecutils.data_utils as dt\n",
    "import hecutils.scoring_utils as sc\n",
    "import hecutils.plotting_utils as pt\n",
    "import hecutils.caption_utils as caput\n",
    "import hecutils.image_utils as imut\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.callbacks import Callback, EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.layers import Dense, Conv1D, MaxPooling1D, GRU, Bidirectional, LSTM\n",
    "from keras.layers.core import Dropout, Flatten, Masking, ActivityRegularization\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from keras.utils import plot_model\n",
    "from keras import regularizers\n",
    "from keras.preprocessing import text\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from gensim.models import word2vec\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Keras to use Tensorflow GPU in the backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "num_cores = 8\n",
    "GPU=True\n",
    "CPU = not GPU\n",
    "\n",
    "if GPU:\n",
    "    num_GPU = 1\n",
    "    num_CPU = 8\n",
    "if CPU:\n",
    "    num_CPU = 8\n",
    "    num_GPU = 0\n",
    "\n",
    "config = tf.ConfigProto(intra_op_parallelism_threads=num_cores,\\\n",
    "        inter_op_parallelism_threads=num_cores, allow_soft_placement=True,\\\n",
    "        device_count = {'CPU' : num_CPU, 'GPU' : num_GPU})\n",
    "session = tf.Session(config=config)\n",
    "K.set_session(session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "\n",
    "oasis_csv_path = \"dataset/metadata/OASIS.csv\"\n",
    "captions_root = \"/home/elkhand/git-repos/human-emotions-classifier/dataset/metadata\"\n",
    "captions_root_path = pathlib.Path(captions_root)\n",
    "human_output_caption_to_label_csv_path = captions_root_path/'humanCaptionWithLabeldf.csv'\n",
    "human_caption_csv_path = captions_root_path/'captions.csv'\n",
    "fasttext_embedding_path = 'embedding/wiki-news-300d-1M.vec'\n",
    "model_results_root_dir = \"model/\"\n",
    "inputDataset_csv_path = captions_root_path/\"inputDataset.csv\"\n",
    "testDataset_csv_path = captions_root_path/\"testDataset.csv\"\n",
    "\n",
    "neutralLow = 3.0 \n",
    "neutralHigh = 5.0\n",
    "\n",
    "auto_output_caption_to_label_csv_path = captions_root_path/'autoCaptionWithLabeldf.csv'\n",
    "auto_caption_csv_path = captions_root_path/'auto_generated_captions.csv'\n",
    "\n",
    "\n",
    "dataset_path = human_output_caption_to_label_csv_path\n",
    "# dataset_path = auto_output_caption_to_label_csv_path\n",
    "\n",
    "kfold_splits = 2 # 7\n",
    "test_size = 0.1\n",
    "\n",
    "embedding_dimension = 300\n",
    "hidden_layer_dim = 32\n",
    "batch_size = 64\n",
    "nb_epochs = 100\n",
    "dropout = 0.3\n",
    "recurrent_dropout=0.3\n",
    "patience = 10\n",
    "verbose = 0\n",
    "\n",
    "useF1Score = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create <caption,label> CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.create_caption_to_label(oasis_csv_path,human_caption_csv_path, human_output_caption_to_label_csv_path,neutralLow, neutralHigh)\n",
    "dt.create_caption_to_label(oasis_csv_path,auto_caption_csv_path, auto_output_caption_to_label_csv_path,neutralLow, neutralHigh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide data into train/val/test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df.head()\n",
      "                                              caption  label\n",
      "0           two acorns lying ground next oak leaves.      0\n",
      "1  three acorns flat surface. one acorn missing c...      0\n",
      "2  five acorns arranged circle black background. ...      0\n",
      "3  two cocktails brown wooden table. cocktail fro...      0\n",
      "4                three shelves full bottles whiskey.      0\n",
      "inputDataset.head()\n",
      "                                                caption  label\n",
      "723                    two sea lions face open mouths.      0\n",
      "751  empty lighted ski run twilight spectators watc...      0\n",
      "309                        pile dog feces lying grass.     -1\n",
      "104  group young students classroom knees hands fol...      0\n",
      "446         close-up computer keyboard. layout german.      0\n",
      "testDataset.head()\n",
      "                                                caption  label\n",
      "340  flooded river flowing bridge carrying vehicula...     -1\n",
      "108  three young boys older boy blue white canoe. t...      1\n",
      "91   three plastic bottles water blue caps laid fla...      0\n",
      "719  group students, sitting front blackboard asian...      0\n",
      "88   man sitting chair head tilted back eyes open g...      0\n",
      "\n",
      "\n",
      "Label distribution in inputDataset label\n",
      "-1    147\n",
      " 0    378\n",
      " 1    285\n",
      "Name: label, dtype: int64\n",
      "Label distribution in testDataset label\n",
      "-1    16\n",
      " 0    42\n",
      " 1    32\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(dataset_path, header=0, sep=\"|\")\n",
    "df[\"label\"] = df[\"label\"].apply(lambda x: caput.change_label_str_to_int(x))\n",
    "df[\"caption\"] = df[\"caption\"].apply(lambda x: \" \".join(caput.get_words_withoutstopwords(x.lower().split())))\n",
    "\n",
    "\n",
    "\n",
    "input_x, test_x, input_y,  test_y = train_test_split(df[\"caption\"],\n",
    "                                                     df[\"label\"],\n",
    "                                                     test_size=test_size,\n",
    "                                                     random_state=seed,\n",
    "                                                     stratify=df[\"label\"])\n",
    "\n",
    "inputDataset = pd.concat([input_x, input_y], axis=1)\n",
    "testDataset = pd.concat([test_x, test_y], axis=1)\n",
    "\n",
    "inputDataset = inputDataset.dropna()\n",
    "testDataset = testDataset.dropna()\n",
    "inputDataset = inputDataset.reset_index()\n",
    "testDataset = testDataset.reset_index()\n",
    "\n",
    "\n",
    "inputDataset.to_csv(inputDataset_csv_path, index=False, sep=\"|\")\n",
    "testDataset.to_csv(testDataset_csv_path, index=False, sep=\"|\")\n",
    "\n",
    "print(\"df.head()\\n\", df.head())\n",
    "print(\"inputDataset.head()\\n\", inputDataset.head())\n",
    "print(\"testDataset.head()\\n\", testDataset.head())\n",
    "print(\"\\n\")\n",
    "print(\"Label distribution in inputDataset\", inputDataset.groupby('label').label.count())\n",
    "print(\"Label distribution in testDataset\", testDataset.groupby('label').label.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Fasttext Embeddings\n",
    "\n",
    "You can download fasttext word vectors from here:\n",
    "\n",
    "https://fasttext.cc/docs/en/english-vectors.html    \n",
    "https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki-news-300d-1M.vec.zip\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding size : 999995\n",
      "embedding dimension : (300,)\n"
     ]
    }
   ],
   "source": [
    "def load_embedding(path):\n",
    "    word2vec = {}\n",
    "    with io.open(path, encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            entries = line.rstrip().split(\" \")\n",
    "            word, entries = entries[0], entries[1:]\n",
    "            word2vec[word] = np.array(entries).astype(np.float) # Convert String type to float\n",
    "    print('embedding size : %d' % len(word2vec))\n",
    "    print('embedding dimension : %s' % (word2vec['apple'].shape,))\n",
    "    return word2vec\n",
    "    \n",
    "wordToVec = {}\n",
    "wordToVec = load_embedding(fasttext_embedding_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Epoch 300/300\n",
    "663/663 [==============================] - 2s 3ms/step - loss: 0.1011 - acc: 0.9789 - val_loss: 1.9239 - val_acc: 0.6426\n",
    "\n",
    "best_val_acc:  0.7148936180358237\n",
    "filename adidas-0.7149-1527322080\n",
    "Total time passed for training 9.198103360335033\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config:\n",
      " {'kfold_splits': 2, 'batch_size': 64, 'embedding_dimension': 300, 'recurrent_dropout': 0.3, 'dropout': 0.3, 'nb_epochs': 100, 'useF1Score': True, 'verbose': 0}\n",
      "# words:  999995  word vector dimension 300\n",
      "Label distribution:  label\n",
      "-1    147\n",
      " 0    378\n",
      " 1    285\n",
      "Name: label, dtype: int64\n",
      "max_seq_len 38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elkhand/anaconda3/envs/cs231n/lib/python3.6/site-packages/pandas/core/series.py:841: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
      "  return self.loc[key]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-11b0b3b27739>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0;31m# execute only if run as a script\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-59-11b0b3b27739>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0mwordToVec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfasttext_embedding_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"# words: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwordToVec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" word vector dimension\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwordToVec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwordToVec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m     \u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_StratifiedKFold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwordToVec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m     \u001b[0;31m# Evaluate Test data set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0mevalaute_on_test_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwordToVec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-59-11b0b3b27739>\u001b[0m in \u001b[0;36mtrain_StratifiedKFold\u001b[0;34m(inputDataset, testDataset, wordToVec, config)\u001b[0m\n\u001b[1;32m    112\u001b[0m                             \u001b[0mclass_to_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                             \u001b[0mindex_to_class\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m                             config)\n\u001b[0m\u001b[1;32m    115\u001b[0m         \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcaput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_index_to_one_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_of_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git-repos/human-emotions-classifier/hecutils/caption_utils.py\u001b[0m in \u001b[0;36mload_dataset_StratifiedKFold\u001b[0;34m(dfKFold, wordToVec, max_seq_len, class_to_index, index_to_class, config)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mcaption\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'caption'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcaption\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_non_stop_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0msentence_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_sequence_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwordToVec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_seq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "def build_model(max_seq_len, num_of_classes, config): \n",
    "    # Cross-validation results: 0.61% (+/- 0.13%)\n",
    "    model = Sequential()\n",
    "    model.add(Masking(mask_value=0., input_shape=(None, config['embedding_dimension'])))\n",
    "    #model.add(GRU(max_seq_len,return_sequences=True, recurrent_dropout=recurrent_dropout))\n",
    "    #model.add(GRU(max_seq_len, dropout=0.65, recurrent_dropout=0.35))\n",
    "    model.add(Bidirectional(GRU(max_seq_len, return_sequences=True, dropout=config['dropout'],\n",
    "                                recurrent_dropout=config['recurrent_dropout']), merge_mode='concat'))\n",
    "    model.add(Bidirectional(GRU(max_seq_len, dropout=config['dropout'] - 0.1, \n",
    "                                recurrent_dropout=config['recurrent_dropout'] - 0.1 ), merge_mode='concat'))    \n",
    "    model.add(Dense(num_of_classes, activation='softmax'))\n",
    "    \n",
    "    if config['useF1Score']:\n",
    "        metrics = ['accuracy', sc.f1, sc.recall, sc.precision]\n",
    "    else:\n",
    "        metrics = ['accuracy']\n",
    "        \n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', \n",
    "                      metrics=metrics)\n",
    "    #print(model.summary())\n",
    "    return model\n",
    "\n",
    "\n",
    "def evalaute_on_test_data(model, testDataset, inputDataset, wordToVec, config):\n",
    "    max_seq_len = int(testDataset['caption'].map(lambda x: caput.get_non_stop_word_count(x.split())).max())\n",
    "    print(\"max_seq_len\", max_seq_len)\n",
    "    \n",
    "    num_of_classes, class_to_index, index_to_class = caput.get_label_map_from_train_set(inputDataset, wordToVec, max_seq_len, config)\n",
    "    \n",
    "    X_test, y_test_index, _, _, _  = caput.load_dataset_StratifiedKFold(testDataset,wordToVec,max_seq_len,class_to_index, index_to_class, config)\n",
    "    y_test = caput.convert_index_to_one_hot(y_test_index, num_of_classes) \n",
    "    print(model.summary())\n",
    "    results = model.evaluate(X_test, y_test, verbose=1) # batch_size=1,\n",
    "    print(model.metrics_names, results)\n",
    "        \n",
    "    predictions = model.predict(X_test)\n",
    "    print(\"1 predictions\", predictions)\n",
    "    print(\"index_to_class\", index_to_class)\n",
    "    \n",
    "    label_map_from_train_gen = index_to_class\n",
    "    print(\"label_map_from_train_gen\", label_map_from_train_gen, \"label_map_from_train_gen.keys()\", label_map_from_train_gen.keys())\n",
    "    predictions = np.argmax(predictions, axis=-1) #multiple categories\n",
    "    print(\"2 predictions\", predictions)\n",
    "    predictions = [label_map_from_train_gen[k] for k in predictions]\n",
    "    print(\"3 predictions\", predictions)\n",
    "    \n",
    "    #predictions = imut.conver_predictions_to_classes(predictions, label_map_from_train_gen)\n",
    "    print(\"1 y_test_index\",y_test_index)\n",
    "    y_test_index = [label_map_from_train_gen[k] for k in y_test_index]\n",
    "    print(\"2 y_test_index\",y_test_index)\n",
    "    print(\"from df\", testDataset['label'])\n",
    "    print(\"y_test\", y_test)\n",
    "    y_true = y_test_index\n",
    "    y_pred = predictions\n",
    "    \n",
    "    \n",
    "    cnf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    print(\"cnf_matrix\", cnf_matrix)\n",
    "    titleOfConfusionMatrix = \"Confusion Matrix based on GRU based Seq2seq model on captions\"\n",
    "    pt.plot_confusion_matrix_from_labels(y_true, y_pred, titleOfConfusionMatrix)\n",
    "    \n",
    "#     X_test, y_test, _,_,_  = caput.load_dataset_StratifiedKFold(testDataset,wordToVec,max_seq_len,class_to_index, index_to_class, config)\n",
    "    \n",
    "\n",
    "#     predictions = model.predict(X_test, batch_size=1, verbose=1) # \n",
    "    \n",
    "#     print(\"class_to_index\",class_to_index)\n",
    "#     print(\"index_to_class\",index_to_class)\n",
    "    \n",
    "\n",
    "    \n",
    "#     test_filenames = test_batches.filenames\n",
    "#     y_true = get_truth_labels_test_data(test_filenames)\n",
    "#     y_pred = predictions\n",
    "\n",
    "\n",
    "\n",
    "def train_StratifiedKFold(inputDataset, testDataset, wordToVec, config):\n",
    "    \"\"\"StratifiedKFold cross-validation\"\"\"\n",
    "    # Shuffle dataset\n",
    "    df = shuffle(inputDataset)\n",
    "    X = df[\"caption\"]\n",
    "    y = df[\"label\"]\n",
    "    print(\"Label distribution: \",df.groupby('label').label.count())\n",
    "    max_seq_len = int(df['caption'].map(lambda x: caput.get_non_stop_word_count(x.split())).max())\n",
    "    print(\"max_seq_len\", max_seq_len)\n",
    "    \n",
    "    # Instantiate the cross validator\n",
    "    skf = StratifiedKFold(n_splits=kfold_splits, shuffle=True)\n",
    "    cv_accuracies = []\n",
    "    cv_f1s = []\n",
    "    \n",
    "    best_model = None\n",
    "    best_model_best_acc = -1\n",
    "    \n",
    "    # Loop through the indices the split() method returns\n",
    "    for index, (train_indices, val_indices) in enumerate(skf.split(X, y)):\n",
    "        start = time.time()        \n",
    "        class_to_index = {}\n",
    "        index_to_class = {}\n",
    "        \n",
    "        X_train, X_val = X[train_indices], X[val_indices]\n",
    "        y_train_index, y_val_index = y[train_indices], y[val_indices]\n",
    "        \n",
    "        dfTrain = pd.concat([X_train, y_train_index], axis=1)\n",
    "        dfTrain.columns = ['caption', 'label']\n",
    "        X_train, y_train_index, num_of_classes, class_to_index, index_to_class = \\\n",
    "            caput.load_dataset_StratifiedKFold(\n",
    "                            dfTrain,\n",
    "                            wordToVec, \n",
    "                            max_seq_len, \n",
    "                            class_to_index, \n",
    "                            index_to_class,\n",
    "                            config)\n",
    "        y_train = caput.convert_index_to_one_hot(y_train_index, num_of_classes) \n",
    "        \n",
    "        dfVal= pd.concat([X_val, y_val_index], axis=1)\n",
    "        dfVal.columns = ['caption', 'label']\n",
    "        X_val, y_val_index, _, _, _ = caput.load_dataset_StratifiedKFold(\n",
    "                            dfVal,\n",
    "                            wordToVec,\n",
    "                            max_seq_len, \n",
    "                            class_to_index, \n",
    "                            index_to_class,\n",
    "                            config)\n",
    "        y_val = caput.convert_index_to_one_hot(y_val_index, num_of_classes) \n",
    "        \n",
    "        \n",
    "        print(\"\\nTRAIN size:\", len(train_indices), \"\\t VAL size:\", len(val_indices))\n",
    "        print(\"Train label distribution: \",dfTrain.groupby('label').label.count())\n",
    "        print(\"Val label distribution: \",dfVal.groupby('label').label.count())\n",
    "\n",
    "        \n",
    "        model = build_model(max_seq_len,\n",
    "                            num_of_classes, \n",
    "                            config\n",
    "                            )\n",
    "        plot_model(model, to_file = model_results_root_dir + '/model.png', show_shapes=True, show_layer_names=True)#\n",
    "        \n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', \n",
    "                                  factor=0.7, \n",
    "                                  patience=2, \n",
    "                                  min_lr=10e-7,\n",
    "                                  cooldown=1,\n",
    "                                  verbose=config['verbose'])\n",
    "        \n",
    "        history = {}\n",
    "        filename = \"\"\n",
    "        # checkpoint\n",
    "        #filepath= model_results_root_dir + \"/weights.best.h5\"\n",
    "        #checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=0, save_best_only=True, mode='max')\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=patience)\n",
    "        \n",
    "        callbacks_list = [  reduce_lr, early_stopping]  # checkpoint,\n",
    "        history = model.fit(x=X_train,\n",
    "                      y=y_train, \n",
    "                      batch_size=config['batch_size'],# 64 seems fine, 32 is better \n",
    "                      epochs=config['nb_epochs'], \n",
    "                      verbose=config['verbose'], \n",
    "                      validation_data = (X_val, y_val),\n",
    "                      shuffle=True,\n",
    "                      callbacks=callbacks_list)        \n",
    "        val_acc_list = history.history['val_acc']\n",
    "        best_val_acc =  max(val_acc_list)\n",
    "        cv_accuracies.append(best_val_acc)\n",
    "        print(\"best_val_acc: \", best_val_acc)\n",
    "        \n",
    "        if config['useF1Score']:\n",
    "            val_f1_list = history.history['val_f1']\n",
    "            best_f1 =  max(val_f1_list)\n",
    "            print(\"best_f1: \", best_f1)\n",
    "            cv_f1s.append(best_f1)\n",
    "        \n",
    "#         filename = \"caption\" \n",
    "#         filename = model_results_root_dir + caput.generate_model_name(filename, best_val_acc) + \".h5\"\n",
    "#         os.rename(filepath, filename)\n",
    "        \n",
    "        pt.plot_model_accuracy(history,model_results_root_dir, config['useF1Score'])\n",
    "        \n",
    "        end = time.time()\n",
    "        print(\"Time passed for training\", (end-start))\n",
    "        \n",
    "        if best_val_acc > best_model_best_acc:\n",
    "            best_model_best_acc = best_val_acc\n",
    "            best_model = model\n",
    "    \n",
    "    print(\"=========================================\")\n",
    "    print(\"Cross-validation val accuracy results: \" , cv_accuracies)\n",
    "    print(\"Cross-validation val accuracy results: %.2f%% (+/- %.2f%%)\" % (np.mean(cv_accuracies), np.std(cv_accuracies)))\n",
    "    \n",
    "    print(\"\\n\",\"Cross-validation val f1 results: \" , cv_f1s)\n",
    "    print(\"Cross-validation val f1 results: %.2f%% (+/- %.2f%%)\" % (np.mean(cv_f1s), np.std(cv_f1s)))\n",
    "    \n",
    "    best_model.save(model_results_root_dir + \"/bestmodel-\" + str(best_model_best_acc) + \".h5\")\n",
    "    return best_model\n",
    "\n",
    "\n",
    "def get_config():\n",
    "    conf = {}\n",
    "    conf[\"kfold_splits\"] = kfold_splits\n",
    "    conf[\"batch_size\"] = batch_size\n",
    "    conf['embedding_dimension'] = embedding_dimension\n",
    "    conf['recurrent_dropout'] = recurrent_dropout\n",
    "    conf['dropout'] = dropout\n",
    "    conf[\"nb_epochs\"] = nb_epochs\n",
    "    conf['useF1Score'] = useF1Score\n",
    "    conf['verbose'] = verbose\n",
    "    conf['useF1Score'] = useF1Score\n",
    "    return conf    \n",
    "    \n",
    "best_model = None\n",
    "def main():\n",
    "    global wordToVec, best_model\n",
    "    config = get_config()\n",
    "    print(\"config:\\n\", config)\n",
    "    if wordToVec is None:\n",
    "        wordToVec = load_embedding(fasttext_embedding_path)\n",
    "    print(\"# words: \", len(wordToVec.keys()),\" word vector dimension\", len(wordToVec[list(wordToVec.keys())[1]]))\n",
    "    best_model = train_StratifiedKFold(inputDataset, testDataset, wordToVec, config)\n",
    "    # Evaluate Test data set\n",
    "    evalaute_on_test_data(best_model, testDataset, inputDataset, wordToVec, config)\n",
    "    pass\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # execute only if run as a script\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "evalaute_on_test_data() takes 3 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-3b5540a190c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mevalaute_on_test_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: evalaute_on_test_data() takes 3 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "evalaute_on_test_data(best_model, testDataset, inputDataset, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determien Vocabulary size - unique word counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(dataset_path, header=0, sep=\"|\")\n",
    "df[\"caption\"] = df[\"caption\"].apply(lambda x: \" \".join(get_words_withoutstopwords(x.lower().split())))\n",
    "vocabulary = set()\n",
    "for index, row in df.iterrows():\n",
    "    caption = row['caption']\n",
    "    words = caption.lower().split()\n",
    "    for word in words:\n",
    "        vocabulary.add(word)\n",
    "    \n",
    "print(\"Voc size: \",len(vocabulary))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Train on Twitter dataset, validate on OASIS images caption dataset\n",
    "\n",
    "1,578,628 Tweets with positive/negative sentiments - labelled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "# import os, sys\n",
    "# import re\n",
    "# import string\n",
    "# import pathlib\n",
    "# import random\n",
    "# from collections import Counter, OrderedDict\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import spacy\n",
    "# from tqdm import tqdm, tqdm_notebook, tnrange\n",
    "# tqdm.pandas(desc='Progress')\n",
    "\n",
    "# from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# from IPython.core.interactiveshell import InteractiveShell\n",
    "# InteractiveShell.ast_node_interactivity='all'\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp = spacy.load('en',disable=['parser', 'tagger', 'ner'])\n",
    "# def tokenizer(s): return [w.text.lower() for w in nlp(tweet_clean(s))]\n",
    "\n",
    "# def tweet_clean(text):\n",
    "#     text = re.sub(r'[^A-Za-z0-9]+', ' ', text) # remove non alphanumeric character\n",
    "#     text = re.sub(r'https?:/\\/\\S+', ' ', text) # remove links\n",
    "#     return text.strip()\n",
    "\n",
    "\n",
    "# twitter_dataset_path = \"/home/elkhand/datasets/twitter-sentiment/Sentiment_Analysis_Dataset.csv\" \n",
    "# cleaned_twitter_dataset_path = \"/home/elkhand/datasets/twitter-sentiment/Cleaned_Sentiment_Analysis_Dataset.csv\" \n",
    "# df = pd.read_csv(twitter_dataset_path, error_bad_lines=False)\n",
    "# df = df[['SentimentText','Sentiment']]\n",
    "# df[\"SentimentText\"] = df[\"SentimentText\"].apply(lambda x: tokenizer(x))\n",
    "# df[\"SentimentText\"] = df[\"SentimentText\"].apply(lambda x: \" \".join(x))\n",
    "# df[\"SentimentText\"] = df[\"SentimentText\"].apply(lambda x: \" \".join(get_words_withoutstopwords(x.lower().split())))\n",
    "# df.columns = ['caption','label']\n",
    "# df = df.dropna()\n",
    "# df.to_csv(cleaned_twitter_dataset_path, index=False)\n",
    "# print(\"df.shape\", df.shape)\n",
    "# print(\"df.head()\", df.head())\n",
    "\n",
    "# fig = plt.figure(figsize=(8,5))\n",
    "# ax = sns.barplot(x=df.label.unique(),y=df.label.value_counts());\n",
    "# ax.set(xlabel='Labels');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train on Twitter dataset, validate on Image captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cleaned_twitter_dataset_path = \"/home/elkhand/datasets/twitter-sentiment/Cleaned_Sentiment_Analysis_Dataset.csv\"\n",
    "\n",
    "# import tensorflow as tf\n",
    "# tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "\n",
    "\n",
    "# def build_model(isBinaryClassification, max_seq_len, num_of_classes, embedding_dim, hidden_layer_dim, dropout, recurrent_dropout, final_activation): \n",
    "#     # Cross-validation results: 0.61% (+/- 0.13%)\n",
    "#     model = Sequential()\n",
    "#     model.add(Masking(mask_value=0., input_shape=(None, embedding_dim)))\n",
    "#     #, dropout=dropout, recurrent_dropout=recurrent_dropout\n",
    "#     model.add(GRU(max_seq_len,return_sequences=True, dropout=dropout, recurrent_dropout=recurrent_dropout))\n",
    "#     #model.add(GRU(max_seq_len // 2,return_sequences=True, dropout=dropout, recurrent_dropout=recurrent_dropout))\n",
    "#     model.add(Dropout(0.2))\n",
    "#     model.add(GRU(max_seq_len, dropout=dropout, recurrent_dropout=recurrent_dropout)) \n",
    "# #     model.add(Dropout(dropout))\n",
    "    \n",
    "    \n",
    "#     model.add(Dense(num_of_classes, activation=final_activation))\n",
    "\n",
    "#     if isBinaryClassification:\n",
    "#         model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', f1, recall, precision])#\n",
    "#     else:\n",
    "#         model.compile(loss='categorical_crossentropy', optimizer='adam', \n",
    "#                       metrics=['accuracy', f1, recall, precision])#\n",
    "#     print(model.summary())\n",
    "#     return model\n",
    "\n",
    "# def twitter_train_StratifiedKFold(isBinaryClassification):\n",
    "#     \"\"\"StratifiedKFold cross-validation to solve class imbalance issue\"\"\"\n",
    "    \n",
    "#     dfTwitter = pd.read_csv(cleaned_twitter_dataset_path, header=0)\n",
    "#     print(dfTwitter.head())\n",
    "#     dfTwitter.columns = ['caption', 'label']\n",
    "#     dfTwitter = dfTwitter.dropna()\n",
    "#     dfTwitterPos = dfTwitter[dfTwitter.label == 1]\n",
    "#     dfTwitterPos = dfTwitterPos[:5000]\n",
    "#     dfTwitterNeg = dfTwitter[dfTwitter.label == 0]\n",
    "#     dfTwitterNeg = dfTwitterNeg[:5000]\n",
    "#     dfTwitter =  pd.concat([dfTwitterNeg, dfTwitterPos]) \n",
    "\n",
    "#     print(\"dfTwitter.count()\",dfTwitter.count())\n",
    "\n",
    "#     dfCaption = pd.read_csv(dataset_path, header=0, sep=\"|\")\n",
    "#     # Doing only binary classification\n",
    "#     if isBinaryClassification:\n",
    "#         dfCaption = get_df_for_binary_classification()\n",
    "    \n",
    "\n",
    "#     print(\"Label distribution: \",dfCaption.groupby('label').label.count())\n",
    "#     max_seq_len_captions = int(dfCaption['caption'].map(lambda x: get_non_stop_word_count(x.split())).max())\n",
    "#     print(\"max_seq_len_captions\", max_seq_len_captions)\n",
    "    \n",
    "#     max_seq_len_tweets = int(dfTwitter['caption'].map(lambda x: len(x.split())).max())\n",
    "#     print(\"max_seq_len_tweets\", max_seq_len_tweets)\n",
    "    \n",
    "#     max_seq_len = max(max_seq_len_captions,max_seq_len_tweets)\n",
    "#     print(\"max_seq_len\", max_seq_len)\n",
    "    \n",
    "    \n",
    "    \n",
    "#     X = dfCaption['caption']\n",
    "#     y = dfCaption['label']\n",
    "    \n",
    " \n",
    "    \n",
    "#     if isBinaryClassification:\n",
    "#         final_activation ='sigmoid'\n",
    "#     else:\n",
    "#         final_activation ='softmax'\n",
    "#     print(\"final_activation\",final_activation)\n",
    "    \n",
    "    \n",
    "#     # Instantiate the cross validator\n",
    "#     skf = StratifiedKFold(n_splits=kfold_splits, shuffle=True)\n",
    "#     printCnt = 5\n",
    "#     cv_accuracies = []\n",
    "#     cv_f1s = []\n",
    "    \n",
    "#     # Loop through the indices the split() method returns\n",
    "#     for index, (train_indices, val_indices) in enumerate(skf.split(X, y)):\n",
    "        \n",
    "#         X_train_caption, X_val_caption = X[train_indices], X[val_indices]\n",
    "#         y_train_caption, y_val_caption = y[train_indices], y[val_indices]\n",
    "        \n",
    "#         dfCaptionTrain = pd.concat([X_train_caption, y_train_caption], axis=1) \n",
    "        \n",
    "#         dfTweetCaptionTrain = pd.concat([dfTwitter, dfCaptionTrain])               \n",
    "        \n",
    "#         X_train, y_train_index, num_of_classes = load_dataset_StratifiedKFold(dfTweetCaptionTrain, max_seq_len)\n",
    "#         y_train = convert_index_to_one_hot(y_train_index, num_of_classes)\n",
    "#         print(\"Train label distribution: \",dfTweetCaptionTrain.groupby('label').label.count())\n",
    "#         print(\"num_of_classes\",num_of_classes)\n",
    "        \n",
    "#         dfCaptionVal = pd.concat([X_val_caption, y_val_caption], axis=1) \n",
    "        \n",
    "#         X_val, y_val_index, _ = load_dataset_StratifiedKFold(dfCaptionVal,max_seq_len)\n",
    "#         y_val = convert_index_to_one_hot(y_val_index, num_of_classes) \n",
    "#         print(\"Val label distribution: \",dfCaptionVal.groupby('label').label.count())       \n",
    "\n",
    "#         model = build_model(isBinaryClassification,\n",
    "#                                max_seq_len,\n",
    "#                                 num_of_classes, \n",
    "#                                 embedding_dim=300, \n",
    "#                                 hidden_layer_dim=40, \n",
    "#                                 dropout=0.2, \n",
    "#                                 recurrent_dropout=0.2,# 0.3\n",
    "#                                 final_activation=final_activation\n",
    "#                                 )\n",
    "#         plot_model(model, to_file= 'model/model.png', show_shapes=True, show_layer_names=True)#\n",
    "\n",
    "#         reduce_lr = ReduceLROnPlateau(monitor='val_loss', \n",
    "#                                       factor=0.2, \n",
    "#                                       patience=5, \n",
    "#                                       min_lr=0.001)\n",
    "\n",
    "#         history = {}\n",
    "#         filename = \"\"\n",
    "#         # checkpoint\n",
    "#         filepath=\"model/weights.best.h5\"\n",
    "#         checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=0, save_best_only=True, mode='max')\n",
    "#         early_stopping = EarlyStopping(monitor='val_loss', patience=5)    \n",
    "#         callbacks_list = [ checkpoint, early_stopping, reduce_lr]  #\n",
    "#         history = model.fit(x=X_train,\n",
    "#                           y=y_train, \n",
    "#                           batch_size=32,# 64 seems fine, 32 is better \n",
    "#                           epochs=100, \n",
    "#                           verbose=1, \n",
    "#                           validation_data = (X_val, y_val),\n",
    "#                           shuffle=True,\n",
    "#                           callbacks=callbacks_list)        \n",
    "#         val_acc_list = history.history['val_acc']\n",
    "#         val_f1_list = history.history['val_f1']\n",
    "#         best_val_acc =  max(val_acc_list)\n",
    "#         best_f1 =  max(val_f1_list)\n",
    "#         print(\"best_val_acc: \", best_val_acc)\n",
    "#         print(\"best_f1: \", best_f1)\n",
    "#         filename = \"hec\" \n",
    "#         filename = \"model/\" + generate_model_name(filename, best_val_acc) + \".h5\"\n",
    "#         os.rename(filepath, filename)\n",
    "#         cv_accuracies.append(best_val_acc)\n",
    "#         cv_f1s.append(best_f1)\n",
    "#         plot_model_accuracy(history, \"model/\")\n",
    "#         end = time.time()\n",
    "#         print(\"Time passed for training\", (end-start)/60)\n",
    "    \n",
    "#     print(\"=========================================\")\n",
    "#     print(\"Cross-validation val accuracy results: \" , cv_accuracies)\n",
    "#     print(\"Cross-validation val accuracy results: %.2f%% (+/- %.2f%%)\" % (np.mean(cv_accuracies), np.std(cv_accuracies)))\n",
    "    \n",
    "#     print(\"\\n\",\"Cross-validation val f1 results: \" , cv_f1s)\n",
    "#     print(\"Cross-validation val f1 results: %.2f%% (+/- %.2f%%)\" % (np.mean(cv_f1s), np.std(cv_f1s)))\n",
    "    \n",
    "    \n",
    "#     return history\n",
    "\n",
    "\n",
    "# dataset_path = human_output_caption_to_label_csv_path\n",
    "# kfold_splits = 7\n",
    "# # dataset_path = auto_output_caption_to_label_csv_path\n",
    "\n",
    "\n",
    "# embedding_dimension = 300\n",
    "# class_to_index = {}\n",
    "# index_to_class = {}\n",
    "# #max_seq_len = 40\n",
    "# start = time.time()\n",
    "# isBinaryClassification = True\n",
    "# #history = twitter_train_StratifiedKFold(isBinaryClassification) \n",
    "# #dataset_path = caption_to_label_dataset_path + \"/\" +'human-train.csv'\n",
    "# #val_dataset_path = caption_to_label_dataset_path + \"/\" +'human-val.csv'\n",
    "# #history = train()\n",
    "# end = time.time()\n",
    "# print(\"Total time passed for training\", (end-start)/60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "\n",
    "tf = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "tf.list_devices()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
