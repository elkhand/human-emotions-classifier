{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Image & Caption joint training\n",
    "\n",
    "https://gist.github.com/elkhand/412f9dc4cd1a72c4571354e81c93d695"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Imports for Caption model\n",
    "\n",
    "import os, sys, io,re, string, pathlib, random\n",
    "from collections import Counter, OrderedDict\n",
    "\n",
    "import hecutils.data_utils as dt\n",
    "import hecutils.scoring_utils as sc\n",
    "import hecutils.plotting_utils as pt\n",
    "import hecutils.caption_utils as caput\n",
    "import hecutils.image_utils as imut\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.callbacks import Callback, EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "from keras.layers import Dense, Conv1D, MaxPooling1D, GRU, Bidirectional, LSTM\n",
    "from keras.layers.core import Dropout, Flatten, Masking, ActivityRegularization\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from keras.utils import plot_model\n",
    "from keras import regularizers\n",
    "from keras.preprocessing import text\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "from sklearn.utils import shuffle, class_weight\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from gensim.models import word2vec\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "################################################################################################################\n",
    "# Imports for Image model\n",
    "\n",
    "import os, sys, re, string, pathlib, random, io, time, glob\n",
    "from collections import Counter, OrderedDict\n",
    "from shutil import copyfile, rmtree\n",
    "\n",
    "#import hecutils.resnet152 as resnet\n",
    "from hecutils.resnet152 import ResNet152\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import hecutils.data_utils as dt\n",
    "import hecutils.scoring_utils as sc\n",
    "import hecutils.plotting_utils as pt\n",
    "import hecutils.image_utils as imut\n",
    "\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, Callback\n",
    "from keras.layers import GRU, Bidirectional, LSTM, MaxPooling1D, Conv1D,Dense, GlobalAveragePooling2D, GlobalMaxPooling2D\n",
    "from keras.layers.core import Dropout, Flatten, Masking, ActivityRegularization\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from keras.utils import plot_model\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from keras.preprocessing import text\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from keras.models import Model\n",
    "from keras import optimizers\n",
    "\n",
    "# from fastText import load_model\n",
    "from gensim.models import word2vec\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle, class_weight\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Keras to use Tensorflow GPU in the backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "num_cores = 8\n",
    "GPU=True\n",
    "CPU = not GPU\n",
    "\n",
    "if GPU:\n",
    "    num_GPU = 1\n",
    "    num_CPU = 8\n",
    "if CPU:\n",
    "    num_CPU = 8\n",
    "    num_GPU = 0\n",
    "\n",
    "config = tf.ConfigProto(intra_op_parallelism_threads=num_cores,\\\n",
    "        inter_op_parallelism_threads=num_cores, allow_soft_placement=True,\\\n",
    "        device_count = {'CPU' : num_CPU, 'GPU' : num_GPU})\n",
    "session = tf.Session(config=config)\n",
    "K.set_session(session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caption model\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "\n",
    "oasis_csv_path = \"dataset/metadata/OASIS.csv\"\n",
    "captions_root = \"/home/elkhand/git-repos/human-emotions-classifier/dataset/metadata\"\n",
    "captions_root_path = pathlib.Path(captions_root)\n",
    "human_output_caption_to_label_csv_path = captions_root_path/'humanCaptionWithLabeldf.csv'\n",
    "human_caption_csv_path = captions_root_path/'captions.csv'\n",
    "#fasttext_embedding_path = 'embedding/wiki-news-300d-1M.vec'\n",
    "fasttext_embedding_path = '/home/elkhand/datasets/glove-vectors/glove.twitter.27B.200d.txt'\n",
    "model_results_root_dir = \"model/\"\n",
    "inputDataset_csv_path = captions_root_path/\"inputDataset.csv\"\n",
    "testDataset_csv_path = captions_root_path/\"testDataset.csv\"\n",
    "\n",
    "neutralLow = 3.0 \n",
    "neutralHigh = 5.0\n",
    "\n",
    "auto_output_caption_to_label_csv_path = captions_root_path/'autoCaptionWithLabeldf.csv'\n",
    "auto_caption_csv_path = captions_root_path/'auto_generated_captions.csv'\n",
    "\n",
    "\n",
    "dataset_path = human_output_caption_to_label_csv_path\n",
    "# dataset_path = auto_output_caption_to_label_csv_path\n",
    "\n",
    "kfold_splits = 7 # 10 # 7 # 5 # 10 # 7 \n",
    "test_size = 0.1\n",
    "\n",
    "embedding_dimension = 200 # 300\n",
    "hidden_layer_dim = 32\n",
    "batch_size = 16 # 64\n",
    "nb_epochs = 100\n",
    "dropout = 0.3\n",
    "recurrent_dropout=  0.6\n",
    "patience = 10\n",
    "verbose = 1\n",
    "\n",
    "useF1Score = False # True\n",
    "\n",
    "################################################################################################################\n",
    "# Image model\n",
    "\n",
    "oasis_csv_path = \"dataset/metadata/OASIS.csv\"\n",
    "oasis_images_src = \"dataset/images/\"\n",
    "input_images_src = \"dataset/input-joint/\"\n",
    "test_images_src = \"dataset/test-joint/\"\n",
    "model_results_root_dir = \"img_model-joint/\"\n",
    "\n",
    "input_images_classified = \"dataset/input-classified-joint/\"\n",
    "test_images_classified = \"dataset/test-classified-joint/\"\n",
    "\n",
    "# ou can downlaod weights here: https://gist.github.com/flyyufelix/7e2eafb149f72f4d38dd661882c554a6\n",
    "weights_path = \"/home/elkhand/weights/resnet152_weights_tf.h5\"\n",
    "\n",
    "dataset_groups=[\"train\", \"val\"]\n",
    "classes = [\"negative\", \"neutral\", \"positive\"]\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "\n",
    "test_size = 0.1\n",
    "\n",
    "kfold_splits =  7 #5\n",
    "\n",
    "neutralLow = 3.0\n",
    "neutralHigh = 5.0\n",
    "\n",
    "nb_epochs = 100\n",
    "patience = 10 # ReduceLROnPlateau has 5\n",
    "batch_size = 32 # 32  \n",
    "\n",
    "FC_SIZE = 128 # 1024\n",
    "LAYERS_TO_UNFREEZE = 10\n",
    "\n",
    "img_height = 224 # 299\n",
    "img_width = 224  # 299\n",
    "\n",
    "useF1Score = False\n",
    "verbose=1 \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create <caption,label> CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.create_caption_to_label(oasis_csv_path,human_caption_csv_path, human_output_caption_to_label_csv_path,neutralLow, neutralHigh)\n",
    "dt.create_caption_to_label(oasis_csv_path,auto_caption_csv_path, auto_output_caption_to_label_csv_path,neutralLow, neutralHigh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide data into train/val/test datasets\n",
    "\n",
    "Read dataframe to have:\n",
    "\n",
    "<imageName, caption, label>\n",
    "\n",
    "1. Read into df <imageId, label>\n",
    "2. Then separate data into input and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Label distribution in inputDataset label\n",
      "negative    147\n",
      "neutral     378\n",
      "positive    285\n",
      "Name: label, dtype: int64\n",
      "Label distribution in testDataset label\n",
      "negative    16\n",
      "neutral     42\n",
      "positive    32\n",
      "Name: label, dtype: int64\n",
      "Input data size 810\n",
      "Test data size 90\n"
     ]
    }
   ],
   "source": [
    "dfImageIdCaptionLabel = pd.read_csv(dataset_path, header=0, sep=\"|\")\n",
    "dfImageIdCaptionLabel.columns = [\"id\",\"caption\", \"label\"]\n",
    "dfImageIdCaptionLabel[\"caption\"] = dfImageIdCaptionLabel[\"caption\"].apply(lambda x: \" \".join(caput.get_words_withoutstopwords(x.lower().split())))\n",
    "#dfImageIdCaptionLabel[\"label\"] = dfImageIdCaptionLabel[\"label\"].apply(lambda x: caput.change_label_str_to_int(x))\n",
    "\n",
    "\n",
    "dfImageIdImageName = dt.get_image_id_to_image_title_as_df(oasis_csv_path)\n",
    "dfImageIdImageName.columns = ['id', 'image_name']\n",
    "dfImageIdImageName['image_name'] = dfImageIdImageName['image_name'].apply(lambda x: x + \".jpg\") \n",
    "printCnt = 5\n",
    "# has [id, caption, label]\n",
    "df = pd.merge(dfImageIdCaptionLabel, dfImageIdImageName, on=\"id\")\n",
    "#print(df.head(printCnt))\n",
    "\n",
    "\n",
    "\n",
    "input_x, test_x, input_y,  test_y = train_test_split(df[\"id\"],\n",
    "                                                     df[\"label\"],\n",
    "                                                     test_size=test_size,\n",
    "                                                     random_state=seed,\n",
    "                                                     stratify=df[\"label\"])\n",
    "\n",
    "inputDataset = pd.concat([input_x, input_y], axis=1)\n",
    "testDataset = pd.concat([test_x, test_y], axis=1)\n",
    "\n",
    "inputDataset = inputDataset.dropna()\n",
    "testDataset = testDataset.dropna()\n",
    "inputDataset = inputDataset.reset_index()\n",
    "testDataset = testDataset.reset_index()\n",
    "\n",
    "# print(\"inputDataset\\n\", inputDataset.head(10))\n",
    "# print(\"testDataset\\n\", testDataset.head(10))\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Label distribution in inputDataset\", inputDataset.groupby('label').label.count())\n",
    "print(\"Label distribution in testDataset\", testDataset.groupby('label').label.count())\n",
    "\n",
    "\n",
    "inputData = df.loc[df['id'].isin(inputDataset.id)]\n",
    "testData = df.loc[df['id'].isin(testDataset.id)]\n",
    "\n",
    "# print(\"inputData\\n\", inputData.head())\n",
    "# print(\"testData\\n\", testData.head())\n",
    "\n",
    "inputIds = set(inputData['id'].values)\n",
    "testIds = set(testData['id'].values)\n",
    "\n",
    "print(\"Input data size\", len(inputIds))\n",
    "print(\"Test data size\", len(testIds))\n",
    "\n",
    "for inputId in inputIds:\n",
    "    if inputId in testIds:\n",
    "        raise inputId + \" inputId exists both in test and input dataset\"\n",
    "        \n",
    "for testId in testIds:\n",
    "    if testId in inputIds:\n",
    "        raise testId + \" testId exists both in test and input dataset\"        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating test and input dataset, and `positive,neutral,negative` under each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete input images dir\n",
    "rmtree(input_images_src, ignore_errors=True)\n",
    "os.makedirs(input_images_src)\n",
    "\n",
    "\n",
    "# Delete test images dir\n",
    "rmtree(test_images_src, ignore_errors=True)\n",
    "os.makedirs(test_images_src)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Copy input images into input dir, and test images into test dir\n",
    "imut.copy_imgs_into(oasis_images_src, inputData['image_name'], input_images_src)\n",
    "imut.copy_imgs_into(oasis_images_src, testData['image_name'], test_images_src)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Fasttext Embeddings\n",
    "\n",
    "You can download fasttext word vectors from here:\n",
    "\n",
    "https://fasttext.cc/docs/en/english-vectors.html\n",
    "\n",
    "https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki-news-300d-1M.vec.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embedding(path):\n",
    "    word2vec = {}\n",
    "    with io.open(path, encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            entries = line.rstrip().split(\" \")\n",
    "            word, entries = entries[0], entries[1:]\n",
    "            word2vec[word] = np.array(entries).astype(np.float) # Convert String type to float\n",
    "    print('embedding size : %d' % len(word2vec))\n",
    "    print('embedding dimension : %s' % (word2vec['apple'].shape,))\n",
    "    return word2vec\n",
    "    \n",
    "wordToVec = {}\n",
    "wordToVec = load_embedding(fasttext_embedding_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joint model, which will learn both from images and captions together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.layers import Dropout\n",
    "from keras import applications\n",
    "from keras.layers import Dense, GlobalAveragePooling2D, merge, Input\n",
    "from keras.models import Model\n",
    "\n",
    "max_words = 10000\n",
    "epochs = 50\n",
    "batch_size = 32\n",
    "\n",
    "X_train_image = ...  #images training input\n",
    "X_train_text = ... #text training input\n",
    "y_train = ... #training output\n",
    "\n",
    "num_classes = np.max(y_train) + 1\n",
    "\n",
    "# Text input branch - just a simple MLP\n",
    "text_inputs = Input(shape=(max_words,))\n",
    "branch_1 = Dense(512, activation='relu')(text_inputs)\n",
    "\n",
    "# Image input branch - a pre-trained Inception module followed by an added fully connected layer\n",
    "base_model = applications.InceptionV3(weights='imagenet', include_top=False)\n",
    "# Freeze Inception's weights - we don't want to train these\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# add a fully connected layer after Inception - we do want to train these\n",
    "branch_2 = base_model.output\n",
    "branch_2 = GlobalAveragePooling2D()(branch_2)\n",
    "branch_2 = Dense(1024, activation='relu')(branch_2)\n",
    "\n",
    "# merge the text input branch and the image input branch and add another fully connected layer\n",
    "joint = merge([branch_1, branch_2], mode='concat')\n",
    "joint = Dense(512, activation='relu')(joint)\n",
    "joint = Dropout(0.5)(joint)\n",
    "predictions = Dense(num_classes, activation='sigmoid')(joint)\n",
    "\n",
    "full_model = Model(inputs=[base_model.input, text_inputs], outputs=[predictions])\n",
    "\n",
    "full_model.compile(loss='categorical_crossentropy',\n",
    "                   optimizer='rmsprop',\n",
    "                   metrics=['accuracy'])\n",
    "\n",
    "history = full_model.fit([X_train_image, X_train_text], y_train,\n",
    "                         epochs=epochs, batch_size=batch_size,\n",
    "                         verbose=1, validation_split=0.2, shuffle=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
