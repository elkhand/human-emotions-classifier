{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Image & Caption joint training\n",
    "\n",
    "https://gist.github.com/elkhand/412f9dc4cd1a72c4571354e81c93d695"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Imports for Caption model\n",
    "\n",
    "import os, sys, io,re, string, pathlib, random\n",
    "from collections import Counter, OrderedDict\n",
    "\n",
    "import hecutils.data_utils as dt\n",
    "import hecutils.scoring_utils as sc\n",
    "import hecutils.plotting_utils as pt\n",
    "import hecutils.caption_utils as caput\n",
    "import hecutils.image_utils as imut\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.callbacks import Callback, EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "from keras.layers import Dense, Conv1D, MaxPooling1D, GRU, Bidirectional, LSTM\n",
    "from keras.layers.core import Dropout, Flatten, Masking, ActivityRegularization\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from keras.utils import plot_model\n",
    "from keras import regularizers\n",
    "from keras.preprocessing import text\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "from sklearn.utils import shuffle, class_weight\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from gensim.models import word2vec\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "################################################################################################################\n",
    "# Imports for Image model\n",
    "\n",
    "import os, sys, re, string, pathlib, random, io, time, glob\n",
    "from collections import Counter, OrderedDict\n",
    "from shutil import copyfile, rmtree\n",
    "\n",
    "#import hecutils.resnet152 as resnet\n",
    "from hecutils.resnet152 import ResNet152\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import hecutils.data_utils as dt\n",
    "import hecutils.scoring_utils as sc\n",
    "import hecutils.plotting_utils as pt\n",
    "import hecutils.image_utils as imut\n",
    "\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, Callback\n",
    "from keras.layers import Concatenate, MaxPooling2D, Conv2D, ZeroPadding2D, merge, Input, GRU, Bidirectional, LSTM, MaxPooling1D, Conv1D,Dense, GlobalAveragePooling2D, GlobalMaxPooling2D\n",
    "from keras.layers.core import Dropout, Flatten, Masking, ActivityRegularization\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from keras.utils import plot_model\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from keras.preprocessing import text\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from keras.models import Model\n",
    "from keras import optimizers\n",
    "from keras.regularizers import l2\n",
    "\n",
    "# from fastText import load_model\n",
    "from gensim.models import word2vec\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle, class_weight\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Keras to use Tensorflow GPU in the backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "num_cores = 8\n",
    "GPU=True\n",
    "CPU = not GPU\n",
    "\n",
    "if GPU:\n",
    "    num_GPU = 1\n",
    "    num_CPU = 8\n",
    "if CPU:\n",
    "    num_CPU = 8\n",
    "    num_GPU = 0\n",
    "\n",
    "config = tf.ConfigProto(intra_op_parallelism_threads=num_cores,\\\n",
    "        inter_op_parallelism_threads=num_cores, allow_soft_placement=True,\\\n",
    "        device_count = {'CPU' : num_CPU, 'GPU' : num_GPU})\n",
    "session = tf.Session(config=config)\n",
    "K.set_session(session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caption model\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "\n",
    "oasis_csv_path = \"dataset/metadata/OASIS.csv\"\n",
    "captions_root = \"/home/elkhand/git-repos/human-emotions-classifier/dataset/metadata\"\n",
    "captions_root_path = pathlib.Path(captions_root)\n",
    "human_output_caption_to_label_csv_path = captions_root_path/'humanCaptionWithLabeldf.csv'\n",
    "human_caption_csv_path = captions_root_path/'captions.csv'\n",
    "#fasttext_embedding_path = 'embedding/wiki-news-300d-1M.vec'\n",
    "fasttext_embedding_path = '/home/elkhand/datasets/glove-vectors/glove.twitter.27B.200d.txt'\n",
    "model_results_root_dir = \"model/\"\n",
    "inputDataset_csv_path = captions_root_path/\"inputDataset.csv\"\n",
    "testDataset_csv_path = captions_root_path/\"testDataset.csv\"\n",
    "\n",
    "neutralLow = 3.0 \n",
    "neutralHigh = 5.0\n",
    "\n",
    "auto_output_caption_to_label_csv_path = captions_root_path/'autoCaptionWithLabeldf.csv'\n",
    "auto_caption_csv_path = captions_root_path/'auto_generated_captions.csv'\n",
    "\n",
    "\n",
    "dataset_path = human_output_caption_to_label_csv_path\n",
    "# dataset_path = auto_output_caption_to_label_csv_path\n",
    "\n",
    "kfold_splits = 7 # 10 # 7 # 5 # 10 # 7 \n",
    "test_size = 0.1\n",
    "\n",
    "embedding_dimension = 200 # 300\n",
    "hidden_layer_dim = 32\n",
    "batch_size = 16 # 64\n",
    "nb_epochs = 100\n",
    "dropout = 0.3\n",
    "recurrent_dropout=  0.6\n",
    "patience = 10\n",
    "verbose = 1\n",
    "\n",
    "useF1Score = False # True\n",
    "\n",
    "################################################################################################################\n",
    "# Image model\n",
    "\n",
    "oasis_csv_path = \"dataset/metadata/OASIS.csv\"\n",
    "oasis_images_src = \"dataset/images/\"\n",
    "input_images_src = \"dataset/input-joint/\"\n",
    "test_images_src = \"dataset/test-joint/\"\n",
    "model_results_root_dir = \"img_model-joint/\"\n",
    "\n",
    "input_images_classified = \"dataset/input-classified-joint/\"\n",
    "test_images_classified = \"dataset/test-classified-joint/\"\n",
    "\n",
    "# ou can downlaod weights here: https://gist.github.com/flyyufelix/7e2eafb149f72f4d38dd661882c554a6\n",
    "weights_path = \"/home/elkhand/weights/resnet152_weights_tf.h5\"\n",
    "\n",
    "dataset_groups=[\"train\", \"val\"]\n",
    "classes = [\"negative\", \"neutral\", \"positive\"]\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "\n",
    "test_size = 0.1\n",
    "\n",
    "kfold_splits =  7 #5\n",
    "\n",
    "neutralLow = 3.0\n",
    "neutralHigh = 5.0\n",
    "\n",
    "nb_epochs = 100\n",
    "patience = 10 # ReduceLROnPlateau has 5\n",
    "batch_size = 32 # 32  \n",
    "\n",
    "FC_SIZE = 128 # 1024\n",
    "LAYERS_TO_UNFREEZE = 10\n",
    "\n",
    "img_height = 224 # 299\n",
    "img_width = 224  # 299\n",
    "\n",
    "useF1Score = False\n",
    "verbose=1 \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create <caption,label> CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.create_caption_to_label(oasis_csv_path,human_caption_csv_path, human_output_caption_to_label_csv_path,neutralLow, neutralHigh)\n",
    "dt.create_caption_to_label(oasis_csv_path,auto_caption_csv_path, auto_output_caption_to_label_csv_path,neutralLow, neutralHigh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide data into train/val/test datasets\n",
    "\n",
    "Read dataframe to have:\n",
    "\n",
    "<imageName, caption, label>\n",
    "\n",
    "1. Read into df <imageId, label>\n",
    "2. Then separate data into input and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Label distribution in inputDataset label\n",
      "negative    147\n",
      "neutral     378\n",
      "positive    285\n",
      "Name: label, dtype: int64\n",
      "Label distribution in testDataset label\n",
      "negative    16\n",
      "neutral     42\n",
      "positive    32\n",
      "Name: label, dtype: int64\n",
      "Input data size 810\n",
      "Test data size 90\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>caption</th>\n",
       "      <th>label</th>\n",
       "      <th>image_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I1</td>\n",
       "      <td>two acorns lying ground next oak leaves.</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Acorns 1.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I100</td>\n",
       "      <td>ruined walls church backdrop white clouds blue...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Building 2.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I101</td>\n",
       "      <td>man free fall attached blue bungee jumping app...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Bungee jumping 1.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I102</td>\n",
       "      <td>falling man attached bungee jumping apparatus....</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Bungee jumping 2.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I104</td>\n",
       "      <td>man kneeling front tent two similar-looking gi...</td>\n",
       "      <td>positive</td>\n",
       "      <td>Camping 1.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                            caption     label  \\\n",
       "0    I1           two acorns lying ground next oak leaves.   neutral   \n",
       "2  I100  ruined walls church backdrop white clouds blue...   neutral   \n",
       "3  I101  man free fall attached blue bungee jumping app...   neutral   \n",
       "4  I102  falling man attached bungee jumping apparatus....   neutral   \n",
       "6  I104  man kneeling front tent two similar-looking gi...  positive   \n",
       "\n",
       "             image_name  \n",
       "0          Acorns 1.jpg  \n",
       "2        Building 2.jpg  \n",
       "3  Bungee jumping 1.jpg  \n",
       "4  Bungee jumping 2.jpg  \n",
       "6         Camping 1.jpg  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfImageIdCaptionLabel = pd.read_csv(dataset_path, header=0, sep=\"|\")\n",
    "dfImageIdCaptionLabel.columns = [\"id\",\"caption\", \"label\"]\n",
    "dfImageIdCaptionLabel[\"caption\"] = dfImageIdCaptionLabel[\"caption\"].apply(lambda x: \" \".join(caput.get_words_withoutstopwords(x.lower().split())))\n",
    "#dfImageIdCaptionLabel[\"label\"] = dfImageIdCaptionLabel[\"label\"].apply(lambda x: caput.change_label_str_to_int(x))\n",
    "\n",
    "\n",
    "dfImageIdImageName = dt.get_image_id_to_image_title_as_df(oasis_csv_path)\n",
    "dfImageIdImageName.columns = ['id', 'image_name']\n",
    "dfImageIdImageName['image_name'] = dfImageIdImageName['image_name'].apply(lambda x: x + \".jpg\") \n",
    "printCnt = 5\n",
    "# has [id, caption, label]\n",
    "df = pd.merge(dfImageIdCaptionLabel, dfImageIdImageName, on=\"id\")\n",
    "#print(df.head(printCnt))\n",
    "\n",
    "\n",
    "\n",
    "input_x, test_x, input_y,  test_y = train_test_split(df[\"id\"],\n",
    "                                                     df[\"label\"],\n",
    "                                                     test_size=test_size,\n",
    "                                                     random_state=seed,\n",
    "                                                     stratify=df[\"label\"])\n",
    "\n",
    "inputDataset = pd.concat([input_x, input_y], axis=1)\n",
    "testDataset = pd.concat([test_x, test_y], axis=1)\n",
    "\n",
    "inputDataset = inputDataset.dropna()\n",
    "testDataset = testDataset.dropna()\n",
    "inputDataset = inputDataset.reset_index()\n",
    "testDataset = testDataset.reset_index()\n",
    "\n",
    "# print(\"inputDataset\\n\", inputDataset.head(10))\n",
    "# print(\"testDataset\\n\", testDataset.head(10))\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Label distribution in inputDataset\", inputDataset.groupby('label').label.count())\n",
    "print(\"Label distribution in testDataset\", testDataset.groupby('label').label.count())\n",
    "\n",
    "\n",
    "inputData = df.loc[df['id'].isin(inputDataset.id)]\n",
    "testData = df.loc[df['id'].isin(testDataset.id)]\n",
    "\n",
    "# print(\"inputData\\n\", inputData.head())\n",
    "# print(\"testData\\n\", testData.head())\n",
    "\n",
    "inputIds = set(inputData['id'].values)\n",
    "testIds = set(testData['id'].values)\n",
    "\n",
    "print(\"Input data size\", len(inputIds))\n",
    "print(\"Test data size\", len(testIds))\n",
    "\n",
    "for inputId in inputIds:\n",
    "    if inputId in testIds:\n",
    "        raise inputId + \" inputId exists both in test and input dataset\"\n",
    "        \n",
    "for testId in testIds:\n",
    "    if testId in inputIds:\n",
    "        raise testId + \" testId exists both in test and input dataset\"        \n",
    "\n",
    "inputData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating test and input dataset, and `positive,neutral,negative` under each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete input images dir\n",
    "rmtree(input_images_src, ignore_errors=True)\n",
    "os.makedirs(input_images_src)\n",
    "\n",
    "\n",
    "# Delete test images dir\n",
    "rmtree(test_images_src, ignore_errors=True)\n",
    "os.makedirs(test_images_src)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Copy input images into input dir, and test images into test dir\n",
    "imut.copy_imgs_into(oasis_images_src, inputData['image_name'], input_images_src)\n",
    "imut.copy_imgs_into(oasis_images_src, testData['image_name'], test_images_src)\n",
    "\n",
    "# Divide input images into train and dev set, and each one into {negative, neutral, positive}\n",
    "isForTest = False\n",
    "X_train = inputData['image_name']\n",
    "y_train = inputData['label']\n",
    "dt.create_dataset(\"train\", input_images_src, input_images_classified, X_train, y_train, isForTest)\n",
    "#X_val = inputData['image_name'] # TODO COrrect\n",
    "#y_val = inputData['label']\n",
    "#dt.create_dataset(\"val\", input_images_src, input_images_classified, X_val, y_val, isForTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Fasttext Embeddings\n",
    "\n",
    "You can download fasttext word vectors from here:\n",
    "\n",
    "https://fasttext.cc/docs/en/english-vectors.html\n",
    "\n",
    "https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki-news-300d-1M.vec.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding size : 1193514\n",
      "embedding dimension : (200,)\n"
     ]
    }
   ],
   "source": [
    "def load_embedding(path):\n",
    "    word2vec = {}\n",
    "    with io.open(path, encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            entries = line.rstrip().split(\" \")\n",
    "            word, entries = entries[0], entries[1:]\n",
    "            word2vec[word] = np.array(entries).astype(np.float) # Convert String type to float\n",
    "    print('embedding size : %d' % len(word2vec))\n",
    "    print('embedding dimension : %s' % (word2vec['apple'].shape,))\n",
    "    return word2vec\n",
    "    \n",
    "wordToVec = {}\n",
    "wordToVec = load_embedding(fasttext_embedding_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joint model, which will learn both from images and captions together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def build_model(max_seq_len, num_of_classes, config): \n",
    "    #model = Sequential()\n",
    "    max_words = 5000\n",
    "    text_inputs = Input(shape=(None, config['embedding_dimension']))\n",
    "    masking = Masking(mask_value=0., )(text_inputs) #input_shape=(None, config['embedding_dimension'])\n",
    "    lstm1 = LSTM(max_seq_len, return_sequences=True, dropout=config['dropout'], recurrent_dropout=config['recurrent_dropout'])(text_inputs)\n",
    "    branch_1 = LSTM(max_seq_len, dropout=config['dropout'], recurrent_dropout=config['recurrent_dropout'])(lstm1)\n",
    "    \n",
    "    # Image input branch - a pre-trained Inception module followed by an added fully connected layer\n",
    "    base_model = InceptionV3(weights='imagenet', include_top=False)\n",
    "    # Freeze Inception's weights - we don't want to train these\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # add a fully connected layer after Inception - we do want to train these\n",
    "    branch_2 = base_model.output\n",
    "    branch_2 = GlobalAveragePooling2D()(branch_2)\n",
    "    branch_2 = Dense(1024, activation='relu')(branch_2)\n",
    "\n",
    "    # merge the text input branch and the image input branch and add another fully connected layer\n",
    "    joint = merge([branch_1, branch_2], mode='concat')\n",
    "    joint = Dense(512, activation='relu')(joint)\n",
    "    joint = Dropout(0.5)(joint)\n",
    "    predictions = Dense(num_of_classes, activation='softmax')(joint)\n",
    "    \n",
    "    \n",
    "    if config['useF1Score']:\n",
    "        metrics = ['accuracy', sc.f1, sc.recall, sc.precision]\n",
    "    else:\n",
    "        metrics = ['accuracy']\n",
    "        \n",
    "    full_model = Model(inputs=[base_model.input, text_inputs], outputs=[predictions])\n",
    "\n",
    "    full_model.compile(loss='categorical_crossentropy',\n",
    "                   optimizer='adam', # 'rmsprop'\n",
    "                   metrics=metrics)\n",
    "    print(full_model.summary())\n",
    "    return full_model\n",
    "\n",
    "\n",
    "def get_config():\n",
    "    conf = {}\n",
    "    conf[\"kfold_splits\"] = kfold_splits\n",
    "    conf[\"batch_size\"] = batch_size\n",
    "    conf['embedding_dimension'] = embedding_dimension\n",
    "    conf['recurrent_dropout'] = recurrent_dropout\n",
    "    conf['dropout'] = dropout\n",
    "    conf[\"nb_epochs\"] = nb_epochs\n",
    "    conf['useF1Score'] = useF1Score\n",
    "    conf['verbose'] = verbose\n",
    "    conf['useF1Score'] = useF1Score\n",
    "    conf['seed'] = seed\n",
    "    conf[\"img_height\"] = img_height\n",
    "    conf[\"img_width\"] = img_width\n",
    "    conf[\"kfold_splits\"] = kfold_splits\n",
    "    conf[\"batch_size\"] = batch_size\n",
    "    conf[\"nb_epochs\"] = nb_epochs\n",
    "    conf['useF1Score'] = useF1Score\n",
    "    conf['verbose'] = verbose\n",
    "    return conf \n",
    "\n",
    "#full_model = build_model(40, 3, get_config())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.preprocessing.image.DirectoryIterator object at 0x7fe293adb3c8>\n",
      "     id                                            caption     label  \\\n",
      "0    I1           two acorns lying ground next oak leaves.   neutral   \n",
      "2  I100  ruined walls church backdrop white clouds blue...   neutral   \n",
      "3  I101  man free fall attached blue bungee jumping app...   neutral   \n",
      "4  I102  falling man attached bungee jumping apparatus....   neutral   \n",
      "6  I104  man kneeling front tent two similar-looking gi...  positive   \n",
      "\n",
      "             image_name  \n",
      "0          Acorns 1.jpg  \n",
      "2        Building 2.jpg  \n",
      "3  Bungee jumping 1.jpg  \n",
      "4  Bungee jumping 2.jpg  \n",
      "6         Camping 1.jpg  \n",
      "['Acorns 1.jpg' 'Alcohol 7.jpg']\n",
      "input_images_src dataset/input-joint/\n",
      "[array([[[176., 155., 174.],\n",
      "        [166., 148., 162.],\n",
      "        [163., 145., 171.],\n",
      "        ...,\n",
      "        [245., 244., 213.],\n",
      "        [237., 248., 208.],\n",
      "        [255., 247., 218.]],\n",
      "\n",
      "       [[171., 148., 168.],\n",
      "        [178., 156., 179.],\n",
      "        [163., 144., 164.],\n",
      "        ...,\n",
      "        [250., 247., 214.],\n",
      "        [251., 252., 218.],\n",
      "        [254., 238., 212.]],\n",
      "\n",
      "       [[177., 153., 175.],\n",
      "        [170., 145., 174.],\n",
      "        [169., 148., 165.],\n",
      "        ...,\n",
      "        [251., 242., 209.],\n",
      "        [255., 247., 216.],\n",
      "        [240., 213., 186.]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[ 79.,  90.,  50.],\n",
      "        [ 70.,  84.,  69.],\n",
      "        [ 60.,  67.,  60.],\n",
      "        ...,\n",
      "        [ 71.,  75.,  78.],\n",
      "        [ 43.,  51.,  36.],\n",
      "        [ 90.,  91.,  77.]],\n",
      "\n",
      "       [[117., 123., 109.],\n",
      "        [113., 126., 116.],\n",
      "        [  6.,  13.,   5.],\n",
      "        ...,\n",
      "        [ 69.,  74.,  94.],\n",
      "        [128., 140., 130.],\n",
      "        [ 59.,  58.,  54.]],\n",
      "\n",
      "       [[ 28.,  34.,  22.],\n",
      "        [ 81.,  97.,  71.],\n",
      "        [ 73.,  84.,  54.],\n",
      "        ...,\n",
      "        [ 87.,  94.,  87.],\n",
      "        [ 46.,  57.,  40.],\n",
      "        [ 57.,  54.,  45.]]], dtype=float32), array([[[ 20.,  48.,  96.],\n",
      "        [ 21.,  52.,  99.],\n",
      "        [ 19.,  49.,  99.],\n",
      "        ...,\n",
      "        [ 88.,  96., 107.],\n",
      "        [ 82.,  90., 101.],\n",
      "        [ 77.,  85.,  96.]],\n",
      "\n",
      "       [[ 20.,  50., 100.],\n",
      "        [ 22.,  52., 102.],\n",
      "        [ 21.,  51., 101.],\n",
      "        ...,\n",
      "        [ 83.,  91., 102.],\n",
      "        [ 78.,  86.,  97.],\n",
      "        [ 72.,  80.,  91.]],\n",
      "\n",
      "       [[ 22.,  54., 103.],\n",
      "        [ 25.,  57., 106.],\n",
      "        [ 24.,  56., 107.],\n",
      "        ...,\n",
      "        [ 80.,  88.,  99.],\n",
      "        [ 73.,  81.,  92.],\n",
      "        [ 68.,  76.,  87.]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[ 79.,  65.,  39.],\n",
      "        [ 97.,  88.,  49.],\n",
      "        [126., 120.,  86.],\n",
      "        ...,\n",
      "        [ 71.,  70.,  65.],\n",
      "        [ 21.,  20.,  15.],\n",
      "        [ 43.,  42.,  37.]],\n",
      "\n",
      "       [[ 56.,  45.,  25.],\n",
      "        [ 93.,  78.,  57.],\n",
      "        [ 78.,  63.,  40.],\n",
      "        ...,\n",
      "        [ 69.,  68.,  63.],\n",
      "        [ 29.,  28.,  23.],\n",
      "        [ 86.,  85.,  80.]],\n",
      "\n",
      "       [[ 31.,  15.,   2.],\n",
      "        [ 48.,  23.,  18.],\n",
      "        [ 59.,  33.,  18.],\n",
      "        ...,\n",
      "        [ 63.,  63.,  55.],\n",
      "        [ 68.,  67.,  63.],\n",
      "        [ 69.,  68.,  64.]]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "#isForTrain = True\n",
    "#trainDir = input_images_classified + \"/\" + \"train\" + \"/\"\n",
    "#train_batches = imut.get_data_generator(trainDir, get_config(), isForTrain)  \n",
    "\n",
    "print(train_batches)\n",
    "print(inputData.head())\n",
    "\n",
    "\n",
    "\n",
    "#image_names, image_labels = dt.get_image_name_and_label(oasis_csv_path, neutralLow, neutralHigh)\n",
    "#image_names = np.array(image_names)\n",
    "#image_labels = np.array(image_labels)\n",
    "print(image_names[:2])\n",
    "print(\"input_images_src\", input_images_src)\n",
    "\n",
    "X_train = []\n",
    "for img_name in inputData['image_name']:\n",
    "    fpath = input_images_src + img_name\n",
    "    img = load_img(fpath, target_size=(224,224))\n",
    "    x = img_to_array(img)  # this is a Numpy array with shape (3, 150, 150)\n",
    "    X_train.append(x)\n",
    "    \n",
    "print(X_train[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 two acorns lying ground next oak leaves. neutral\n",
      "2 ruined walls church backdrop white clouds blue sky. neutral\n",
      "3 man free fall attached blue bungee jumping apparatus. river mountain him. wearing white shirt blue pants. neutral\n",
      "4 falling man attached bungee jumping apparatus. wearing white shirt black sunglasses. neutral\n",
      "6 man kneeling front tent two similar-looking girls sides. hands rest shoulder, one top other. positive\n",
      "7 group young students classroom knees hands folded. neutral\n",
      "9 man boy playing board game camping site tabletop supported logs tied together. positive\n",
      "10 man wearing blue jacket headlamp lighting fire pile wood kindling front him. neutral\n",
      "11 three young boys older boy blue white canoe. three young ones orange lifevests on. positive\n",
      "12 two glasses champagne bread cheese plate, outdoors front body water pier. positive\n",
      "15 tent glowing light inside, pitched snow field snow covered mountains background. neutral\n",
      "16 assembly lit orange cream candles different sizes set table. positive\n",
      "17 red black supercar front wheels turned right. positive\n",
      "18 vintage borgward car blue yellow color, rusty rio de janeiro license plate. car leaves hood parked nature trees background leaves floor. neutral\n",
      "20 blue car front end severely damaged. negative\n",
      "21 pieces glass right front side car shattered flying air. negative\n",
      "                                              caption     label\n",
      "0            two acorns lying ground next oak leaves.   neutral\n",
      "2   ruined walls church backdrop white clouds blue...   neutral\n",
      "3   man free fall attached blue bungee jumping app...   neutral\n",
      "4   falling man attached bungee jumping apparatus....   neutral\n",
      "6   man kneeling front tent two similar-looking gi...  positive\n",
      "7   group young students classroom knees hands fol...   neutral\n",
      "9   man boy playing board game camping site tablet...  positive\n",
      "10  man wearing blue jacket headlamp lighting fire...   neutral\n",
      "11  three young boys older boy blue white canoe. t...  positive\n",
      "12  two glasses champagne bread cheese plate, outd...  positive\n",
      "15  tent glowing light inside, pitched snow field ...   neutral\n",
      "16  assembly lit orange cream candles different si...  positive\n",
      "17      red black supercar front wheels turned right.  positive\n",
      "18  vintage borgward car blue yellow color, rusty ...   neutral\n",
      "20               blue car front end severely damaged.  negative\n",
      "class_to_index {'neutral': 0, 'positive': 1, 'negative': 2}\n",
      "[[[ 0.45059  -0.54344  -0.044098 ...  0.62188   0.41809  -0.022259]\n",
      "  [ 0.02107  -0.28299  -0.11678  ... -0.021647 -0.46166  -0.71745 ]\n",
      "  [-0.39633  -0.12244  -0.47945  ...  0.31324  -0.28907   0.033147]\n",
      "  ...\n",
      "  [ 0.        0.        0.       ...  0.        0.        0.      ]\n",
      "  [ 0.        0.        0.       ...  0.        0.        0.      ]\n",
      "  [ 0.        0.        0.       ...  0.        0.        0.      ]]\n",
      "\n",
      " [[ 0.18196  -0.019551  0.30295  ...  0.38334  -0.17505  -0.22327 ]\n",
      "  [ 0.26315  -0.34248   0.51966  ... -0.27671  -0.6327   -0.43191 ]\n",
      "  [-0.18397   0.013893  0.61283  ... -0.42203  -0.37829  -0.18466 ]\n",
      "  ...\n",
      "  [ 0.        0.        0.       ...  0.        0.        0.      ]\n",
      "  [ 0.        0.        0.       ...  0.        0.        0.      ]\n",
      "  [ 0.        0.        0.       ...  0.        0.        0.      ]]]\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "X = inputData[\"caption\"]\n",
    "y = inputData[\"label\"]\n",
    "max_seq_len = int(inputData['caption'].map(lambda x: caput.get_non_stop_word_count(x.split())).max())\n",
    "\n",
    "\n",
    "dfTrain = pd.concat([X, y], axis=1)\n",
    "dfTrain.columns = ['caption', 'label']\n",
    "\n",
    "class_to_index = {}\n",
    "index_to_class = {}\n",
    "\n",
    "print(inputData.head(15))\n",
    "\n",
    "cnt = 0\n",
    "for index, row in dfTrain.iterrows():\n",
    "    print(index, row['caption'],row['label'])\n",
    "    cnt +=1\n",
    "    if cnt >15:\n",
    "        break\n",
    "\n",
    "print(dfTrain.head(15))        \n",
    "\n",
    "X_train_text, y_train_index, num_of_classes, class_to_index, index_to_class = \\\n",
    "            caput.load_dataset_StratifiedKFold(\n",
    "                            dfTrain,\n",
    "                            wordToVec, \n",
    "                            max_seq_len, \n",
    "                            class_to_index, \n",
    "                            index_to_class,\n",
    "                            get_config())\n",
    "y_train = caput.convert_index_to_one_hot(y_train_index, num_of_classes) \n",
    "        \n",
    "#y_ints = [y.argmax() for y in y_train]\n",
    "# class_weights = class_weight.compute_class_weight('balanced',\n",
    "#                                                  np.unique(y_ints),\n",
    "#                                                  y_ints)\n",
    "#print(\"class_weights\", class_weights)\n",
    "print(\"class_to_index\", class_to_index)\n",
    "print(X_train_text[:2])\n",
    "print(type(X_train_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]]\n",
      "[[1. 0. 0.]\n",
      " [1. 0. 0.]]\n",
      "<class 'numpy.ndarray'>\n",
      "Train on 648 samples, validate on 162 samples\n",
      "Epoch 1/100\n",
      "648/648 [==============================] - 6s 9ms/step - loss: 0.6353 - acc: 0.7315 - val_loss: 11.4253 - val_acc: 0.2840\n",
      "Epoch 2/100\n",
      "648/648 [==============================] - 3s 4ms/step - loss: 0.5296 - acc: 0.7716 - val_loss: 11.5764 - val_acc: 0.2778\n",
      "Epoch 3/100\n",
      "648/648 [==============================] - 3s 4ms/step - loss: 0.5136 - acc: 0.7793 - val_loss: 11.3108 - val_acc: 0.2840\n",
      "Epoch 4/100\n",
      "648/648 [==============================] - 3s 5ms/step - loss: 0.4505 - acc: 0.8225 - val_loss: 11.5415 - val_acc: 0.2840\n",
      "Epoch 5/100\n",
      "648/648 [==============================] - 3s 4ms/step - loss: 0.4452 - acc: 0.8133 - val_loss: 11.5414 - val_acc: 0.2840\n",
      "Epoch 6/100\n",
      "648/648 [==============================] - 3s 5ms/step - loss: 0.3434 - acc: 0.8580 - val_loss: 11.3471 - val_acc: 0.2901\n",
      "Epoch 7/100\n",
      "648/648 [==============================] - 3s 4ms/step - loss: 0.4176 - acc: 0.8164 - val_loss: 11.2685 - val_acc: 0.2778\n",
      "Epoch 8/100\n",
      "648/648 [==============================] - 3s 5ms/step - loss: 0.3082 - acc: 0.8750 - val_loss: 11.4584 - val_acc: 0.2778\n",
      "Epoch 9/100\n",
      "648/648 [==============================] - 3s 4ms/step - loss: 0.3648 - acc: 0.8534 - val_loss: 11.5934 - val_acc: 0.2593\n",
      "Epoch 10/100\n",
      "648/648 [==============================] - 3s 5ms/step - loss: 0.4152 - acc: 0.8318 - val_loss: 11.4960 - val_acc: 0.2840\n",
      "Epoch 11/100\n",
      "648/648 [==============================] - 3s 5ms/step - loss: 0.3069 - acc: 0.8843 - val_loss: 11.3922 - val_acc: 0.2778\n",
      "Epoch 12/100\n",
      "648/648 [==============================] - 3s 5ms/step - loss: 0.2522 - acc: 0.9120 - val_loss: 11.2883 - val_acc: 0.2778\n",
      "Epoch 13/100\n",
      "648/648 [==============================] - 3s 5ms/step - loss: 0.2422 - acc: 0.9151 - val_loss: 11.5414 - val_acc: 0.2840\n",
      "Epoch 14/100\n",
      "648/648 [==============================] - 3s 5ms/step - loss: 0.1633 - acc: 0.9522 - val_loss: 11.4753 - val_acc: 0.2716\n",
      "Epoch 15/100\n",
      "648/648 [==============================] - 3s 5ms/step - loss: 0.2113 - acc: 0.9228 - val_loss: 11.4569 - val_acc: 0.2840\n",
      "Epoch 16/100\n",
      "648/648 [==============================] - 3s 5ms/step - loss: 0.2129 - acc: 0.9228 - val_loss: 10.8645 - val_acc: 0.3148\n",
      "Epoch 17/100\n",
      "648/648 [==============================] - 3s 5ms/step - loss: 0.3710 - acc: 0.8503 - val_loss: 11.5787 - val_acc: 0.2778\n",
      "Epoch 18/100\n",
      "648/648 [==============================] - 3s 5ms/step - loss: 0.2358 - acc: 0.9182 - val_loss: 11.2398 - val_acc: 0.2963\n",
      "Epoch 19/100\n",
      "648/648 [==============================] - 3s 5ms/step - loss: 0.2451 - acc: 0.8981 - val_loss: 11.3697 - val_acc: 0.2716\n",
      "Epoch 20/100\n",
      "648/648 [==============================] - 3s 4ms/step - loss: 0.2598 - acc: 0.9120 - val_loss: 11.5414 - val_acc: 0.2840\n",
      "Epoch 21/100\n",
      "648/648 [==============================] - 3s 5ms/step - loss: 0.1200 - acc: 0.9645 - val_loss: 11.6408 - val_acc: 0.2778\n",
      "Epoch 22/100\n",
      "648/648 [==============================] - 3s 5ms/step - loss: 0.1982 - acc: 0.9228 - val_loss: 11.6408 - val_acc: 0.2778\n",
      "Epoch 23/100\n",
      "648/648 [==============================] - 3s 5ms/step - loss: 0.2188 - acc: 0.9213 - val_loss: 11.5768 - val_acc: 0.2778\n",
      "Epoch 24/100\n",
      "648/648 [==============================] - 3s 5ms/step - loss: 0.1174 - acc: 0.9630 - val_loss: 10.9878 - val_acc: 0.3025\n",
      "Epoch 25/100\n",
      "648/648 [==============================] - 3s 5ms/step - loss: 0.1804 - acc: 0.9306 - val_loss: 11.5067 - val_acc: 0.2840\n",
      "Epoch 26/100\n",
      "648/648 [==============================] - 3s 5ms/step - loss: 0.1625 - acc: 0.9491 - val_loss: 11.4675 - val_acc: 0.2840\n",
      "Epoch 27/100\n",
      "648/648 [==============================] - 3s 5ms/step - loss: 0.1297 - acc: 0.9491 - val_loss: 11.6408 - val_acc: 0.2778\n",
      "Epoch 28/100\n",
      "648/648 [==============================] - 3s 5ms/step - loss: 0.1337 - acc: 0.9552 - val_loss: 11.5414 - val_acc: 0.2840\n",
      "Epoch 29/100\n",
      "648/648 [==============================] - 3s 5ms/step - loss: 0.1647 - acc: 0.9414 - val_loss: 11.4370 - val_acc: 0.2901\n",
      "Epoch 30/100\n",
      "648/648 [==============================] - 3s 5ms/step - loss: 0.0902 - acc: 0.9676 - val_loss: 11.4017 - val_acc: 0.2716\n",
      "Epoch 31/100\n",
      "648/648 [==============================] - 3s 5ms/step - loss: 0.2399 - acc: 0.9151 - val_loss: 11.5957 - val_acc: 0.2716\n",
      "Epoch 32/100\n",
      "648/648 [==============================] - 3s 4ms/step - loss: 0.1613 - acc: 0.9460 - val_loss: 11.5092 - val_acc: 0.2840\n",
      "Epoch 33/100\n",
      "648/648 [==============================] - 3s 5ms/step - loss: 0.1192 - acc: 0.9660 - val_loss: 11.6416 - val_acc: 0.2778\n",
      "Epoch 34/100\n",
      "648/648 [==============================] - 3s 4ms/step - loss: 0.0594 - acc: 0.9815 - val_loss: 11.3765 - val_acc: 0.2840\n",
      "Epoch 35/100\n",
      "648/648 [==============================] - 3s 4ms/step - loss: 0.0947 - acc: 0.9660 - val_loss: 11.4788 - val_acc: 0.2840\n",
      "Epoch 36/100\n",
      "648/648 [==============================] - 3s 4ms/step - loss: 0.0735 - acc: 0.9769 - val_loss: 11.6104 - val_acc: 0.2778\n",
      "Epoch 37/100\n",
      "648/648 [==============================] - 3s 5ms/step - loss: 0.0743 - acc: 0.9722 - val_loss: 11.6408 - val_acc: 0.2778\n",
      "Epoch 38/100\n",
      "648/648 [==============================] - 3s 5ms/step - loss: 0.1090 - acc: 0.9614 - val_loss: 11.4933 - val_acc: 0.2716\n",
      "Epoch 39/100\n",
      "648/648 [==============================] - 3s 4ms/step - loss: 0.1622 - acc: 0.9367 - val_loss: 11.7243 - val_acc: 0.2716\n",
      "Epoch 40/100\n",
      "648/648 [==============================] - 3s 4ms/step - loss: 0.0872 - acc: 0.9583 - val_loss: 11.7748 - val_acc: 0.2593\n",
      "Epoch 41/100\n",
      "648/648 [==============================] - 3s 5ms/step - loss: 0.0616 - acc: 0.9815 - val_loss: 11.7838 - val_acc: 0.2593\n",
      "Epoch 42/100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-116-f49ac7b152b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m                                    \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                                    \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                                    verbose=1) \n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m#,  validation_split=0.2,epochs=epochs,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cs231n/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/cs231n/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1234\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1236\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1237\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cs231n/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2480\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2481\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2482\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2483\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cs231n/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cs231n/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1140\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cs231n/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1321\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cs231n/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cs231n/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1312\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cs231n/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[1;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m             status, run_metadata)\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "print(y_train[:3])\n",
    "y_train = np.array(y_train)\n",
    "print(y_train[:2])\n",
    "print(type(y_train))\n",
    "history = full_model.fit([np.array(X_train), np.array(X_train_text)], np.array(y_train),\n",
    "                                   batch_size=batch_size,\n",
    "                                   epochs=100,\n",
    "                                   shuffle=True,\n",
    "                                   validation_split=0.2,\n",
    "                                   verbose=1) \n",
    "\n",
    "#,  validation_split=0.2,epochs=epochs,\n",
    "#  steps_per_epoch=  2000 // get_config()['batch_size'],"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
