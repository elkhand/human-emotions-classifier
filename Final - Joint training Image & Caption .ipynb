{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Image & Caption joint training\n",
    "\n",
    "https://gist.github.com/elkhand/412f9dc4cd1a72c4571354e81c93d695"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Imports for Caption model\n",
    "\n",
    "import os, sys, io,re, string, pathlib, random\n",
    "from collections import Counter, OrderedDict\n",
    "\n",
    "import hecutils.data_utils as dt\n",
    "import hecutils.scoring_utils as sc\n",
    "import hecutils.plotting_utils as pt\n",
    "import hecutils.caption_utils as caput\n",
    "import hecutils.image_utils as imut\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.callbacks import Callback, EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "from keras.layers import Dense, Conv1D, MaxPooling1D, GRU, Bidirectional, LSTM\n",
    "from keras.layers.core import Dropout, Flatten, Masking, ActivityRegularization\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from keras.utils import plot_model\n",
    "from keras import regularizers\n",
    "from keras.preprocessing import text\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "from sklearn.utils import shuffle, class_weight\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from gensim.models import word2vec\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "################################################################################################################\n",
    "# Imports for Image model\n",
    "\n",
    "import os, sys, re, string, pathlib, random, io, time, glob\n",
    "from collections import Counter, OrderedDict\n",
    "from shutil import copyfile, rmtree\n",
    "\n",
    "#import hecutils.resnet152 as resnet\n",
    "from hecutils.resnet152 import ResNet152\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import hecutils.data_utils as dt\n",
    "import hecutils.scoring_utils as sc\n",
    "import hecutils.plotting_utils as pt\n",
    "import hecutils.image_utils as imut\n",
    "\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, Callback\n",
    "from keras.layers import Concatenate, MaxPooling2D, Conv2D, ZeroPadding2D, merge, Input, GRU, Bidirectional, LSTM, MaxPooling1D, Conv1D,Dense, GlobalAveragePooling2D, GlobalMaxPooling2D\n",
    "from keras.layers.core import Dropout, Flatten, Masking, ActivityRegularization\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from keras.utils import plot_model\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.applications.inception_v3 import InceptionV3#, preprocess_input\n",
    "#from imagenet_utils import preprocess_input\n",
    "from keras_applications import imagenet_utils\n",
    "preprocess_input = imagenet_utils.preprocess_input\n",
    "\n",
    "from keras.preprocessing import text\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from keras.models import Model\n",
    "from keras import optimizers\n",
    "from keras.regularizers import l2\n",
    "\n",
    "# from fastText import load_model\n",
    "from gensim.models import word2vec\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle, class_weight\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Keras to use Tensorflow GPU in the backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "num_cores = 8\n",
    "GPU=True\n",
    "CPU = not GPU\n",
    "\n",
    "if GPU:\n",
    "    num_GPU = 1\n",
    "    num_CPU = 8\n",
    "if CPU:\n",
    "    num_CPU = 8\n",
    "    num_GPU = 0\n",
    "\n",
    "config = tf.ConfigProto(intra_op_parallelism_threads=num_cores,\\\n",
    "        inter_op_parallelism_threads=num_cores, allow_soft_placement=True,\\\n",
    "        device_count = {'CPU' : num_CPU, 'GPU' : num_GPU})\n",
    "session = tf.Session(config=config)\n",
    "K.set_session(session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caption model\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "\n",
    "oasis_csv_path = \"dataset/metadata/OASIS.csv\"\n",
    "captions_root = \"/home/elkhand/git-repos/human-emotions-classifier/dataset/metadata\"\n",
    "captions_root_path = pathlib.Path(captions_root)\n",
    "human_output_caption_to_label_csv_path = captions_root_path/'humanCaptionWithLabeldf.csv'\n",
    "human_caption_csv_path = captions_root_path/'captions.csv'\n",
    "#fasttext_embedding_path = 'embedding/wiki-news-300d-1M.vec'\n",
    "fasttext_embedding_path = '/home/elkhand/datasets/glove-vectors/glove.twitter.27B.100d.txt'\n",
    "#model_results_root_dir = \"model/\"\n",
    "inputDataset_csv_path = captions_root_path/\"inputDataset.csv\"\n",
    "testDataset_csv_path = captions_root_path/\"testDataset.csv\"\n",
    "\n",
    "neutralLow = 3.0 \n",
    "neutralHigh = 5.0\n",
    "\n",
    "auto_output_caption_to_label_csv_path = captions_root_path/'autoCaptionWithLabeldf.csv'\n",
    "auto_caption_csv_path = captions_root_path/'auto_generated_captions.csv'\n",
    "\n",
    "\n",
    "dataset_path = human_output_caption_to_label_csv_path\n",
    "# dataset_path = auto_output_caption_to_label_csv_path\n",
    "\n",
    "kfold_splits = 7 # 10 # 7 # 5 # 10 # 7 \n",
    "test_size = 0.1\n",
    "\n",
    "embedding_dimension = 200 # 300\n",
    "hidden_layer_dim = 32\n",
    "batch_size = 16 # 64\n",
    "nb_epochs = 100\n",
    "dropout = 0.3\n",
    "recurrent_dropout=  0.6\n",
    "patience = 10\n",
    "verbose = 1\n",
    "\n",
    "useF1Score = False # True\n",
    "\n",
    "################################################################################################################\n",
    "# Image model\n",
    "\n",
    "oasis_csv_path = \"dataset/metadata/OASIS.csv\"\n",
    "oasis_images_src = \"dataset/images/\"\n",
    "input_images_src = \"dataset/input-joint/\"\n",
    "test_images_src = \"dataset/test-joint/\"\n",
    "model_results_root_dir = \"img_model-joint/\"\n",
    "\n",
    "input_images_classified = \"dataset/input-classified-joint/\"\n",
    "test_images_classified = \"dataset/test-classified-joint/\"\n",
    "\n",
    "# ou can downlaod weights here: https://gist.github.com/flyyufelix/7e2eafb149f72f4d38dd661882c554a6\n",
    "weights_path = \"/home/elkhand/weights/resnet152_weights_tf.h5\"\n",
    "\n",
    "dataset_groups=[\"train\", \"val\"]\n",
    "classes = [\"negative\", \"neutral\", \"positive\"]\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "\n",
    "test_size = 0.1\n",
    "\n",
    "kfold_splits =  7 #5\n",
    "\n",
    "neutralLow = 3.0\n",
    "neutralHigh = 5.0\n",
    "\n",
    "nb_epochs = 100\n",
    "patience = 10 # ReduceLROnPlateau has 5\n",
    "batch_size = 32 # 32  \n",
    "\n",
    "FC_SIZE = 128 # 1024\n",
    "LAYERS_TO_UNFREEZE = 10\n",
    "\n",
    "img_height = 224 # 299\n",
    "img_width = 224  # 299\n",
    "\n",
    "useF1Score = False\n",
    "verbose=1 \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create <caption,label> CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.create_caption_to_label(oasis_csv_path,human_caption_csv_path, human_output_caption_to_label_csv_path,neutralLow, neutralHigh)\n",
    "dt.create_caption_to_label(oasis_csv_path,auto_caption_csv_path, auto_output_caption_to_label_csv_path,neutralLow, neutralHigh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide data into train/val/test datasets\n",
    "\n",
    "Read dataframe to have:\n",
    "\n",
    "<imageName, caption, label>\n",
    "\n",
    "1. Read into df <imageId, label>\n",
    "2. Then separate data into input and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Label distribution in inputDataset label\n",
      "negative    147\n",
      "neutral     378\n",
      "positive    285\n",
      "Name: label, dtype: int64\n",
      "Label distribution in testDataset label\n",
      "negative    16\n",
      "neutral     42\n",
      "positive    32\n",
      "Name: label, dtype: int64\n",
      "Input data size 810\n",
      "Test data size 90\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>caption</th>\n",
       "      <th>label</th>\n",
       "      <th>image_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I1</td>\n",
       "      <td>two acorns lying ground next oak leaves.</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Acorns 1.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I100</td>\n",
       "      <td>ruined walls church backdrop white clouds blue...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Building 2.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I101</td>\n",
       "      <td>man free fall attached blue bungee jumping app...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Bungee jumping 1.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I102</td>\n",
       "      <td>falling man attached bungee jumping apparatus....</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Bungee jumping 2.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I104</td>\n",
       "      <td>man kneeling front tent two similar-looking gi...</td>\n",
       "      <td>positive</td>\n",
       "      <td>Camping 1.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                            caption     label  \\\n",
       "0    I1           two acorns lying ground next oak leaves.   neutral   \n",
       "2  I100  ruined walls church backdrop white clouds blue...   neutral   \n",
       "3  I101  man free fall attached blue bungee jumping app...   neutral   \n",
       "4  I102  falling man attached bungee jumping apparatus....   neutral   \n",
       "6  I104  man kneeling front tent two similar-looking gi...  positive   \n",
       "\n",
       "             image_name  \n",
       "0          Acorns 1.jpg  \n",
       "2        Building 2.jpg  \n",
       "3  Bungee jumping 1.jpg  \n",
       "4  Bungee jumping 2.jpg  \n",
       "6         Camping 1.jpg  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfImageIdCaptionLabel = pd.read_csv(dataset_path, header=0, sep=\"|\")\n",
    "dfImageIdCaptionLabel.columns = [\"id\",\"caption\", \"label\"]\n",
    "dfImageIdCaptionLabel[\"caption\"] = dfImageIdCaptionLabel[\"caption\"].apply(lambda x: \" \".join(caput.get_words_withoutstopwords(x.lower().split())))\n",
    "#dfImageIdCaptionLabel[\"label\"] = dfImageIdCaptionLabel[\"label\"].apply(lambda x: caput.change_label_str_to_int(x))\n",
    "\n",
    "\n",
    "dfImageIdImageName = dt.get_image_id_to_image_title_as_df(oasis_csv_path)\n",
    "dfImageIdImageName.columns = ['id', 'image_name']\n",
    "dfImageIdImageName['image_name'] = dfImageIdImageName['image_name'].apply(lambda x: x + \".jpg\") \n",
    "printCnt = 5\n",
    "# has [id, caption, label]\n",
    "df = pd.merge(dfImageIdCaptionLabel, dfImageIdImageName, on=\"id\")\n",
    "#print(df.head(printCnt))\n",
    "\n",
    "\n",
    "\n",
    "input_x, test_x, input_y,  test_y = train_test_split(df[\"id\"],\n",
    "                                                     df[\"label\"],\n",
    "                                                     test_size=test_size,\n",
    "                                                     random_state=seed,\n",
    "                                                     stratify=df[\"label\"])\n",
    "\n",
    "inputDataset = pd.concat([input_x, input_y], axis=1)\n",
    "testDataset = pd.concat([test_x, test_y], axis=1)\n",
    "\n",
    "inputDataset = inputDataset.dropna()\n",
    "testDataset = testDataset.dropna()\n",
    "inputDataset = inputDataset.reset_index()\n",
    "testDataset = testDataset.reset_index()\n",
    "\n",
    "# print(\"inputDataset\\n\", inputDataset.head(10))\n",
    "# print(\"testDataset\\n\", testDataset.head(10))\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Label distribution in inputDataset\", inputDataset.groupby('label').label.count())\n",
    "print(\"Label distribution in testDataset\", testDataset.groupby('label').label.count())\n",
    "\n",
    "\n",
    "inputData = df.loc[df['id'].isin(inputDataset.id)]\n",
    "testData = df.loc[df['id'].isin(testDataset.id)]\n",
    "\n",
    "# print(\"inputData\\n\", inputData.head())\n",
    "# print(\"testData\\n\", testData.head())\n",
    "\n",
    "inputIds = set(inputData['id'].values)\n",
    "testIds = set(testData['id'].values)\n",
    "\n",
    "print(\"Input data size\", len(inputIds))\n",
    "print(\"Test data size\", len(testIds))\n",
    "\n",
    "for inputId in inputIds:\n",
    "    if inputId in testIds:\n",
    "        raise inputId + \" inputId exists both in test and input dataset\"\n",
    "        \n",
    "for testId in testIds:\n",
    "    if testId in inputIds:\n",
    "        raise testId + \" testId exists both in test and input dataset\"        \n",
    "\n",
    "inputData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating test and input dataset, and `positive,neutral,negative` under each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete input images dir\n",
    "rmtree(input_images_src, ignore_errors=True)\n",
    "os.makedirs(input_images_src)\n",
    "\n",
    "\n",
    "# Delete test images dir\n",
    "rmtree(test_images_src, ignore_errors=True)\n",
    "os.makedirs(test_images_src)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Copy input images into input dir, and test images into test dir\n",
    "imut.copy_imgs_into(oasis_images_src, inputData['image_name'], input_images_src)\n",
    "imut.copy_imgs_into(oasis_images_src, testData['image_name'], test_images_src)\n",
    "\n",
    "# Divide input images into train and dev set, and each one into {negative, neutral, positive}\n",
    "isForTest = False\n",
    "X_train = inputData['image_name']\n",
    "y_train = inputData['label']\n",
    "dt.create_dataset(\"train\", input_images_src, input_images_classified, X_train, y_train, isForTest)\n",
    "#X_val = inputData['image_name'] # TODO COrrect\n",
    "#y_val = inputData['label']\n",
    "#dt.create_dataset(\"val\", input_images_src, input_images_classified, X_val, y_val, isForTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Fasttext Embeddings\n",
    "\n",
    "You can download fasttext word vectors from here:\n",
    "\n",
    "https://fasttext.cc/docs/en/english-vectors.html\n",
    "\n",
    "https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki-news-300d-1M.vec.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding size : 1193514\n",
      "embedding dimension : (100,)\n"
     ]
    }
   ],
   "source": [
    "def load_embedding(path):\n",
    "    word2vec = {}\n",
    "    with io.open(path, encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            entries = line.rstrip().split(\" \")\n",
    "            word, entries = entries[0], entries[1:]\n",
    "            word2vec[word] = np.array(entries).astype(np.float) # Convert String type to float\n",
    "    print('embedding size : %d' % len(word2vec))\n",
    "    print('embedding dimension : %s' % (word2vec['apple'].shape,))\n",
    "    return word2vec\n",
    "    \n",
    "wordToVec = {}\n",
    "wordToVec = load_embedding(fasttext_embedding_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joint model, which will learn both from images and captions together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'x' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-212-885a043c4f65>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m \u001b[0mfull_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-212-885a043c4f65>\u001b[0m in \u001b[0;36mbuild_model\u001b[0;34m(max_seq_len, num_of_classes, config)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m#x = Dropout(0.5)(branch_2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGlobalMaxPooling2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFC_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#new FC layer, random init\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'x' referenced before assignment"
     ]
    }
   ],
   "source": [
    "def build_model(max_seq_len, num_of_classes, config): \n",
    "    text_inputs = Input(shape=(None, config['embedding_dimension']))\n",
    "    masking = Masking(mask_value=0., input_shape=(None, config['embedding_dimension']))(text_inputs) #input_shape=(None, config['embedding_dimension'])\n",
    "    lstm1 = LSTM(max_seq_len, return_sequences=True, dropout=config['dropout'], recurrent_dropout=config['recurrent_dropout'])(text_inputs)\n",
    "    branch_1 = LSTM(max_seq_len, dropout=config['dropout'], recurrent_dropout=config['recurrent_dropout'])(lstm1)\n",
    "    \n",
    "    #branch_1 = Dropout(0.2)(branch_1)\n",
    "    \n",
    "    # Image input branch - a pre-trained Inception module followed by an added fully connected layer\n",
    "    #base_model = InceptionV3(weights='imagenet', include_top=False)\n",
    "    #base_model = ResNet152(include_top=False, weights='imagenet')\n",
    "    base_model = InceptionResNetV2(weights='imagenet', include_top=False)\n",
    "    # Freeze Inception's weights - we don't want to train these\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # add a fully connected layer after Inception - we do want to train these\n",
    "    branch_2 = base_model.output\n",
    "    \n",
    "#     branch_2 = Dropout(0.5)(branch_2) # NEW ADDED\n",
    "    \n",
    "#     branch_2 = GlobalAveragePooling2D()(branch_2)\n",
    "#     branch_2 = Dense(1024, activation='relu')(branch_2)\n",
    "    \n",
    "    #x = Dropout(0.5)(branch_2)\n",
    "    x = GlobalMaxPooling2D()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(FC_SIZE, activation='relu')(x) #new FC layer, random init\n",
    "    x = Dropout(0.5)(x)\n",
    "    branch_2 = x\n",
    "    \n",
    "    # merge the text input branch and the image input branch and add another fully connected layer\n",
    "    joint = merge([branch_1, branch_2], mode='concat')\n",
    "    joint = Dense(256, activation='relu')(joint)\n",
    "    joint = Dropout(0.5)(joint)\n",
    "    predictions = Dense(num_of_classes, activation='softmax')(joint)\n",
    "    \n",
    "    \n",
    "    if config['useF1Score']:\n",
    "        metrics = ['accuracy', sc.f1, sc.recall, sc.precision]\n",
    "    else:\n",
    "        metrics = ['accuracy']\n",
    "        \n",
    "    full_model = Model(inputs=[base_model.input, text_inputs], outputs=[predictions])\n",
    "\n",
    "    full_model.compile(loss='categorical_crossentropy',\n",
    "                   optimizer='adam', # 'rmsprop'\n",
    "                   metrics=metrics)\n",
    "    print(full_model.summary())\n",
    "    return full_model\n",
    "\n",
    "def setup_to_finetune(model, useF1Score):\n",
    "    \"\"\"Freeze the bottom LAYERS_TO_FREEZE and retrain the remaining top layers.\n",
    "  note: LAYERS_TO_FREEZE corresponds to the top 2 inception blocks in the inceptionv3 arch\n",
    "  Args:\n",
    "    model: keras model\n",
    "    \"\"\"    \n",
    "    totalLayers = len(model.layers)\n",
    "    lastFreezeLayer = totalLayers - LAYERS_TO_UNFREEZE\n",
    "    print(\"LAYERS_TO_UNFREEZE:\", LAYERS_TO_UNFREEZE, \"last layer id to freeze\", lastFreezeLayer, \"total layers, \",totalLayers)\n",
    "    for layer in model.layers[:lastFreezeLayer]:\n",
    "        layer.trainable = False\n",
    "    for layer in model.layers[lastFreezeLayer:]:\n",
    "        layer.trainable = True\n",
    "    #optimizers.SGD(lr=0.0001, momentum=0.9)\n",
    "    model.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy',\\\n",
    "                  metrics=imut.get_metrics(useF1Score))\n",
    "    \n",
    "def get_config():\n",
    "    conf = {}\n",
    "    conf[\"kfold_splits\"] = kfold_splits\n",
    "    conf[\"batch_size\"] = batch_size\n",
    "    conf['embedding_dimension'] = embedding_dimension\n",
    "    conf['recurrent_dropout'] = recurrent_dropout\n",
    "    conf['dropout'] = dropout\n",
    "    conf[\"nb_epochs\"] = nb_epochs\n",
    "    conf['useF1Score'] = useF1Score\n",
    "    conf['verbose'] = verbose\n",
    "    conf['useF1Score'] = useF1Score\n",
    "    conf['seed'] = seed\n",
    "    conf[\"img_height\"] = img_height\n",
    "    conf[\"img_width\"] = img_width\n",
    "    conf[\"kfold_splits\"] = kfold_splits\n",
    "    conf[\"batch_size\"] = batch_size\n",
    "    conf[\"nb_epochs\"] = nb_epochs\n",
    "    conf['useF1Score'] = useF1Score\n",
    "    conf['verbose'] = verbose\n",
    "    return conf \n",
    "\n",
    "full_model = build_model(40, 3, get_config())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Acorns 1.jpg' 'Alcohol 7.jpg']\n",
      "input_images_src dataset/input-joint/\n",
      "\n",
      "\n",
      "dataset/input-joint/Camping 2.jpg\n",
      "dataset/input-joint/Camping 4.jpg\n",
      "dataset/input-joint/Camping 6.jpg\n",
      "dataset/input-joint/Camping 7.jpg\n",
      "dataset/input-joint/Alcohol 8.jpg\n",
      "dataset/input-joint/Camping 10.jpg\n",
      "dataset/input-joint/Candle 1.jpg\n",
      "dataset/input-joint/Car 1.jpg\n",
      "dataset/input-joint/Car 2.jpg\n",
      "dataset/input-joint/Car accident 2.jpg\n",
      "dataset/input-joint/Car accident 3.jpg\n",
      "dataset/input-joint/Car accident 4.jpg\n",
      "dataset/input-joint/Ambulance 1.jpg\n",
      "dataset/input-joint/Car crash 1.jpg\n",
      "dataset/input-joint/Car crash 2.jpg\n",
      "dataset/input-joint/Car crash 3.jpg\n",
      "dataset/input-joint/Car race 1.jpg\n",
      "dataset/input-joint/Car race 2.jpg\n",
      "dataset/input-joint/Car race 3.jpg\n",
      "dataset/input-joint/Car race 4.jpg\n",
      "dataset/input-joint/Cardboard 1.jpg\n",
      "dataset/input-joint/Cardboard 2.jpg\n",
      "dataset/input-joint/Cardboard 3.jpg\n",
      "dataset/input-joint/Ambulance 2.jpg\n",
      "dataset/input-joint/Cat 1.jpg\n",
      "dataset/input-joint/Cat 2.jpg\n",
      "dataset/input-joint/Cat 3.jpg\n",
      "dataset/input-joint/Cat 4.jpg\n",
      "dataset/input-joint/Cat 5.jpg\n",
      "dataset/input-joint/Cat 6.jpg\n",
      "dataset/input-joint/Cat 7.jpg\n",
      "dataset/input-joint/Cat 8.jpg\n",
      "dataset/input-joint/Cat 9.jpg\n",
      "dataset/input-joint/Cat 10.jpg\n",
      "dataset/input-joint/Ambulance 3.jpg\n",
      "dataset/input-joint/Cat 11.jpg\n",
      "dataset/input-joint/Cat 12.jpg\n",
      "dataset/input-joint/Cat 13.jpg\n",
      "dataset/input-joint/Cat 14.jpg\n",
      "dataset/input-joint/Celebration 1.jpg\n",
      "dataset/input-joint/Celebration 2.jpg\n",
      "dataset/input-joint/Cemetery 1.jpg\n",
      "dataset/input-joint/Cemetery 2.jpg\n",
      "dataset/input-joint/Cemetery 3.jpg\n",
      "dataset/input-joint/Cemetery 4.jpg\n",
      "dataset/input-joint/Angry face 1.jpg\n",
      "dataset/input-joint/Cemetery 5.jpg\n",
      "dataset/input-joint/Cheerleader 1.jpg\n",
      "dataset/input-joint/Cheerleader 2.jpg\n",
      "dataset/input-joint/Child labor 2.jpg\n",
      "dataset/input-joint/Child labor 3.jpg\n",
      "dataset/input-joint/Child labor 4.jpg\n",
      "dataset/input-joint/Children 1.jpg\n",
      "dataset/input-joint/Chipmunk 1.jpg\n",
      "dataset/input-joint/Chipmunk 2.jpg\n",
      "dataset/input-joint/Angry face 2.jpg\n",
      "dataset/input-joint/Chipmunk 3.jpg\n",
      "dataset/input-joint/City 1.jpg\n",
      "dataset/input-joint/Clean 1.jpg\n",
      "dataset/input-joint/Cliff diver 1.jpg\n",
      "dataset/input-joint/Cliff diver 2.jpg\n",
      "dataset/input-joint/Cliff diver 3.jpg\n",
      "dataset/input-joint/Cockroach 1.jpg\n",
      "dataset/input-joint/Cockroach 2.jpg\n",
      "dataset/input-joint/Cockroach 3.jpg\n",
      "dataset/input-joint/Cockroach 4.jpg\n",
      "dataset/input-joint/Angry face 3.jpg\n",
      "dataset/input-joint/Cold 1.jpg\n",
      "dataset/input-joint/Cold 2.jpg\n",
      "dataset/input-joint/Cold 3.jpg\n",
      "dataset/input-joint/Cold 4.jpg\n",
      "dataset/input-joint/Cold 5.jpg\n",
      "dataset/input-joint/Cold 6.jpg\n",
      "dataset/input-joint/Cold 8.jpg\n",
      "dataset/input-joint/Collaboration 1.jpg\n",
      "dataset/input-joint/Angry face 4.jpg\n",
      "dataset/input-joint/Cotton swabs 1.jpg\n",
      "dataset/input-joint/Cotton swabs 2.jpg\n",
      "dataset/input-joint/Cotton swabs 3.jpg\n",
      "dataset/input-joint/Couple 1.jpg\n",
      "dataset/input-joint/Couple 2.jpg\n",
      "dataset/input-joint/Couple 3.jpg\n",
      "dataset/input-joint/Couple 5.jpg\n",
      "dataset/input-joint/Couple 6.jpg\n",
      "dataset/input-joint/Couple 7.jpg\n",
      "dataset/input-joint/Angry face 5.jpg\n",
      "dataset/input-joint/Couple 9.jpg\n",
      "dataset/input-joint/Crow 1.jpg\n",
      "dataset/input-joint/Crow 2.jpg\n",
      "dataset/input-joint/Cups 2.jpg\n",
      "dataset/input-joint/Cups 3.jpg\n",
      "dataset/input-joint/Dancing 1.jpg\n",
      "dataset/input-joint/Acorns 2.jpg\n",
      "dataset/input-joint/Angry pose 1.jpg\n",
      "dataset/input-joint/Dancing 2.jpg\n",
      "dataset/input-joint/Dancing 3.jpg\n",
      "dataset/input-joint/Dancing 4.jpg\n",
      "dataset/input-joint/Dancing 5.jpg\n",
      "dataset/input-joint/Dancing 6.jpg\n",
      "dataset/input-joint/Dancing 8.jpg\n",
      "dataset/input-joint/Dancing 9.jpg\n",
      "dataset/input-joint/Dead bodies 1.jpg\n",
      "dataset/input-joint/Dead bodies 2.jpg\n",
      "dataset/input-joint/Angry pose 2.jpg\n",
      "dataset/input-joint/Depressed face 1.jpg\n",
      "dataset/input-joint/Depressed face 2.jpg\n",
      "dataset/input-joint/Depressed pose 1.jpg\n",
      "dataset/input-joint/Depressed pose 2.jpg\n",
      "dataset/input-joint/Depressed pose 4.jpg\n",
      "dataset/input-joint/Desert 1.jpg\n",
      "dataset/input-joint/Dessert 1.jpg\n",
      "dataset/input-joint/Dessert 2.jpg\n",
      "dataset/input-joint/Animal carcass 1.jpg\n",
      "dataset/input-joint/Dessert 4.jpg\n",
      "dataset/input-joint/Dessert 5.jpg\n",
      "dataset/input-joint/Dessert 6.jpg\n",
      "dataset/input-joint/Dessert 7.jpg\n",
      "dataset/input-joint/Dessert 8.jpg\n",
      "dataset/input-joint/Destruction 1.jpg\n",
      "dataset/input-joint/Destruction 4.jpg\n",
      "dataset/input-joint/Destruction 5.jpg\n",
      "dataset/input-joint/Destruction 6.jpg\n",
      "dataset/input-joint/Animal carcass 2.jpg\n",
      "dataset/input-joint/Destruction 7.jpg\n",
      "dataset/input-joint/Destruction 8.jpg\n",
      "dataset/input-joint/Destruction 9.jpg\n",
      "dataset/input-joint/Destruction 10.jpg\n",
      "dataset/input-joint/Dirt 1.jpg\n",
      "dataset/input-joint/Destruction 2.jpg\n",
      "dataset/input-joint/Destruction 3.jpg\n",
      "dataset/input-joint/Dirt 2.jpg\n",
      "dataset/input-joint/Dirt 3.jpg\n",
      "dataset/input-joint/Dirt 4.jpg\n",
      "dataset/input-joint/Animal carcass 3.jpg\n",
      "dataset/input-joint/Dirt 5.jpg\n",
      "dataset/input-joint/Dock 1.jpg\n",
      "dataset/input-joint/Doctor 1.jpg\n",
      "dataset/input-joint/Doctor 2.jpg\n",
      "dataset/input-joint/Doctor 4.jpg\n",
      "dataset/input-joint/Doctor 5.jpg\n",
      "dataset/input-joint/Doctor 6.jpg\n",
      "dataset/input-joint/Doctor 7.jpg\n",
      "dataset/input-joint/Doctor 8.jpg\n",
      "dataset/input-joint/Animal carcass 4.jpg\n",
      "dataset/input-joint/Doctor 9.jpg\n",
      "dataset/input-joint/Dog 1.jpg\n",
      "dataset/input-joint/Dog 2.jpg\n",
      "dataset/input-joint/Dog 4.jpg\n",
      "dataset/input-joint/Dog 5.jpg\n",
      "dataset/input-joint/Dog 7.jpg\n",
      "dataset/input-joint/Dog 8.jpg\n",
      "dataset/input-joint/Dog 9.jpg\n",
      "dataset/input-joint/Animal carcass 5.jpg\n",
      "dataset/input-joint/Dog 10.jpg\n",
      "dataset/input-joint/Dog 12.jpg\n",
      "dataset/input-joint/Dog 13.jpg\n",
      "dataset/input-joint/Dog 14.jpg\n",
      "dataset/input-joint/Dog 15.jpg\n",
      "dataset/input-joint/Dog 16.jpg\n",
      "dataset/input-joint/Dog 17.jpg\n",
      "dataset/input-joint/Dog 18.jpg\n",
      "dataset/input-joint/Animal carcass 6.jpg\n",
      "dataset/input-joint/Dog 21.jpg\n",
      "dataset/input-joint/Dog 22.jpg\n",
      "dataset/input-joint/Dog 23.jpg\n",
      "dataset/input-joint/Dog 24.jpg\n",
      "dataset/input-joint/Dog 25.jpg\n",
      "dataset/input-joint/Dog 26.jpg\n",
      "dataset/input-joint/Dog 27.jpg\n",
      "dataset/input-joint/Dog 28.jpg\n",
      "dataset/input-joint/Dog 29.jpg\n",
      "dataset/input-joint/Archery 1.jpg\n",
      "dataset/input-joint/Dog 30.jpg\n",
      "dataset/input-joint/Dog attack 1.jpg\n",
      "dataset/input-joint/Dog attack 2.jpg\n",
      "dataset/input-joint/Dog attack 3.jpg\n",
      "dataset/input-joint/Drink 1.jpg\n",
      "dataset/input-joint/Drink 2.jpg\n",
      "dataset/input-joint/Dummy 1.jpg\n",
      "dataset/input-joint/Eating 1.jpg\n",
      "dataset/input-joint/Eating 2.jpg\n",
      "dataset/input-joint/Archery 2.jpg\n",
      "dataset/input-joint/Eating 3.jpg\n",
      "dataset/input-joint/Elephant 1.jpg\n",
      "dataset/input-joint/Excited face 1.jpg\n",
      "dataset/input-joint/Excited face 2.jpg\n",
      "dataset/input-joint/Excited face 3.jpg\n",
      "dataset/input-joint/Excited face 4.jpg\n",
      "dataset/input-joint/Excited face 5.jpg\n",
      "dataset/input-joint/Excited face 6.jpg\n",
      "dataset/input-joint/Excited face 7.jpg\n",
      "dataset/input-joint/Exercise 1.jpg\n",
      "dataset/input-joint/Acorns 3.jpg\n",
      "dataset/input-joint/Astronaut 1.jpg\n",
      "dataset/input-joint/Exercise 2.jpg\n",
      "dataset/input-joint/Exercise 3.jpg\n",
      "dataset/input-joint/Explosion 1.jpg\n",
      "dataset/input-joint/Explosion 2.jpg\n",
      "dataset/input-joint/Explosion 3.jpg\n",
      "dataset/input-joint/Explosion 4.jpg\n",
      "dataset/input-joint/Explosion 5.jpg\n",
      "dataset/input-joint/Explosion 6.jpg\n",
      "dataset/input-joint/Father 1.jpg\n",
      "dataset/input-joint/Feces 1.jpg\n",
      "dataset/input-joint/Astronaut 2.jpg\n",
      "dataset/input-joint/Feces 2.jpg\n",
      "dataset/input-joint/Fence 1.jpg\n",
      "dataset/input-joint/Fence 2.jpg\n",
      "dataset/input-joint/Fence 3.jpg\n",
      "dataset/input-joint/Fence 4.jpg\n",
      "dataset/input-joint/Fence 5.jpg\n",
      "dataset/input-joint/Fence 6.jpg\n",
      "dataset/input-joint/Ferret 1.jpg\n",
      "dataset/input-joint/Fire 1.jpg\n",
      "dataset/input-joint/Fire 2.jpg\n",
      "dataset/input-joint/Baby 1.jpg\n",
      "dataset/input-joint/Fire 3.jpg\n",
      "dataset/input-joint/Fire 4.jpg\n",
      "dataset/input-joint/Fire 5.jpg\n",
      "dataset/input-joint/Fire 6.jpg\n",
      "dataset/input-joint/Fire 7.jpg\n",
      "dataset/input-joint/Fire 8.jpg\n",
      "dataset/input-joint/Fire 9.jpg\n",
      "dataset/input-joint/Fire 10.jpg\n",
      "dataset/input-joint/Fire 11.jpg\n",
      "dataset/input-joint/Baby 2.jpg\n",
      "dataset/input-joint/Fire hydrant 2.jpg\n",
      "dataset/input-joint/Fire hydrant 3.jpg\n",
      "dataset/input-joint/Fire hydrant 4.jpg\n",
      "dataset/input-joint/Fireman 1.jpg\n",
      "dataset/input-joint/Fireworks 1.jpg\n",
      "dataset/input-joint/Fireworks 2.jpg\n",
      "dataset/input-joint/Fireworks 3.jpg\n",
      "dataset/input-joint/Fireworks 4.jpg\n",
      "dataset/input-joint/Fireworks 5.jpg\n",
      "dataset/input-joint/Fireworks 6.jpg\n",
      "dataset/input-joint/Baby 3.jpg\n",
      "dataset/input-joint/Fireworks 7.jpg\n",
      "dataset/input-joint/Flood 1.jpg\n",
      "dataset/input-joint/Flood 2.jpg\n",
      "dataset/input-joint/Flood 3.jpg\n",
      "dataset/input-joint/Flowers 1.jpg\n",
      "dataset/input-joint/Flowers 2.jpg\n",
      "dataset/input-joint/Flowers 3.jpg\n",
      "dataset/input-joint/Flowers 4.jpg\n",
      "dataset/input-joint/Flowers 5.jpg\n",
      "dataset/input-joint/Flowers 6.jpg\n",
      "dataset/input-joint/Baby 4.jpg\n",
      "dataset/input-joint/Flowers 8.jpg\n",
      "dataset/input-joint/Flowers 9.jpg\n",
      "dataset/input-joint/Food 2.jpg\n",
      "dataset/input-joint/Food 3.jpg\n",
      "dataset/input-joint/Food 4.jpg\n",
      "dataset/input-joint/Food 6.jpg\n",
      "dataset/input-joint/Baby 5.jpg\n",
      "dataset/input-joint/Football player 1.jpg\n",
      "dataset/input-joint/Frisbee 1.jpg\n",
      "dataset/input-joint/Frustrated pose 3.jpg\n",
      "dataset/input-joint/Frustrated pose 4.jpg\n",
      "dataset/input-joint/Frustrated pose 5.jpg\n",
      "dataset/input-joint/Frustrated pose 6.jpg\n",
      "dataset/input-joint/Frustrated pose 7.jpg\n",
      "dataset/input-joint/Frustrated pose 8.jpg\n",
      "dataset/input-joint/Baby 6.jpg\n",
      "dataset/input-joint/Galaxy 1.jpg\n",
      "dataset/input-joint/Galaxy 2.jpg\n",
      "dataset/input-joint/Galaxy 3.jpg\n",
      "dataset/input-joint/Galaxy 4.jpg\n",
      "dataset/input-joint/Galaxy 5.jpg\n",
      "dataset/input-joint/Galaxy 6.jpg\n",
      "dataset/input-joint/Galaxy 7.jpg\n",
      "dataset/input-joint/Galaxy 8.jpg\n",
      "dataset/input-joint/Garbage dump 1.jpg\n",
      "dataset/input-joint/Baby 7.jpg\n",
      "dataset/input-joint/Garbage dump 2.jpg\n",
      "dataset/input-joint/Garbage dump 3.jpg\n",
      "dataset/input-joint/Garbage dump 4.jpg\n",
      "dataset/input-joint/Garbage dump 5.jpg\n",
      "dataset/input-joint/Garbage dump 6.jpg\n",
      "dataset/input-joint/Garbage dump 7.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/input-joint/Garbage dump 8.jpg\n",
      "dataset/input-joint/Gargoyle 1.jpg\n",
      "dataset/input-joint/Gargoyle 2.jpg\n",
      "dataset/input-joint/Gazing 1.jpg\n",
      "dataset/input-joint/Baby 8.jpg\n",
      "dataset/input-joint/Gazing 2.jpg\n",
      "dataset/input-joint/Gazing 3.jpg\n",
      "dataset/input-joint/Gazing 4.jpg\n",
      "dataset/input-joint/Gazing 5.jpg\n",
      "dataset/input-joint/Gazing 7.jpg\n",
      "dataset/input-joint/Goat 1.jpg\n",
      "dataset/input-joint/Goat 2.jpg\n",
      "dataset/input-joint/Gorrila 1.jpg\n",
      "dataset/input-joint/Grass 1.jpg\n",
      "dataset/input-joint/Alcohol 1.jpg\n",
      "dataset/input-joint/Baby 9.jpg\n",
      "dataset/input-joint/Grass 2.jpg\n",
      "dataset/input-joint/Grass 3.jpg\n",
      "dataset/input-joint/Grass 4.jpg\n",
      "dataset/input-joint/Grass 5.jpg\n",
      "dataset/input-joint/Grass 6.jpg\n",
      "dataset/input-joint/Grass 7.jpg\n",
      "dataset/input-joint/Graveyard 2.jpg\n",
      "dataset/input-joint/Graveyard 3.jpg\n",
      "dataset/input-joint/Graveyard 4.jpg\n",
      "dataset/input-joint/Baby 10.jpg\n",
      "dataset/input-joint/Guitar 1.jpg\n",
      "dataset/input-joint/Gun 1.jpg\n",
      "dataset/input-joint/Gun 2.jpg\n",
      "dataset/input-joint/Gun 3.jpg\n",
      "dataset/input-joint/Gun 4.jpg\n",
      "dataset/input-joint/Gun 5.jpg\n",
      "dataset/input-joint/Gun 6.jpg\n",
      "dataset/input-joint/Gun 7.jpg\n",
      "dataset/input-joint/Gun 8.jpg\n",
      "dataset/input-joint/Band 1.jpg\n",
      "dataset/input-joint/Gun 10.jpg\n",
      "dataset/input-joint/Hallway 1.jpg\n",
      "dataset/input-joint/Hang gliding 1.jpg\n",
      "dataset/input-joint/Hang gliding 2.jpg\n",
      "dataset/input-joint/Hang gliding 3.jpg\n",
      "dataset/input-joint/Happy face 1.jpg\n",
      "dataset/input-joint/Happy face 2.jpg\n",
      "dataset/input-joint/Happy pose 1.jpg\n",
      "dataset/input-joint/Happy pose 2.jpg\n",
      "dataset/input-joint/Band 2.jpg\n",
      "dataset/input-joint/Happy pose 3.jpg\n",
      "dataset/input-joint/Heart 1.jpg\n",
      "dataset/input-joint/Heart 2.jpg\n",
      "dataset/input-joint/Heart 3.jpg\n",
      "dataset/input-joint/Horse 1.jpg\n",
      "dataset/input-joint/Horse racing 1.jpg\n",
      "dataset/input-joint/Injury 1.jpg\n",
      "dataset/input-joint/Injury 2.jpg\n",
      "dataset/input-joint/Injury 3.jpg\n",
      "dataset/input-joint/Bar 1.jpg\n",
      "dataset/input-joint/Injury 4.jpg\n",
      "dataset/input-joint/Intensity 1.jpg\n",
      "dataset/input-joint/Jail 1.jpg\n",
      "dataset/input-joint/Jail 2.jpg\n",
      "dataset/input-joint/Jail 3.jpg\n",
      "dataset/input-joint/Jail 4.jpg\n",
      "dataset/input-joint/Jail 5.jpg\n",
      "dataset/input-joint/Keyboard 1.jpg\n",
      "dataset/input-joint/Keyboard 2.jpg\n",
      "dataset/input-joint/Keyboard 3.jpg\n",
      "dataset/input-joint/Bar 2.jpg\n",
      "dataset/input-joint/Keys 1.jpg\n",
      "dataset/input-joint/KKK rally 1.jpg\n",
      "dataset/input-joint/Knife 1.jpg\n",
      "dataset/input-joint/Knife 2.jpg\n",
      "dataset/input-joint/Lake 1.jpg\n",
      "dataset/input-joint/Lake 2.jpg\n",
      "dataset/input-joint/Lake 3.jpg\n",
      "dataset/input-joint/Lake 4.jpg\n",
      "dataset/input-joint/Lake 5.jpg\n",
      "dataset/input-joint/Bar 3.jpg\n",
      "dataset/input-joint/Lake 6.jpg\n",
      "dataset/input-joint/Lake 7.jpg\n",
      "dataset/input-joint/Lake 8.jpg\n",
      "dataset/input-joint/Lake 9.jpg\n",
      "dataset/input-joint/Lake 10.jpg\n",
      "dataset/input-joint/Lake 11.jpg\n",
      "dataset/input-joint/Lake 12.jpg\n",
      "dataset/input-joint/Lake 13.jpg\n",
      "dataset/input-joint/Lake 14.jpg\n",
      "dataset/input-joint/Lake 15.jpg\n",
      "dataset/input-joint/Barbeque 1.jpg\n",
      "dataset/input-joint/Lake 16.jpg\n",
      "dataset/input-joint/Lake 17.jpg\n",
      "dataset/input-joint/Lamb 1.jpg\n",
      "dataset/input-joint/Lava 1.jpg\n",
      "dataset/input-joint/Lightning 1.jpg\n",
      "dataset/input-joint/Lightning 3.jpg\n",
      "dataset/input-joint/Lightning 4.jpg\n",
      "dataset/input-joint/Lightning 5.jpg\n",
      "dataset/input-joint/Lightning 6.jpg\n",
      "dataset/input-joint/Barbeque 2.jpg\n",
      "dataset/input-joint/Lightning 7.jpg\n",
      "dataset/input-joint/Lion 1.jpg\n",
      "dataset/input-joint/Lion 2.jpg\n",
      "dataset/input-joint/Lion 3.jpg\n",
      "dataset/input-joint/Lion 4.jpg\n",
      "dataset/input-joint/Lion 5.jpg\n",
      "dataset/input-joint/Massage 1.jpg\n",
      "dataset/input-joint/Massage 2.jpg\n",
      "dataset/input-joint/Memorial 1.jpg\n",
      "dataset/input-joint/Bark 1.jpg\n",
      "dataset/input-joint/Memorial 2.jpg\n",
      "dataset/input-joint/Miserable face 1.jpg\n",
      "dataset/input-joint/Miserable face 2.jpg\n",
      "dataset/input-joint/Miserable pose 1.jpg\n",
      "dataset/input-joint/Miserable pose 2.jpg\n",
      "dataset/input-joint/Miserable pose 3.jpg\n",
      "dataset/input-joint/Miserable pose 4.jpg\n",
      "dataset/input-joint/Money 1.jpg\n",
      "dataset/input-joint/Alcohol 2.jpg\n",
      "dataset/input-joint/Bark 2.jpg\n",
      "dataset/input-joint/Monkey 2.jpg\n",
      "dataset/input-joint/Monkey 3.jpg\n",
      "dataset/input-joint/Monkey 4.jpg\n",
      "dataset/input-joint/Moon 1.jpg\n",
      "dataset/input-joint/Mother 1.jpg\n",
      "dataset/input-joint/Mother 2.jpg\n",
      "dataset/input-joint/Mother 4.jpg\n",
      "dataset/input-joint/Mother 5.jpg\n",
      "dataset/input-joint/Bark 3.jpg\n",
      "dataset/input-joint/Mother 7.jpg\n",
      "dataset/input-joint/Mother 8.jpg\n",
      "dataset/input-joint/Mother 9.jpg\n",
      "dataset/input-joint/Motocross 1.jpg\n",
      "dataset/input-joint/Musician 1.jpg\n",
      "dataset/input-joint/Nature 1.jpg\n",
      "dataset/input-joint/Nature 2.jpg\n",
      "dataset/input-joint/Neonazi 1.jpg\n",
      "dataset/input-joint/Neutral face 1.jpg\n",
      "dataset/input-joint/Bark 4.jpg\n",
      "dataset/input-joint/Neutral face 2.jpg\n",
      "dataset/input-joint/Neutral face 3.jpg\n",
      "dataset/input-joint/Neutral face 4.jpg\n",
      "dataset/input-joint/Neutral face 5.jpg\n",
      "dataset/input-joint/Neutral pose 1.jpg\n",
      "dataset/input-joint/Neutral pose 2.jpg\n",
      "dataset/input-joint/Neutral pose 3.jpg\n",
      "dataset/input-joint/Nude couple 1.jpg\n",
      "dataset/input-joint/Nude couple 2.jpg\n",
      "dataset/input-joint/Nude couple 3.jpg\n",
      "dataset/input-joint/Bark 5.jpg\n",
      "dataset/input-joint/Nude couple 4.jpg\n",
      "dataset/input-joint/Nude couple 5.jpg\n",
      "dataset/input-joint/Nude couple 6.jpg\n",
      "dataset/input-joint/Nude couple 7.jpg\n",
      "dataset/input-joint/Nude couple 8.jpg\n",
      "dataset/input-joint/Nude couple 9.jpg\n",
      "dataset/input-joint/Nude couple 10.jpg\n",
      "dataset/input-joint/Nude couple 11.jpg\n",
      "dataset/input-joint/Nude couple 13.jpg\n",
      "dataset/input-joint/Bark 6.jpg\n",
      "dataset/input-joint/Nude couple 14.jpg\n",
      "dataset/input-joint/Nude man 1.jpg\n",
      "dataset/input-joint/Nude man 2.jpg\n",
      "dataset/input-joint/Nude man 4.jpg\n",
      "dataset/input-joint/Nude man 5.jpg\n",
      "dataset/input-joint/Nude man 6.jpg\n",
      "dataset/input-joint/Nude man 7.jpg\n",
      "dataset/input-joint/Nude man 9.jpg\n",
      "dataset/input-joint/Nude man 10.jpg\n",
      "dataset/input-joint/Barrels 1.jpg\n",
      "dataset/input-joint/Nude man 11.jpg\n",
      "dataset/input-joint/Nude man 12.jpg\n",
      "dataset/input-joint/Nude man 13.jpg\n",
      "dataset/input-joint/Nude man 14.jpg\n",
      "dataset/input-joint/Nude man 15.jpg\n",
      "dataset/input-joint/Nude man 16.jpg\n",
      "dataset/input-joint/Nude man 17.jpg\n",
      "dataset/input-joint/Nude man 18.jpg\n",
      "dataset/input-joint/Nude man 19.jpg\n",
      "dataset/input-joint/Nude man 20.jpg\n",
      "dataset/input-joint/BDSM 1.jpg\n",
      "dataset/input-joint/Nude man 3.jpg\n",
      "dataset/input-joint/Nude man 21.jpg\n",
      "dataset/input-joint/Nude man 22.jpg\n",
      "dataset/input-joint/Nude man 23.jpg\n",
      "dataset/input-joint/Nude woman 1.jpg\n",
      "dataset/input-joint/Nude woman 2.jpg\n",
      "dataset/input-joint/Nude woman 3.jpg\n",
      "dataset/input-joint/Nude woman 4.jpg\n",
      "dataset/input-joint/Nude woman 6.jpg\n",
      "dataset/input-joint/BDSM 2.jpg\n",
      "dataset/input-joint/Nude woman 7.jpg\n",
      "dataset/input-joint/Nude woman 8.jpg\n",
      "dataset/input-joint/Nude woman 9.jpg\n",
      "dataset/input-joint/Nude woman 10.jpg\n",
      "dataset/input-joint/Nude woman 11.jpg\n",
      "dataset/input-joint/Nude woman 12.jpg\n",
      "dataset/input-joint/Nude woman 13.jpg\n",
      "dataset/input-joint/Nude woman 15.jpg\n",
      "dataset/input-joint/Nude woman 16.jpg\n",
      "dataset/input-joint/Nude woman 17.jpg\n",
      "dataset/input-joint/Nude woman 18.jpg\n",
      "dataset/input-joint/Nude woman 20.jpg\n",
      "dataset/input-joint/Nude woman 21.jpg\n",
      "dataset/input-joint/Nude woman 22.jpg\n",
      "dataset/input-joint/Office supplies 1.jpg\n",
      "dataset/input-joint/Office supplies 2.jpg\n",
      "dataset/input-joint/Office supplies 3.jpg\n",
      "dataset/input-joint/Office supplies 4.jpg\n",
      "dataset/input-joint/Beach 1.jpg\n",
      "dataset/input-joint/Office supplies 5.jpg\n",
      "dataset/input-joint/Opossum 1.jpg\n",
      "dataset/input-joint/Orangutan 1.jpg\n",
      "dataset/input-joint/Ornament 1.jpg\n",
      "dataset/input-joint/Paintbrush 1.jpg\n",
      "dataset/input-joint/Paper 1.jpg\n",
      "dataset/input-joint/Paper 2.jpg\n",
      "dataset/input-joint/Paper 3.jpg\n",
      "dataset/input-joint/Paper 4.jpg\n",
      "dataset/input-joint/Paper 5.jpg\n",
      "dataset/input-joint/Alcohol 3.jpg\n",
      "dataset/input-joint/Beach 2.jpg\n",
      "dataset/input-joint/Paperclips 1.jpg\n",
      "dataset/input-joint/Paperclips 2.jpg\n",
      "dataset/input-joint/Parachuting 1.jpg\n",
      "dataset/input-joint/Parachuting 2.jpg\n",
      "dataset/input-joint/Parachuting 3.jpg\n",
      "dataset/input-joint/Parachuting 4.jpg\n",
      "dataset/input-joint/Parade 1.jpg\n",
      "dataset/input-joint/Parasailing 1.jpg\n",
      "dataset/input-joint/Beach 3.jpg\n",
      "dataset/input-joint/Parasailing 2.jpg\n",
      "dataset/input-joint/Parasailing 3.jpg\n",
      "dataset/input-joint/Parasailing 4.jpg\n",
      "dataset/input-joint/Party 1.jpg\n",
      "dataset/input-joint/Path 1.jpg\n",
      "dataset/input-joint/Penguins 1.jpg\n",
      "dataset/input-joint/Performance 1.jpg\n",
      "dataset/input-joint/Performance 2.jpg\n",
      "dataset/input-joint/Phone 1.jpg\n",
      "dataset/input-joint/Beach 4.jpg\n",
      "dataset/input-joint/Picnic 2.jpg\n",
      "dataset/input-joint/Picnic 3.jpg\n",
      "dataset/input-joint/Picnic 4.jpg\n",
      "dataset/input-joint/Pig 1.jpg\n",
      "dataset/input-joint/Pig 2.jpg\n",
      "dataset/input-joint/Pigeon 1.jpg\n",
      "dataset/input-joint/Pigeon 3.jpg\n",
      "dataset/input-joint/Pigeon 4.jpg\n",
      "dataset/input-joint/Beach 5.jpg\n",
      "dataset/input-joint/Pigeon 5.jpg\n",
      "dataset/input-joint/Pigeon 6.jpg\n",
      "dataset/input-joint/Pinecone 1.jpg\n",
      "dataset/input-joint/Pinecone 2.jpg\n",
      "dataset/input-joint/Pinecone 3.jpg\n",
      "dataset/input-joint/Pinecone 4.jpg\n",
      "dataset/input-joint/Plane crash 1.jpg\n",
      "dataset/input-joint/Plane crash 2.jpg\n",
      "dataset/input-joint/Plane crash 3.jpg\n",
      "dataset/input-joint/Plane crash 4.jpg\n",
      "dataset/input-joint/Beach 6.jpg\n",
      "dataset/input-joint/Police 1.jpg\n",
      "dataset/input-joint/Police 2.jpg\n",
      "dataset/input-joint/Police 4.jpg\n",
      "dataset/input-joint/Police 5.jpg\n",
      "dataset/input-joint/Pollution 1.jpg\n",
      "dataset/input-joint/Power lines 1.jpg\n",
      "dataset/input-joint/Present 2.jpg\n",
      "dataset/input-joint/Prison 1.jpg\n",
      "dataset/input-joint/Beach 7.jpg\n",
      "dataset/input-joint/Prison 2.jpg\n",
      "dataset/input-joint/Pumpkin 1.jpg\n",
      "dataset/input-joint/Raccoon 1.jpg\n",
      "dataset/input-joint/Rafting 1.jpg\n",
      "dataset/input-joint/Rafting 3.jpg\n",
      "dataset/input-joint/Rafting 4.jpg\n",
      "dataset/input-joint/Rafting 5.jpg\n",
      "dataset/input-joint/Railroad 1.jpg\n",
      "dataset/input-joint/Beach 8.jpg\n",
      "dataset/input-joint/Rainbow 1.jpg\n",
      "dataset/input-joint/Rainbow 2.jpg\n",
      "dataset/input-joint/Research 1.jpg\n",
      "dataset/input-joint/Road 1.jpg\n",
      "dataset/input-joint/Rock climbing 1.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/input-joint/Rock climbing 2.jpg\n",
      "dataset/input-joint/Rock climbing 3.jpg\n",
      "dataset/input-joint/Rock climbing 4.jpg\n",
      "dataset/input-joint/Rocks 1.jpg\n",
      "dataset/input-joint/Rocks 2.jpg\n",
      "dataset/input-joint/Bear 1.jpg\n",
      "dataset/input-joint/Rocks 3.jpg\n",
      "dataset/input-joint/Rocks 6.jpg\n",
      "dataset/input-joint/Rocks 7.jpg\n",
      "dataset/input-joint/Rollercoaster 1.jpg\n",
      "dataset/input-joint/Rollercoaster 2.jpg\n",
      "dataset/input-joint/Rollercoaster 3.jpg\n",
      "dataset/input-joint/Roofing 2.jpg\n",
      "dataset/input-joint/Bear 2.jpg\n",
      "dataset/input-joint/Roofing 3.jpg\n",
      "dataset/input-joint/Roofing 4.jpg\n",
      "dataset/input-joint/Roofing 5.jpg\n",
      "dataset/input-joint/Rooster 1.jpg\n",
      "dataset/input-joint/Rubber duck 1.jpg\n",
      "dataset/input-joint/Rugby 1.jpg\n",
      "dataset/input-joint/Rugby 2.jpg\n",
      "dataset/input-joint/Running away 1.jpg\n",
      "dataset/input-joint/Sad face 1.jpg\n",
      "dataset/input-joint/Sad face 2.jpg\n",
      "dataset/input-joint/Bear 3.jpg\n",
      "dataset/input-joint/Sad face 3.jpg\n",
      "dataset/input-joint/Sad face 4.jpg\n",
      "dataset/input-joint/Sad face 5.jpg\n",
      "dataset/input-joint/Sad face 6.jpg\n",
      "dataset/input-joint/Sad face 7.jpg\n",
      "dataset/input-joint/Sad face 8.jpg\n",
      "dataset/input-joint/Sad face 9.jpg\n",
      "dataset/input-joint/Sad pose 1.jpg\n",
      "dataset/input-joint/Sad pose 2.jpg\n",
      "dataset/input-joint/Alcohol 4.jpg\n",
      "dataset/input-joint/Bed 1.jpg\n",
      "dataset/input-joint/Sad pose 4.jpg\n",
      "dataset/input-joint/Sad pose 6.jpg\n",
      "dataset/input-joint/Sad pose 7.jpg\n",
      "dataset/input-joint/Sailing 1.jpg\n",
      "dataset/input-joint/Sailing 2.jpg\n",
      "dataset/input-joint/Sailing 3.jpg\n",
      "dataset/input-joint/Satellite 1.jpg\n",
      "dataset/input-joint/Scared face 1.jpg\n",
      "dataset/input-joint/Bee 1.jpg\n",
      "dataset/input-joint/Scared face 2.jpg\n",
      "dataset/input-joint/Scared face 3.jpg\n",
      "dataset/input-joint/Scared face 4.jpg\n",
      "dataset/input-joint/Scared face 5.jpg\n",
      "dataset/input-joint/Scary face 1.jpg\n",
      "dataset/input-joint/Scary face 2.jpg\n",
      "dataset/input-joint/School 1.jpg\n",
      "dataset/input-joint/School 2.jpg\n",
      "dataset/input-joint/School 3.jpg\n",
      "dataset/input-joint/School 4.jpg\n",
      "dataset/input-joint/Biking 1.jpg\n",
      "dataset/input-joint/School 6.jpg\n",
      "dataset/input-joint/School 7.jpg\n",
      "dataset/input-joint/School 8.jpg\n",
      "dataset/input-joint/Seal 1.jpg\n",
      "dataset/input-joint/Severed finger 1.jpg\n",
      "dataset/input-joint/Shark 1.jpg\n",
      "dataset/input-joint/Shark 2.jpg\n",
      "dataset/input-joint/Shark 3.jpg\n",
      "dataset/input-joint/Shark 4.jpg\n",
      "dataset/input-joint/Billiards 1.jpg\n",
      "dataset/input-joint/Shark 5.jpg\n",
      "dataset/input-joint/Shark 6.jpg\n",
      "dataset/input-joint/Shark 7.jpg\n",
      "dataset/input-joint/Shark 8.jpg\n",
      "dataset/input-joint/Shark 9.jpg\n",
      "dataset/input-joint/Shark 11.jpg\n",
      "dataset/input-joint/Shooting 1.jpg\n",
      "dataset/input-joint/Shot 1.jpg\n",
      "dataset/input-joint/Shot 2.jpg\n",
      "dataset/input-joint/Bird 1.jpg\n",
      "dataset/input-joint/Shot 3.jpg\n",
      "dataset/input-joint/Siblings 1.jpg\n",
      "dataset/input-joint/Sidewalk 1.jpg\n",
      "dataset/input-joint/Sidewalk 2.jpg\n",
      "dataset/input-joint/Sidewalk 3.jpg\n",
      "dataset/input-joint/Sidewalk 4.jpg\n",
      "dataset/input-joint/Sidewalk 5.jpg\n",
      "dataset/input-joint/Skier 1.jpg\n",
      "dataset/input-joint/Skijump 1.jpg\n",
      "dataset/input-joint/Skijump 2.jpg\n",
      "dataset/input-joint/Skinhead 1.jpg\n",
      "dataset/input-joint/Sky 1.jpg\n",
      "dataset/input-joint/Skydiving 1.jpg\n",
      "dataset/input-joint/Skydiving 2.jpg\n",
      "dataset/input-joint/Skydiving 3.jpg\n",
      "dataset/input-joint/Skydiving 4.jpg\n",
      "dataset/input-joint/Skydiving 5.jpg\n",
      "dataset/input-joint/Skyscraper 1.jpg\n",
      "dataset/input-joint/Skyscraper 2.jpg\n",
      "dataset/input-joint/Sleepy pose 1.jpg\n",
      "dataset/input-joint/Sleepy pose 2.jpg\n",
      "dataset/input-joint/Sleepy pose 3.jpg\n",
      "dataset/input-joint/Sleepy pose 4.jpg\n",
      "dataset/input-joint/Snake 1.jpg\n",
      "dataset/input-joint/Snake 2.jpg\n",
      "dataset/input-joint/Snake 3.jpg\n",
      "dataset/input-joint/Bird 4.jpg\n",
      "dataset/input-joint/Snake 4.jpg\n",
      "dataset/input-joint/Snake 5.jpg\n",
      "dataset/input-joint/Snake 6.jpg\n",
      "dataset/input-joint/Snow 1.jpg\n",
      "dataset/input-joint/Snow 2.jpg\n",
      "dataset/input-joint/Snow 3.jpg\n",
      "dataset/input-joint/Snow 4.jpg\n",
      "dataset/input-joint/Snow 5.jpg\n",
      "dataset/input-joint/Soccer 1.jpg\n",
      "dataset/input-joint/Soccer 2.jpg\n",
      "dataset/input-joint/Socks 1.jpg\n",
      "dataset/input-joint/Solar panel 1.jpg\n",
      "dataset/input-joint/Soldiers 1.jpg\n",
      "dataset/input-joint/Soldiers 3.jpg\n",
      "dataset/input-joint/Soldiers 4.jpg\n",
      "dataset/input-joint/Soldiers 5.jpg\n",
      "dataset/input-joint/Soldiers 6.jpg\n",
      "dataset/input-joint/Soldiers 8.jpg\n",
      "dataset/input-joint/Birthday 1.jpg\n",
      "dataset/input-joint/Soldiers 9.jpg\n",
      "dataset/input-joint/Soldiers 10.jpg\n",
      "dataset/input-joint/Soup 1.jpg\n",
      "dataset/input-joint/Spider 1.jpg\n",
      "dataset/input-joint/Spider 2.jpg\n",
      "dataset/input-joint/Statue 2.jpg\n",
      "dataset/input-joint/Stingray 2.jpg\n",
      "dataset/input-joint/Stingray 3.jpg\n",
      "dataset/input-joint/Alcohol 5.jpg\n",
      "dataset/input-joint/Birthday 2.jpg\n",
      "dataset/input-joint/Storage 1.jpg\n",
      "dataset/input-joint/Storage 3.jpg\n",
      "dataset/input-joint/Street 1.jpg\n",
      "dataset/input-joint/Street 2.jpg\n",
      "dataset/input-joint/Street 3.jpg\n",
      "dataset/input-joint/Street 4.jpg\n",
      "dataset/input-joint/Street 5.jpg\n",
      "dataset/input-joint/Sun 1.jpg\n",
      "dataset/input-joint/Sunflower 1.jpg\n",
      "dataset/input-joint/Birthday 3.jpg\n",
      "dataset/input-joint/Sunset 1.jpg\n",
      "dataset/input-joint/Sunset 2.jpg\n",
      "dataset/input-joint/Sunset 3.jpg\n",
      "dataset/input-joint/Sunset 4.jpg\n",
      "dataset/input-joint/Sunset 5.jpg\n",
      "dataset/input-joint/Sunset 6.jpg\n",
      "dataset/input-joint/Surgery 1.jpg\n",
      "dataset/input-joint/Surgery 2.jpg\n",
      "dataset/input-joint/Surgery 3.jpg\n",
      "dataset/input-joint/Bloody knife 1.jpg\n",
      "dataset/input-joint/Surgery 5.jpg\n",
      "dataset/input-joint/Surprise 1.jpg\n",
      "dataset/input-joint/Surprise 2.jpg\n",
      "dataset/input-joint/Swingset 1.jpg\n",
      "dataset/input-joint/Thunderstorm 1.jpg\n",
      "dataset/input-joint/Thunderstorm 2.jpg\n",
      "dataset/input-joint/Thunderstorm 3.jpg\n",
      "dataset/input-joint/Thunderstorm 4.jpg\n",
      "dataset/input-joint/Bloody knife 2.jpg\n",
      "dataset/input-joint/Thunderstorm 6.jpg\n",
      "dataset/input-joint/Thunderstorm 7.jpg\n",
      "dataset/input-joint/Thunderstorm 8.jpg\n",
      "dataset/input-joint/Thunderstorm 9.jpg\n",
      "dataset/input-joint/Thunderstorm 10.jpg\n",
      "dataset/input-joint/Thunderstorm 11.jpg\n",
      "dataset/input-joint/Tickling 1.jpg\n",
      "dataset/input-joint/Tiger 1.jpg\n",
      "dataset/input-joint/Tiger 2.jpg\n",
      "dataset/input-joint/Timber 1.jpg\n",
      "dataset/input-joint/Boat 1.jpg\n",
      "dataset/input-joint/Timber 2.jpg\n",
      "dataset/input-joint/Timber 3.jpg\n",
      "dataset/input-joint/Timber 4.jpg\n",
      "dataset/input-joint/Toast 1.jpg\n",
      "dataset/input-joint/Toilet 1.jpg\n",
      "dataset/input-joint/Toilet 2.jpg\n",
      "dataset/input-joint/Toilet 3.jpg\n",
      "dataset/input-joint/Toilet 4.jpg\n",
      "dataset/input-joint/Tornado 1.jpg\n",
      "dataset/input-joint/Tornado 2.jpg\n",
      "dataset/input-joint/Bored face 1.jpg\n",
      "dataset/input-joint/Tornado 4.jpg\n",
      "dataset/input-joint/Tornado 5.jpg\n",
      "dataset/input-joint/Torture chamber 1.jpg\n",
      "dataset/input-joint/Traffic 1.jpg\n",
      "dataset/input-joint/Tumor 1.jpg\n",
      "dataset/input-joint/Volcano 1.jpg\n",
      "dataset/input-joint/Volcano 2.jpg\n",
      "dataset/input-joint/Volcano 3.jpg\n",
      "dataset/input-joint/Wall 1.jpg\n",
      "dataset/input-joint/Bored pose 1.jpg\n",
      "dataset/input-joint/Wall 2.jpg\n",
      "dataset/input-joint/Wall 3.jpg\n",
      "dataset/input-joint/Wall 4.jpg\n",
      "dataset/input-joint/Wall 5.jpg\n",
      "dataset/input-joint/War 1.jpg\n",
      "dataset/input-joint/War 2.jpg\n",
      "dataset/input-joint/War 4.jpg\n",
      "dataset/input-joint/War 5.jpg\n",
      "dataset/input-joint/War 6.jpg\n",
      "dataset/input-joint/Bored pose 2.jpg\n",
      "dataset/input-joint/War 7.jpg\n",
      "dataset/input-joint/War 8.jpg\n",
      "dataset/input-joint/Waterfall 1.jpg\n",
      "dataset/input-joint/Weapon 1.jpg\n",
      "dataset/input-joint/Wedding 1.jpg\n",
      "dataset/input-joint/Wedding 2.jpg\n",
      "dataset/input-joint/Wedding 3.jpg\n",
      "dataset/input-joint/Wedding 6.jpg\n",
      "dataset/input-joint/Bored pose 3.jpg\n",
      "dataset/input-joint/Wedding 7.jpg\n",
      "dataset/input-joint/Wedding 8.jpg\n",
      "dataset/input-joint/Wedding 9.jpg\n",
      "dataset/input-joint/Wedding 10.jpg\n",
      "dataset/input-joint/Wedding 11.jpg\n",
      "dataset/input-joint/Wedding ring 1.jpg\n",
      "dataset/input-joint/Windmill 1.jpg\n",
      "dataset/input-joint/Wolf 1.jpg\n",
      "dataset/input-joint/Wolf 2.jpg\n",
      "dataset/input-joint/Bored pose 4.jpg\n",
      "dataset/input-joint/Woods 1.jpg\n",
      "dataset/input-joint/Yarn 1.jpg\n",
      "dataset/input-joint/Yarn 2.jpg\n",
      "dataset/input-joint/Yarn 3.jpg\n",
      "dataset/input-joint/Yarn 4.jpg\n",
      "dataset/input-joint/Yoga 1.jpg\n",
      "dataset/input-joint/Yoga 2.jpg\n",
      "dataset/input-joint/Yoga 3.jpg\n",
      "dataset/input-joint/Yoga 4.jpg\n",
      "dataset/input-joint/Yoga 5.jpg\n",
      "dataset/input-joint/Alcohol 6.jpg\n",
      "dataset/input-joint/Bored pose 5.jpg\n",
      "dataset/input-joint/Zebra 1.jpg\n",
      "dataset/input-joint/Bored pose 6.jpg\n",
      "dataset/input-joint/Bottle 1.jpg\n",
      "dataset/input-joint/Boxing 1.jpg\n",
      "dataset/input-joint/Boxing 2.jpg\n",
      "dataset/input-joint/Bridge 1.jpg\n",
      "dataset/input-joint/Bubble 1.jpg\n",
      "dataset/input-joint/Bubble 2.jpg\n",
      "dataset/input-joint/Building 1.jpg\n"
     ]
    }
   ],
   "source": [
    "#isForTrain = True\n",
    "#trainDir = input_images_classified + \"/\" + \"train\" + \"/\"\n",
    "#train_batches = imut.get_data_generator(trainDir, get_config(), isForTrain)  \n",
    "\n",
    "# print(train_batches)\n",
    "# print(inputData.head())\n",
    "\n",
    "\n",
    "\n",
    "#image_names, image_labels = dt.get_image_name_and_label(oasis_csv_path, neutralLow, neutralHigh)\n",
    "#image_names = np.array(image_names)\n",
    "#image_labels = np.array(image_labels)\n",
    "print(image_names[:2])\n",
    "print(\"input_images_src\", input_images_src)\n",
    "print(\"\\n\")\n",
    "\n",
    "X_train = []\n",
    "cnt = 0\n",
    "# https://github.com/fchollet/deep-learning-models\n",
    "for img_name in inputData['image_name']:\n",
    "    fpath = input_images_src + img_name\n",
    "    cnt += 1\n",
    "    if cnt >5:\n",
    "        print(fpath)\n",
    "    img = load_img(fpath, target_size=(224,224))\n",
    "    x = img_to_array(img)  # this is a Numpy array with shape (3, 150, 150)\n",
    "    #x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    X_train.append(x)\n",
    "    \n",
    "#print(X_train[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = inputData[\"caption\"]\n",
    "y = inputData[\"label\"]\n",
    "max_seq_len = int(inputData['caption'].map(lambda x: caput.get_non_stop_word_count(x.split())).max())\n",
    "\n",
    "\n",
    "dfTrain = pd.concat([X, y], axis=1)\n",
    "dfTrain.columns = ['caption', 'label']\n",
    "\n",
    "class_to_index = {}\n",
    "index_to_class = {}\n",
    "printCnt = 15\n",
    "\n",
    "# print(\"inputData\")\n",
    "# print(inputData.head(printCnt))\n",
    "# print()\n",
    "cnt = 0\n",
    "# print(\"iterrows dfTrain\")\n",
    "# for index, row in dfTrain.iterrows():\n",
    "#     #print(index, row['caption'],row['label'])\n",
    "#     cnt +=1\n",
    "#     if cnt > printCnt:\n",
    "#         break\n",
    "\n",
    "# print()\n",
    "# print(\"dfTrain\")\n",
    "# print(dfTrain.head(printCnt))        \n",
    "# print()\n",
    "X_train_text, y_train_index, num_of_classes, class_to_index, index_to_class = \\\n",
    "            caput.load_dataset_StratifiedKFold(\n",
    "                            dfTrain,\n",
    "                            wordToVec, \n",
    "                            max_seq_len, \n",
    "                            class_to_index, \n",
    "                            index_to_class,\n",
    "                            get_config())\n",
    "y_train = caput.convert_index_to_one_hot(y_train_index, num_of_classes) \n",
    "        \n",
    "# y_ints = [y.argmax() for y in y_train]\n",
    "# class_weights = class_weight.compute_class_weight('balanced',\n",
    "#                                                  np.unique(y_ints),\n",
    "#                                                  y_ints)\n",
    "# print(\"class_weights\", class_weights)\n",
    "# print(\"class_to_index\", class_to_index)\n",
    "# print(X_train_text[:2])\n",
    "# print(type(X_train_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]]\n",
      "[[1. 0. 0.]\n",
      " [0. 0. 1.]]\n",
      "<class 'numpy.ndarray'>\n",
      "batch_size 32\n",
      "class_weights [0.71428571 0.94736842 1.83673469]\n",
      "Train on 729 samples, validate on 81 samples\n",
      "Epoch 1/15\n",
      "729/729 [==============================] - 4s 6ms/step - loss: 0.8636 - acc: 0.5034 - val_loss: 0.8661 - val_acc: 0.6543\n",
      "Epoch 2/15\n",
      "729/729 [==============================] - 4s 6ms/step - loss: 0.9109 - acc: 0.5213 - val_loss: 0.7624 - val_acc: 0.5802\n",
      "Epoch 3/15\n",
      "729/729 [==============================] - 4s 6ms/step - loss: 0.8792 - acc: 0.5226 - val_loss: 0.8302 - val_acc: 0.5309\n",
      "Epoch 4/15\n",
      "729/729 [==============================] - 5s 6ms/step - loss: 0.8647 - acc: 0.4993 - val_loss: 0.7981 - val_acc: 0.6296\n",
      "Epoch 5/15\n",
      "729/729 [==============================] - 5s 6ms/step - loss: 0.8856 - acc: 0.5240 - val_loss: 0.8071 - val_acc: 0.5062\n",
      "Epoch 6/15\n",
      "729/729 [==============================] - 5s 6ms/step - loss: 0.8915 - acc: 0.5240 - val_loss: 0.8150 - val_acc: 0.4938\n",
      "Epoch 7/15\n",
      "729/729 [==============================] - 5s 6ms/step - loss: 0.8548 - acc: 0.4979 - val_loss: 0.8928 - val_acc: 0.6173\n",
      "Epoch 8/15\n",
      "729/729 [==============================] - 5s 6ms/step - loss: 0.8785 - acc: 0.5185 - val_loss: 0.7992 - val_acc: 0.5062\n",
      "Epoch 9/15\n",
      "729/729 [==============================] - 5s 6ms/step - loss: 0.8623 - acc: 0.5199 - val_loss: 0.9647 - val_acc: 0.5802\n",
      "Epoch 10/15\n",
      "729/729 [==============================] - 5s 6ms/step - loss: 0.8694 - acc: 0.5418 - val_loss: 0.7777 - val_acc: 0.5556\n",
      "Epoch 11/15\n",
      "729/729 [==============================] - 5s 6ms/step - loss: 0.8031 - acc: 0.5857 - val_loss: 0.8860 - val_acc: 0.5556\n",
      "Epoch 12/15\n",
      "729/729 [==============================] - 5s 6ms/step - loss: 0.8249 - acc: 0.5405 - val_loss: 0.8870 - val_acc: 0.5432\n",
      "Epoch 13/15\n",
      "729/729 [==============================] - 5s 6ms/step - loss: 0.8574 - acc: 0.5364 - val_loss: 0.9783 - val_acc: 0.5185\n",
      "Epoch 14/15\n",
      "729/729 [==============================] - 5s 6ms/step - loss: 0.8559 - acc: 0.5569 - val_loss: 0.9446 - val_acc: 0.5185\n",
      "Epoch 15/15\n",
      "729/729 [==============================] - 5s 6ms/step - loss: 0.8341 - acc: 0.5734 - val_loss: 0.9915 - val_acc: 0.5185\n",
      "LAYERS_TO_UNFREEZE: 20 last layer id to freeze 774 total layers,  794\n",
      "Train on 729 samples, validate on 81 samples\n",
      "Epoch 1/100\n",
      "729/729 [==============================] - 93s 128ms/step - loss: 0.9221 - acc: 0.5583 - val_loss: 1.4450 - val_acc: 0.5309\n",
      "Epoch 2/100\n",
      "729/729 [==============================] - 5s 6ms/step - loss: 0.9052 - acc: 0.5679 - val_loss: 1.2226 - val_acc: 0.5185\n",
      "Epoch 3/100\n",
      "729/729 [==============================] - 5s 6ms/step - loss: 0.8747 - acc: 0.5857 - val_loss: 1.3276 - val_acc: 0.5556\n",
      "Epoch 4/100\n",
      "729/729 [==============================] - 5s 6ms/step - loss: 0.8849 - acc: 0.5542 - val_loss: 0.9801 - val_acc: 0.5309\n",
      "Epoch 5/100\n",
      "729/729 [==============================] - 5s 7ms/step - loss: 0.8293 - acc: 0.5789 - val_loss: 1.1305 - val_acc: 0.5185\n",
      "Epoch 6/100\n",
      "729/729 [==============================] - 5s 6ms/step - loss: 0.8848 - acc: 0.5487 - val_loss: 1.5414 - val_acc: 0.5062\n",
      "Epoch 7/100\n",
      "729/729 [==============================] - 5s 7ms/step - loss: 0.8104 - acc: 0.5446 - val_loss: 1.5291 - val_acc: 0.5679\n",
      "Epoch 8/100\n",
      "729/729 [==============================] - 5s 7ms/step - loss: 0.8271 - acc: 0.5679 - val_loss: 1.3868 - val_acc: 0.4815\n",
      "Epoch 9/100\n",
      "729/729 [==============================] - 5s 6ms/step - loss: 0.7952 - acc: 0.5871 - val_loss: 1.3070 - val_acc: 0.5185\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0007000000332482159.\n",
      "Epoch 10/100\n",
      "729/729 [==============================] - 5s 6ms/step - loss: 0.7991 - acc: 0.5885 - val_loss: 1.1889 - val_acc: 0.5185\n",
      "Epoch 11/100\n",
      "729/729 [==============================] - 5s 7ms/step - loss: 0.8108 - acc: 0.5830 - val_loss: 1.2730 - val_acc: 0.5185\n",
      "Epoch 12/100\n",
      "729/729 [==============================] - 5s 7ms/step - loss: 0.8107 - acc: 0.5514 - val_loss: 1.3097 - val_acc: 0.5802\n",
      "Epoch 13/100\n",
      "729/729 [==============================] - 5s 7ms/step - loss: 0.7888 - acc: 0.5583 - val_loss: 1.1893 - val_acc: 0.4938\n",
      "Epoch 14/100\n",
      "729/729 [==============================] - 5s 7ms/step - loss: 0.7900 - acc: 0.5597 - val_loss: 1.1609 - val_acc: 0.5062\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0004900000232737511.\n",
      "Epoch 15/100\n",
      "729/729 [==============================] - 5s 6ms/step - loss: 0.7654 - acc: 0.5665 - val_loss: 1.2398 - val_acc: 0.5185\n",
      "Epoch 16/100\n",
      "729/729 [==============================] - 5s 6ms/step - loss: 0.7533 - acc: 0.5857 - val_loss: 1.2573 - val_acc: 0.4815\n",
      "Epoch 17/100\n",
      "729/729 [==============================] - 5s 6ms/step - loss: 0.7439 - acc: 0.5967 - val_loss: 1.2537 - val_acc: 0.4568\n",
      "Epoch 18/100\n",
      "480/729 [==================>...........] - ETA: 1s - loss: 0.7597 - acc: 0.5875"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-214-12588ceb9f68>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m                                    \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                                    \u001b[0;31m#class_weight=class_weights,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                                    verbose=1) \n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0museF1Score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cs231n/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/cs231n/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1234\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1236\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1237\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cs231n/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2480\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2481\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2482\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2483\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cs231n/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cs231n/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1140\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cs231n/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1321\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cs231n/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cs231n/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1312\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cs231n/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[1;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m             status, run_metadata)\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "print(y_train[:3])\n",
    "y_train = np.array(y_train)\n",
    "print(y_train[:2])\n",
    "print(type(y_train))\n",
    "\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', \n",
    "                                  factor=0.7, \n",
    "                                  patience=5,\n",
    "                                  min_delta=0.0001,\n",
    "                                  cooldown=1,\n",
    "                                  min_lr=1e-7,\n",
    "                                  verbose=verbose)\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=patience)    \n",
    "callbacks_list = [early_stopping, reduce_lr] \n",
    "\n",
    "printCnt = 10\n",
    "# print(\"Before shuffle\")\n",
    "# print(y_train[-printCnt:])\n",
    "X_train, X_train_text, y_train= np.array(X_train), np.array(X_train_text), np.array(y_train)\n",
    "X_train, X_train_text, y_train = shuffle(X_train, X_train_text, y_train)\n",
    "# print(\"After shuffle\")\n",
    "# print(y_train[-printCnt:])\n",
    "batch_size = 32 \n",
    "print(\"batch_size\", batch_size)\n",
    "\n",
    "print(\"class_weights\", class_weights)\n",
    "history = full_model.fit([X_train, X_train_text], y_train,\n",
    "                                   batch_size=batch_size,\n",
    "                                   epochs=15,\n",
    "                                   #shuffle=True,\n",
    "                                   validation_split=0.1,\n",
    "                                   #callbacks=callbacks_list,\n",
    "                                   #class_weight=class_weights,\n",
    "                                   verbose=1) \n",
    "\n",
    "LAYERS_TO_UNFREEZE = 20\n",
    "setup_to_finetune(full_model, False)\n",
    "\n",
    "history = full_model.fit([X_train, X_train_text], y_train,\n",
    "                                   batch_size=batch_size,\n",
    "                                   epochs=100,\n",
    "                                   shuffle=True,\n",
    "                                   validation_split=0.1,\n",
    "                                   callbacks=callbacks_list,\n",
    "                                   #class_weight=class_weights,\n",
    "                                   verbose=1) \n",
    "\n",
    "useF1Score = False\n",
    "pt.plot_model_accuracy(history, model_results_root_dir, useF1Score)\n",
    "#,  validation_split=0.2,epochs=epochs,\n",
    "#  steps_per_epoch=  2000 // get_config()['batch_size'],"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
