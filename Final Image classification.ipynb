{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image classification\n",
    "\n",
    "## Plan\n",
    "- seed = 7 \n",
    "- All data exists in `dataset/images` directory.\n",
    "- Separate 10% for test dataset.\n",
    "    - Store test dataset under `dataset/test`\n",
    "    - Store `inputDataset = (dataset - test)` under `dataset/input`\n",
    "- Use Stratified 10-fold cross validation to divide `inputDataset` dataset into:\n",
    "    - `train` datasets \n",
    "    - `val` datasets\n",
    "- Train\n",
    "    - Shuffle `train` dataset\n",
    "    - Use `train` dataset to train your model, with validation_split=0.1\n",
    "    - Evalaute in each epoch on `val` dataset\n",
    "\n",
    "```\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_input, x_test, y_input, y_test = train_test_split(xs, ys,\n",
    "                                                    test_size=0.1,\n",
    "                                                    random_state=0,\n",
    "                                                    stratify=ys)\n",
    "\n",
    "shuffle(x_input, y_input)\n",
    "\n",
    "# X_train = \n",
    "# Y_train = \n",
    "\n",
    "validation_split = 0.1\n",
    "\n",
    "for train_set, validation_set in rkf.split(x_input, y_input):\n",
    "    X_train, X_val = x_input[train_set], x_input[validation_set]\n",
    "    Y_train, Y_val = y_input[train_set], y_input[validation_set]\n",
    "\n",
    "    model.fit(X_train, Y_train, epochs=n_epochs, verbose=0,\n",
    "                              validation_split=validation_split)\n",
    "                              \n",
    "    model.evaluate(X_val, Y_val)\n",
    "    \n",
    "# Now run once after long tuning\n",
    "model.evaluate(x_test, y_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, re, string, pathlib, random, io, time, glob\n",
    "from collections import Counter, OrderedDict\n",
    "from shutil import copyfile, rmtree\n",
    "\n",
    "#import hecutils.resnet152 as resnet\n",
    "from hecutils.resnet152 import ResNet152\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import hecutils.data_utils as dt\n",
    "import hecutils.scoring_utils as sc\n",
    "import hecutils.plotting_utils as pt\n",
    "import hecutils.image_utils as imut\n",
    "\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, Callback\n",
    "from keras.layers import GRU, Bidirectional, LSTM, MaxPooling1D, Conv1D,Dense, GlobalAveragePooling2D, GlobalMaxPooling2D\n",
    "from keras.layers.core import Dropout, Flatten, Masking, ActivityRegularization\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from keras.utils import plot_model\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from keras.preprocessing import text\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from keras.models import Model\n",
    "from keras import optimizers\n",
    "\n",
    "# from fastText import load_model\n",
    "from gensim.models import word2vec\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Keras to use Tensorflow GPU in the backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "\n",
    "num_cores = 8\n",
    "GPU=True\n",
    "CPU = not GPU\n",
    "\n",
    "if GPU:\n",
    "    num_GPU = 1\n",
    "    num_CPU = 8\n",
    "if CPU:\n",
    "    num_CPU = 8\n",
    "    num_GPU = 0\n",
    "\n",
    "config = tf.ConfigProto(intra_op_parallelism_threads=num_cores,\\\n",
    "        inter_op_parallelism_threads=num_cores, allow_soft_placement=True,\\\n",
    "        device_count = {'CPU' : num_CPU, 'GPU' : num_GPU})\n",
    "session = tf.Session(config=config)\n",
    "K.set_session(session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oasis_csv_path = \"dataset/metadata/OASIS.csv\"\n",
    "oasis_images_src = \"dataset/images/\"\n",
    "input_images_src = \"dataset/input/\"\n",
    "test_images_src = \"dataset/test/\"\n",
    "model_results_root_dir = \"img_model/\"\n",
    "\n",
    "input_images_classified = \"dataset/input-classified/\"\n",
    "test_images_classified = \"dataset/test-classified/\"\n",
    "\n",
    "# ou can downlaod weights here: https://gist.github.com/flyyufelix/7e2eafb149f72f4d38dd661882c554a6\n",
    "weights_path = \"/home/elkhand/weights/resnet152_weights_tf.h5\"\n",
    "\n",
    "dataset_groups=[\"train\", \"val\"]\n",
    "classes = [\"negative\", \"neutral\", \"positive\"]\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "\n",
    "test_size = 0.1\n",
    "\n",
    "kfold_splits =  5\n",
    "\n",
    "neutralLow = 3.0\n",
    "neutralHigh = 5.0\n",
    "\n",
    "nb_epochs = 100\n",
    "patience = 10 # ReduceLROnPlateau has 5\n",
    "batch_size = 32 # 32  \n",
    "\n",
    "FC_SIZE = 128 # 1024\n",
    "LAYERS_TO_UNFREEZE = 10\n",
    "\n",
    "img_height = 224 # 299\n",
    "img_width = 224  # 299\n",
    "\n",
    "useF1Score = False\n",
    "verbose=1 \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating test and input dataset, and `positive,neutral,negative` under each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_names, image_labels = dt.get_image_name_and_label(oasis_csv_path, neutralLow, neutralHigh)\n",
    "\n",
    "imageNameToLabel = {}\n",
    "\n",
    "for img_name,label in zip(image_names, image_labels):\n",
    "    if img_name not in imageNameToLabel:\n",
    "        imageNameToLabel[img_name] = label\n",
    "    else:\n",
    "        raise \"There should not be images with same name: \" + img_name + label    \n",
    "        \n",
    "image_names = np.array(image_names)\n",
    "image_labels = np.array(image_labels)\n",
    "\n",
    "input_x, test_x, input_y,  test_y = train_test_split(image_names, image_labels,\n",
    "                                                    test_size=test_size,\n",
    "                                                    random_state=seed,\n",
    "                                                    stratify=image_labels)\n",
    "\n",
    "print(\"Input data (to be used for model learning and validation) label distribution: \\n\",pt.get_label_count(input_y))\n",
    "print()\n",
    "print(\"Test data(never used for learning) label distribution: \\n\",pt.get_label_count(test_y))\n",
    "\n",
    "\n",
    "# Delete input images dir\n",
    "rmtree(input_images_src, ignore_errors=True)\n",
    "os.makedirs(input_images_src)\n",
    "\n",
    "\n",
    "# Delete test images dir\n",
    "rmtree(test_images_src, ignore_errors=True)\n",
    "os.makedirs(test_images_src)\n",
    "\n",
    "\n",
    "# Copy input images into input dir, and test images into test dir\n",
    "imut.copy_imgs_into(oasis_images_src, input_x, input_images_src)\n",
    "imut.copy_imgs_into(oasis_images_src, test_x, test_images_src)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Instantiate the cross validator\n",
    "skf = StratifiedKFold(n_splits=kfold_splits, shuffle=True)\n",
    "cv_accuracies = []\n",
    "cv_f1s = []\n",
    "    \n",
    "X = input_x\n",
    "y = input_y\n",
    "    \n",
    "# Shuffe input data\n",
    "X, y = shuffle(X,y)\n",
    "\n",
    "def setup_to_finetune(model, useF1Score):\n",
    "    \"\"\"Freeze the bottom LAYERS_TO_FREEZE and retrain the remaining top layers.\n",
    "  note: LAYERS_TO_FREEZE corresponds to the top 2 inception blocks in the inceptionv3 arch\n",
    "  Args:\n",
    "    model: keras model\n",
    "    \"\"\"    \n",
    "    totalLayers = len(model.layers)\n",
    "    lastFreezeLayer = totalLayers - LAYERS_TO_UNFREEZE\n",
    "    print(\"LAYERS_TO_UNFREEZE:\", LAYERS_TO_UNFREEZE, \"last layer id to freeze\", lastFreezeLayer, \"total layers, \",totalLayers)\n",
    "    for layer in model.layers[:lastFreezeLayer]:\n",
    "        layer.trainable = False\n",
    "    for layer in model.layers[lastFreezeLayer:]:\n",
    "        layer.trainable = True\n",
    "    #optimizers.SGD(lr=0.0001, momentum=0.9)\n",
    "    model.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy',\\\n",
    "                  metrics=imut.get_metrics(useF1Score))\n",
    "\n",
    "def setup_to_transfer_learn(model, base_model, useF1Score):\n",
    "    \"\"\"Freeze all layers and compile the model\"\"\"    \n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "       \n",
    "    model.compile(optimizer='rmsprop', loss='categorical_crossentropy', \\\n",
    "                  metrics=imut.get_metrics(useF1Score))\n",
    "    print()\n",
    "\n",
    "def add_new_last_layer(base_model, nb_classes):\n",
    "    \"\"\"Add last layer to the convnet\n",
    "      Args:\n",
    "        base_model: keras model excluding top\n",
    "        nb_classes: # of classes\n",
    "      Returns:\n",
    "        new keras model with last layer\n",
    "    \"\"\"\n",
    "    # 62,65,67% w/o reducing lr\n",
    "    x = base_model.output\n",
    "    x = Dropout(0.5)(x)\n",
    "    # convert MxNxC into 1xC\n",
    "    #x = GlobalAveragePooling2D()(x)\n",
    "    x = GlobalMaxPooling2D()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(FC_SIZE, activation='relu')(x) #new FC layer, random init\n",
    "    x = Dropout(0.5)(x)\n",
    "    output_layer = Dense(nb_classes, activation='softmax')(x) #new softmax layer\n",
    "    model = Model(input=base_model.input, output=output_layer)\n",
    "    #print(model.summary())\n",
    "    \n",
    "#     x = base_model.output\n",
    "#     x = Flatten()(x)\n",
    "#     x = Dropout(0.5)(x)\n",
    "#     output_layer = Dense(nb_classes, activation='softmax', name='softmax')(x)\n",
    "#     model = Model(inputs=base_model.input, outputs=output_layer)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train(trainDir, valDir, config): \n",
    "    \"\"\"Use transfer learning and fine-tuning to train a network on a new dataset\"\"\"     \n",
    "    \n",
    "    nb_classes = len(glob.glob(trainDir + \"/*\"))\n",
    "    \n",
    "    # this is a generator that will read pictures found in\n",
    "    # subfolers of 'data/train', and indefinitely generate\n",
    "    # batches of augmented image data\n",
    "    isForTrain = True\n",
    "    train_batches = imut.get_data_generator(trainDir, config, isForTrain)  \n",
    "    \n",
    "\n",
    "    # this is a similar generator, for validation data\n",
    "    isForTrain = False\n",
    "    validation_batches = imut.get_data_generator(valDir, config, isForTrain)  \n",
    "        \n",
    "    # setup model\n",
    "    base_model = ResNet152(include_top=False, weights='imagenet')\n",
    "    model = add_new_last_layer(base_model, nb_classes)\n",
    "\n",
    "    # transfer learning\n",
    "    setup_to_transfer_learn(model, base_model, config['useF1Score'])\n",
    "    \n",
    "    # monitor='val_loss', patience = 5 default\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', \n",
    "                                  factor=0.7, \n",
    "                                  patience=2,\n",
    "                                  min_delta=0.0001,\n",
    "                                  cooldown=1,\n",
    "                                  min_lr=10e-7,\n",
    "                                  verbose=verbose)\n",
    "    # monitor='val_loss', factor=0.1, patience=10, verbose=0, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0\n",
    "    \n",
    "    filepath = model_results_root_dir + \"image.weights.best.h5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=0, save_best_only=True, mode='max')\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=patience)    \n",
    "    callbacks_list = [ early_stopping, reduce_lr]  # , checkpoint\n",
    "\n",
    "    # train the model on the new data for a few epochs\n",
    "    history = model.fit_generator(\n",
    "            train_batches,\n",
    "            steps_per_epoch= 2000 // config['batch_size'], # train_batches.samples\n",
    "            epochs=3,\n",
    "            validation_data=validation_batches,\n",
    "            validation_steps= validation_batches.samples // config['batch_size'], \n",
    "            shuffle=True,\n",
    "            verbose=config['verbose'],\n",
    "            #callbacks=callbacks_list,\n",
    "            class_weight='auto')\n",
    "    \n",
    "    # fine-tuning\n",
    "    print(\"Starting fune-tuning\")\n",
    "    setup_to_finetune(model, config['useF1Score'])\n",
    "    \n",
    "    # Add checkpointing to save best model\n",
    "    #callbacks_list.append(checkpoint)\n",
    "    history = model.fit_generator(\n",
    "            train_batches,\n",
    "            steps_per_epoch= 2000 // config['batch_size'], # train_batches.samples\n",
    "            epochs=config['nb_epochs'],\n",
    "            validation_data=validation_batches,\n",
    "            validation_steps= validation_batches.samples // config['batch_size'],\n",
    "            shuffle=True,\n",
    "            verbose=config['verbose'],\n",
    "            callbacks=callbacks_list,\n",
    "            class_weight='auto')\n",
    "    return history, model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def evalaute_on_test_data(model, config):\n",
    "    testDir = test_images_src\n",
    "    print(\"testDir\", testDir)\n",
    "    isForTrain = False\n",
    "    config['batch_size'] = 1\n",
    "    \n",
    "    # Divide input images into train and dev set, and each one into {negative, neutral, positive}\n",
    "    isForTest = True\n",
    "    dt.create_dataset(\"test\", testDir, test_images_classified, test_x, test_y, isForTest)\n",
    "    test_batches = imut.get_data_generator_for_test(test_images_classified, config) \n",
    "\n",
    "    results = model.evaluate_generator(test_batches, steps=test_batches.samples // config['batch_size'], verbose=1)\n",
    "    print(model.metrics_names, results)\n",
    "    \n",
    "    predictions = model.predict_generator(test_batches, steps=test_batches.samples // config['batch_size'], verbose=1)\n",
    "    \n",
    "    label_map_from_train_gen = (get_label_map_from_train_generator(config))\n",
    "    print(\"============= label_map_from_train_gen \", label_map_from_train_gen,\" =================\")\n",
    "    predictions = conver_predictions_to_classes(predictions, label_map_from_train_gen)\n",
    "    \n",
    "    test_filenames = test_batches.filenames\n",
    "    y_true = get_truth_labels_test_data(test_filenames)\n",
    "    y_pred = predictions\n",
    "\n",
    "    cnf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    print(\"cnf_matrix\", cnf_matrix)\n",
    "    titleOfConfusionMatrix = \"Confusion Matrix based on InceptionResNetV2\"\n",
    "    pt.plot_confusion_matrix_from_labels(y_true, y_pred, titleOfConfusionMatrix)\n",
    "    \n",
    "\n",
    "def prepare_and_train(config):\n",
    "    bestModel = None\n",
    "    final_model_val_acc = -1\n",
    "    for index, (train_indices, val_indices) in enumerate(skf.split(X, y)):\n",
    "        print(\"Train size: \", len(train_indices))\n",
    "        print(\"Val size: \", len(val_indices))\n",
    "\n",
    "        X_train, X_val = X[train_indices], X[val_indices]\n",
    "        y_train, y_val = y[train_indices], y[val_indices]\n",
    "\n",
    "        print(\"Train label distribution: \", pt.get_label_count(y_train))\n",
    "        print(\"Val label distribution: \", pt.get_label_count(y_val))\n",
    "\n",
    "        # Divide input images into train and dev set, and each one into {negative, neutral, positive}\n",
    "        isForTest = False\n",
    "        dt.create_dataset(\"train\", input_images_src, input_images_classified, X_train, y_train, isForTest)\n",
    "        dt.create_dataset(\"val\", input_images_src, input_images_classified, X_val, y_val, isForTest)\n",
    "\n",
    "        trainDir = input_images_classified + \"/\" + \"train\" + \"/\"\n",
    "        valDir = input_images_classified + \"/\" + \"val\" + \"/\"\n",
    "\n",
    "        history, model = train(trainDir, valDir, config)\n",
    "        \n",
    "        pt.plot_model_accuracy(history, model_results_root_dir, useF1Score)\n",
    "        best_val_acc = max(history.history['val_acc'])\n",
    "        \n",
    "        if best_val_acc > final_model_val_acc:\n",
    "            bestModel = model\n",
    "            final_model_val_acc = best_val_acc\n",
    "        \n",
    "        cv_accuracies.append(best_val_acc)\n",
    "        print(\"best_val_acc\", best_val_acc)\n",
    "        if useF1Score:\n",
    "            best_val_f1 = max(history.history['val_f1'])\n",
    "            cv_f1s.append(best_val_f1)\n",
    "\n",
    "    print(\"=========================================\")\n",
    "    print(\"Cross-validation val accuracy results: \" , cv_accuracies)\n",
    "    print(\"Cross-validation val accuracy results: %.2f%% (+/- %.2f%%)\" % (np.mean(cv_accuracies), np.std(cv_accuracies)))\n",
    "\n",
    "    if useF1Score:\n",
    "        print(\"\\n\",\"Cross-validation val f1 results: \" , cv_f1s)\n",
    "        print(\"Cross-validation val f1 results: %.2f%% (+/- %.2f%%)\" % (np.mean(cv_f1s), np.std(cv_f1s)))   \n",
    "    \n",
    "    # Evaluate Test data set\n",
    "    evalaute_on_test_data(model, config)\n",
    "    \n",
    "    bestModel.save(model_results_root_dir + \"/bestmodel-\" + str(final_model_val_acc) + \".h5\")\n",
    "\n",
    "\n",
    "def get_config():\n",
    "    conf = {}\n",
    "    conf[\"img_height\"] = img_height\n",
    "    conf[\"img_width\"] = img_width\n",
    "    conf[\"kfold_splits\"] = kfold_splits\n",
    "    conf[\"batch_size\"] = batch_size\n",
    "    conf[\"nb_epochs\"] = nb_epochs\n",
    "    conf['useF1Score'] = useF1Score\n",
    "    conf['verbose'] = verbose\n",
    "    return conf    \n",
    "    \n",
    "def main():\n",
    "    config = get_config()\n",
    "    print(\"config:\\n\", config)\n",
    "    prepare_and_train(config)\n",
    "    # predict()\n",
    "    # text_pre_processing(\"hello&nbsp;hi\")\n",
    "    pass\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # execute only if run as a script\n",
    "    main()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "\n",
    "### Model Architecture\n",
    "Added ResNet152 -> GlobalAveragePooling2D() -> Dense()\n",
    "\n",
    "Train only the last dense layer for 5 epochs, thn unfreeze few top N layers, and train 100 epochs\n",
    "\n",
    "Experiments:\n",
    "\n",
    "- Freeze all layers, except the last dense layer.\n",
    "    - Overfitting, train > 0.9, val 0.55 - 0.75\n",
    "\n",
    "```\n",
    "Cross-validation val accuracy results:  [0.671875, 0.75, 0.609375, 0.640625, 0.71875, 0.65625, 0.640625, 0.78125, 0.703125, 0.6875]\n",
    "Cross-validation val accuracy results: 0.69% (+/- 0.05%)\n",
    "testDir dataset/test/\n",
    "Found 90 images belonging to 3 classes.\n",
    "90/90 [==============================] - 4s 47ms/step\n",
    "['loss', 'acc'] [1.0616117291642115, 0.6555555555555556]\n",
    "90/90 [==============================] - 20s 219ms/step\n",
    "Found 731 images belonging to 3 classes.\n",
    "cnf_matrix [[ 6  4  6]\n",
    " [ 6 24 12]\n",
    " [ 1  2 29]]\n",
    "```\n",
    "\n",
    "- Unfreeze all layers\n",
    "    - cannot train, does not fit into memory\n",
    "\n",
    "\n",
    "- Unfreeze top N layers\n",
    "    - N = 10 \n",
    "    - N = 30, , \n",
    "    - N = 50, ,"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
