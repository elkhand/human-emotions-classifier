{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image classification\n",
    "\n",
    "## Plan\n",
    "- seed = 7 \n",
    "- All data exists in `dataset/images` directory.\n",
    "- Separate 10% for test dataset.\n",
    "    - Store test dataset under `dataset/test`\n",
    "    - Store `inputDataset = (dataset - test)` under `dataset/input`\n",
    "- Use Stratified 10-fold cross validation to divide `inputDataset` dataset into:\n",
    "    - `train` datasets \n",
    "    - `val` datasets\n",
    "- Train\n",
    "    - Shuffle `train` dataset\n",
    "    - Use `train` dataset to train your model, with validation_split=0.1\n",
    "    - Evalaute in each epoch on `val` dataset\n",
    "\n",
    "```\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_input, x_test, y_input, y_test = train_test_split(xs, ys,\n",
    "                                                    test_size=0.1,\n",
    "                                                    random_state=0,\n",
    "                                                    stratify=ys)\n",
    "\n",
    "shuffle(x_input, y_input)\n",
    "\n",
    "# X_train = \n",
    "# Y_train = \n",
    "\n",
    "validation_split = 0.1\n",
    "\n",
    "for train_set, validation_set in rkf.split(x_input, y_input):\n",
    "    X_train, X_val = x_input[train_set], x_input[validation_set]\n",
    "    Y_train, Y_val = y_input[train_set], y_input[validation_set]\n",
    "\n",
    "    model.fit(X_train, Y_train, epochs=n_epochs, verbose=0,\n",
    "                              validation_split=validation_split)\n",
    "                              \n",
    "    model.evaluate(X_val, Y_val)\n",
    "    \n",
    "# Now run once after long tuning\n",
    "model.evaluate(x_test, y_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elkhand/anaconda3/envs/cs231n/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os, sys, re, string, pathlib, random, io, time, glob\n",
    "from collections import Counter, OrderedDict\n",
    "from shutil import copyfile, rmtree\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import hecutils.data_utils as dt\n",
    "import hecutils.scoring_utils as sc\n",
    "import hecutils.plotting_utils as pt\n",
    "import hecutils.common_utils as cm\n",
    "\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, Callback\n",
    "from keras.layers import GRU, Bidirectional, LSTM, MaxPooling1D, Conv1D,Dense\n",
    "from keras.layers.core import Dropout, Flatten, Masking, ActivityRegularization\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from keras.utils import plot_model\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from keras.preprocessing import text\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "\n",
    "# from fastText import load_model\n",
    "from gensim.models import word2vec\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Keras to use Tensorflow GPU in the backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "\n",
    "num_cores = 8\n",
    "GPU=True\n",
    "CPU = not GPU\n",
    "\n",
    "if GPU:\n",
    "    num_GPU = 1\n",
    "    num_CPU = 8\n",
    "if CPU:\n",
    "    num_CPU = 8\n",
    "    num_GPU = 0\n",
    "\n",
    "config = tf.ConfigProto(intra_op_parallelism_threads=num_cores,\\\n",
    "        inter_op_parallelism_threads=num_cores, allow_soft_placement=True,\\\n",
    "        device_count = {'CPU' : num_CPU, 'GPU' : num_GPU})\n",
    "session = tf.Session(config=config)\n",
    "K.set_session(session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "oasis_csv_path = \"dataset/metadata/OASIS.csv\"\n",
    "oasis_images_src = \"dataset/images/\"\n",
    "input_images_src = \"dataset/input/\"\n",
    "test_images_src = \"dataset/test/\"\n",
    "\n",
    "input_images_classified = \"dataset/input-classified/\"\n",
    "\n",
    "dataset_groups=[\"train\", \"val\"]\n",
    "classes = [\"negative\", \"neutral\", \"positive\"]\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "\n",
    "test_size = 0.1\n",
    "\n",
    "kfold_splits = 10\n",
    "\n",
    "neutralLow = 3.0\n",
    "neutralHigh = 5.0\n",
    "\n",
    "nb_epochs = 1\n",
    "batch_size = 32  \n",
    "FC_SIZE = 1024\n",
    "LAYERS_TO_FREEZE = 249\n",
    "img_height = 299\n",
    "img_width = 299\n",
    "\n",
    "useF1Score = False\n",
    "verbose=1 \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating test and input dataset, and `positive,neutral,negative` under each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data (to be used for model learning and validation) label distribution: \n",
      " OrderedDict([('negative', 147), ('neutral', 378), ('positive', 285)])\n",
      "\n",
      "Test data(never used for learning) label distribution: \n",
      " OrderedDict([('negative', 16), ('neutral', 42), ('positive', 32)])\n"
     ]
    }
   ],
   "source": [
    "image_names, image_labels = dt.get_image_name_and_label(oasis_csv_path, neutralLow, neutralHigh)\n",
    "\n",
    "image_names = np.array(image_names)\n",
    "image_labels = np.array(image_labels)\n",
    "\n",
    "input_x, test_x, input_y,  test_y = train_test_split(image_names, image_labels,\n",
    "                                                    test_size=test_size,\n",
    "                                                    random_state=seed,\n",
    "                                                    stratify=image_labels)\n",
    "\n",
    "print(\"Input data (to be used for model learning and validation) label distribution: \\n\",pt.get_label_count(input_y))\n",
    "print()\n",
    "print(\"Test data(never used for learning) label distribution: \\n\",pt.get_label_count(test_y))\n",
    "\n",
    "\n",
    "# Delete input images dir\n",
    "rmtree(input_images_src, ignore_errors=True)\n",
    "os.makedirs(input_images_src)\n",
    "\n",
    "\n",
    "# Delete test images dir\n",
    "rmtree(test_images_src, ignore_errors=True)\n",
    "os.makedirs(test_images_src)\n",
    "\n",
    "\n",
    "# Copy input images into input dir, and test images into test dir\n",
    "cm.copy_imgs_into(oasis_images_src, input_x, input_images_src)\n",
    "cm.copy_imgs_into(oasis_images_src, test_x, test_images_src)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config {'img_height': 299, 'img_width': 299, 'kfold_splits': 10, 'batch_size': 32, 'nb_epochs': 1, 'useF1Score': False}\n",
      "Train size:  728\n",
      "Val size:  82\n",
      "Train label distribution:  OrderedDict([('negative', 132), ('neutral', 340), ('positive', 256)])\n",
      "Val label distribution:  OrderedDict([('negative', 15), ('neutral', 38), ('positive', 29)])\n",
      "Found 728 images belonging to 3 classes.\n",
      "Found 82 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the cross validator\n",
    "skf = StratifiedKFold(n_splits=kfold_splits, shuffle=True)\n",
    "cv_accuracies = []\n",
    "cv_f1s = []\n",
    "    \n",
    "X = input_x\n",
    "y = input_y\n",
    "    \n",
    "# Shuffe input data\n",
    "X, y = shuffle(X,y)\n",
    "\n",
    "\n",
    "def setup_to_finetune(model, useF1Score):\n",
    "    \"\"\"Freeze the bottom LAYERS_TO_FREEZE and retrain the remaining top layers.\n",
    "  note: LAYERS_TO_FREEZE corresponds to the top 2 inception blocks in the inceptionv3 arch\n",
    "  Args:\n",
    "    model: keras model\n",
    "    \"\"\"    \n",
    "    print(\"LAYERS_TO_FREEZE:\", LAYERS_TO_FREEZE)\n",
    "    for layer in model.layers[:LAYERS_TO_FREEZE]:\n",
    "        layer.trainable = False\n",
    "    for layer in model.layers[LAYERS_TO_FREEZE:]:\n",
    "        layer.trainable = True\n",
    "    model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy',\\\n",
    "                  metrics=cm.get_metrics(useF1Score))\n",
    "\n",
    "def setup_to_transfer_learn(model, base_model, useF1Score):\n",
    "    \"\"\"Freeze all layers and compile the model\"\"\"    \n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "    model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=cm.get_metrics(useF1Score))\n",
    "\n",
    "\n",
    "def add_new_last_layer(base_model, nb_classes):\n",
    "    \"\"\"Add last layer to the convnet\n",
    "      Args:\n",
    "        base_model: keras model excluding top\n",
    "        nb_classes: # of classes\n",
    "      Returns:\n",
    "        new keras model with last layer\n",
    "    \"\"\"\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(FC_SIZE, activation='relu')(x) #new FC layer, random init\n",
    "    predictions = Dense(nb_classes, activation='softmax')(x) #new softmax layer\n",
    "    model = Model(input=base_model.input, output=predictions)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train(trainDir, valDir, config): \n",
    "    \"\"\"Use transfer learning and fine-tuning to train a network on a new dataset\"\"\"     \n",
    "    # this is a generator that will read pictures found in\n",
    "    # subfolers of 'data/train', and indefinitely generate\n",
    "    # batches of augmented image data\n",
    "    train_generator = cm.get_data_generator(trainDir, config)  \n",
    "    \n",
    "\n",
    "    # this is a similar generator, for validation data\n",
    "    validation_generator = cm.get_data_generator(valDir, config)  \n",
    "        \n",
    "    # setup model\n",
    "    #base_model = InceptionV3(weights='imagenet', include_top=False) #include_top=False excludes final FC layer\n",
    "    base_model = InceptionResNetV2(weights='imagenet', include_top=False)\n",
    "    model = add_new_last_layer(base_model, nb_classes)\n",
    "\n",
    "    # transfer learning\n",
    "    setup_to_transfer_learn(model, base_model, config['useF1Score'])\n",
    "    \n",
    "    \n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', \n",
    "                                      factor=0.2, \n",
    "                                      patience=5, \n",
    "                                      min_lr=0.001)\n",
    "    filepath=\"img_model/image.weights.best.h5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=0, save_best_only=True, mode='max')\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10)    \n",
    "    callbacks_list = [ early_stopping, reduce_lr]  #\n",
    "\n",
    "    # train the model on the new data for a few epochs\n",
    "    model.fit_generator(\n",
    "            train_generator,\n",
    "            steps_per_epoch= 5000 // batch_size,\n",
    "            epochs=epochs,\n",
    "            validation_data=validation_generator,\n",
    "            validation_steps= 1000 // batch_size,\n",
    "            shuffle=True,\n",
    "            verbose=verbose,\n",
    "            #callbacks=callbacks_list,\n",
    "            class_weight='auto')\n",
    "    \n",
    "    # fine-tuning\n",
    "    print(\"Starting fune-tuning\")\n",
    "    setup_to_finetune(model, config['useF1Score'])\n",
    "    \n",
    "    callbacks_list.append(checkpoint)\n",
    "    history = model.fit_generator(\n",
    "            train_generator,\n",
    "            steps_per_epoch= 5000 // batch_size,\n",
    "            epochs=epochs*2,\n",
    "            validation_data=validation_generator,\n",
    "            validation_steps= 1000 // batch_size,\n",
    "            shuffle=True,\n",
    "            verbose=verbose,\n",
    "            #callbacks=callbacks_list,\n",
    "            class_weight='auto')\n",
    "    return history, model\n",
    "\n",
    "\n",
    "\n",
    "def prepare_and_train(config):\n",
    "    bestModel = None\n",
    "    final_model_val_acc = -1\n",
    "    for index, (train_indices, val_indices) in enumerate(skf.split(X, y)):\n",
    "        print(\"Train size: \", len(train_indices))\n",
    "        print(\"Val size: \", len(val_indices))\n",
    "\n",
    "        X_train, X_val = X[train_indices], X[val_indices]\n",
    "        y_train, y_val = y[train_indices], y[val_indices]\n",
    "\n",
    "        print(\"Train label distribution: \", pt.get_label_count(y_train))\n",
    "        print(\"Val label distribution: \", pt.get_label_count(y_val))\n",
    "\n",
    "        # Divide input images into train and dev set, and each one into {negative, neutral, positive}\n",
    "        dt.create_cross_validation_train_val(\"train\", input_images_src, input_images_classified, X_train, y_train)\n",
    "        dt.create_cross_validation_train_val(\"val\", input_images_src, input_images_classified, X_val, y_val)\n",
    "\n",
    "        trainDir = input_images_classified + \"/\" + \"train\" + \"/\"\n",
    "        valDir = input_images_classified + \"/\" + \"val\" + \"/\"\n",
    "\n",
    "        history, model = train(trainDir, valDir, config)\n",
    "        \n",
    "        pt.plot_model_accuracy(history, \"img_model/\", useF1Score)\n",
    "        best_val_acc = max(history.history['val_acc'])\n",
    "        \n",
    "        if best_val_acc > final_model_val_acc:\n",
    "            bestModel = model\n",
    "        \n",
    "        cv_accuracies.append(best_val_acc)\n",
    "        print(\"best_val_acc\", best_val_acc)\n",
    "        if useF1Score:\n",
    "            best_val_f1 = max(history.history['val_f1'])\n",
    "            cv_f1s.append(best_val_f1)\n",
    "\n",
    "    print(\"=========================================\")\n",
    "    print(\"Cross-validation val accuracy results: \" , cv_accuracies)\n",
    "    print(\"Cross-validation val accuracy results: %.2f%% (+/- %.2f%%)\" % (np.mean(cv_accuracies), np.std(cv_accuracies)))\n",
    "\n",
    "    if hasF1:\n",
    "        print(\"\\n\",\"Cross-validation val f1 results: \" , cv_f1s)\n",
    "        print(\"Cross-validation val f1 results: %.2f%% (+/- %.2f%%)\" % (np.mean(cv_f1s), np.std(cv_f1s)))   \n",
    "    model.save(\"model/bestmodel-\" + final_model_val_acc + \".h5\")\n",
    "\n",
    "def get_config():\n",
    "    conf = {}\n",
    "    conf[\"img_height\"] = img_height\n",
    "    conf[\"img_width\"] = img_width\n",
    "    conf[\"kfold_splits\"] = kfold_splits\n",
    "    conf[\"batch_size\"] = batch_size\n",
    "    conf[\"nb_epochs\"] = nb_epochs\n",
    "    conf['useF1Score'] = useF1Score\n",
    "    return conf    \n",
    "    \n",
    "def main():\n",
    "    config = get_config()\n",
    "    print(\"config:\\n\", config)\n",
    "    prepare_and_train(config)\n",
    "    # predict()\n",
    "    # text_pre_processing(\"hello&nbsp;hi\")\n",
    "    pass\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # execute only if run as a script\n",
    "    main()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
